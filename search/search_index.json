{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Personal blog","text":""},{"location":"2024/08/24/all-things-start-with-git/","title":"All things start with git","text":""},{"location":"2024/08/24/all-things-start-with-git/#git-gitlab-et-github-pour-le-cicd","title":"Git, GitLab et GitHub pour le CI/CD","text":"<p>Dans cet article, nous allons explorer comment Git, GitLab, et GitHub sont utilis\u00e9s pour impl\u00e9menter des pipelines CI/CD (Int\u00e9gration Continue et D\u00e9ploiement Continu). Ces outils sont devenus essentiels pour automatiser et g\u00e9rer les processus de d\u00e9veloppement logiciel. Comment vous pouvez les utiliser pour am\u00e9liorer votre flux de travail.</p>"},{"location":"2024/08/24/all-things-start-with-git/#quest-ce-que-git","title":"Qu'est-ce que Git ?","text":"<p>Git est un syst\u00e8me de gestion de version distribu\u00e9. Cela signifie qu'il permet \u00e0 plusieurs d\u00e9veloppeurs de travailler sur un projet de mani\u00e8re simultan\u00e9e sans se marcher sur les pieds. Voici quelques concepts de base de Git :</p> <ul> <li>Repository (D\u00e9p\u00f4t) : C'est un espace de stockage o\u00f9 l'historique de votre projet est enregistr\u00e9. Il contient tous les fichiers et l'historique des modifications.</li> <li>Branch (Branche) : Une branche est une version parall\u00e8le du code sur laquelle vous pouvez travailler ind\u00e9pendamment. Une branche peut \u00eatre fusionn\u00e9e avec la branche principale (<code>main</code> ou <code>master</code>) apr\u00e8s approbation.</li> <li>Commit : Un commit est un enregistrement de changements dans le d\u00e9p\u00f4t. Chaque commit a un identifiant unique qui permet de revenir en arri\u00e8re ou de fusionner des modifications.</li> <li>Merge : C'est l'action de fusionner les changements d'une branche dans une autre.</li> </ul>"},{"location":"2024/08/24/all-things-start-with-git/#comment-fonctionne-git","title":"Comment fonctionne Git ?","text":"<p>Voici une illustration simple du fonctionnement de Git avec un exemple de flux de travail :</p> <pre><code>graph TD;\n    A[Clone du d\u00e9p\u00f4t] --&gt; B[Cr\u00e9ation d'une branche];\n    B --&gt; C[Modification du code];\n    C --&gt; D[Commit des changements];\n    D --&gt; E[Fusion de la branche];\n    E --&gt; F[Push vers le d\u00e9p\u00f4t distant];</code></pre>"},{"location":"2024/08/24/all-things-start-with-git/#explication-du-processus-git","title":"Explication du Processus Git","text":"<ol> <li>Clone du D\u00e9p\u00f4t : Vous commencez par cloner un d\u00e9p\u00f4t existant depuis GitHub ou GitLab vers votre machine locale.</li> <li>Cr\u00e9ation d'une Branche : Vous cr\u00e9ez une nouvelle branche pour travailler sur une fonctionnalit\u00e9 ou une correction de bug.</li> <li>Modification du Code : Vous faites les changements n\u00e9cessaires dans votre code.</li> <li>Commit des Changements : Vous enregistrez vos modifications dans le d\u00e9p\u00f4t local avec un message de commit.</li> <li>Fusion de la Branche : Une fois les modifications pr\u00eates, vous fusionnez votre branche dans la branche principale.</li> <li>Push vers le D\u00e9p\u00f4t Distant : Enfin, vous poussez vos changements vers le d\u00e9p\u00f4t distant pour les partager avec les autres d\u00e9veloppeurs.</li> </ol>"},{"location":"2024/08/24/all-things-start-with-git/#quest-ce-que-gitlab-et-github","title":"Qu'est-ce que GitLab et GitHub ?","text":""},{"location":"2024/08/24/all-things-start-with-git/#github","title":"GitHub","text":"<p>GitHub est une plateforme de d\u00e9veloppement collaboratif qui repose sur Git. Elle est principalement utilis\u00e9e pour h\u00e9berger des d\u00e9p\u00f4ts Git et permet de collaborer sur des projets de mani\u00e8re transparente. GitHub offre \u00e9galement des fonctionnalit\u00e9s de CI/CD via GitHub Actions, qui permettent d'automatiser les tests, les builds, et les d\u00e9ploiements.</p>"},{"location":"2024/08/24/all-things-start-with-git/#gitlab","title":"GitLab","text":"<p>GitLab est une plateforme similaire \u00e0 GitHub, mais avec un ensemble d'outils encore plus complet pour le DevOps. GitLab CI/CD est une fonctionnalit\u00e9 int\u00e9gr\u00e9e qui permet de cr\u00e9er des pipelines pour automatiser les tests, les builds, et les d\u00e9ploiements directement depuis le d\u00e9p\u00f4t GitLab.</p>"},{"location":"2024/08/24/all-things-start-with-git/#fonctionnement-de-la-cicd-avec-gitlab-et-github","title":"Fonctionnement de la CI/CD avec GitLab et GitHub","text":"<ol> <li>GitHub Actions (CI/CD)</li> </ol> <p>GitHub Actions vous permet de cr\u00e9er des workflows pour automatiser les processus de d\u00e9veloppement. Ces workflows sont d\u00e9finis dans un fichier YAML au sein du d\u00e9p\u00f4t.</p> <p>Exemple de workflow pour GitHub Actions :</p> <pre><code>name: CI/CD Pipeline\n\non:\n  push:\n    branches:\n      - main\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v2\n\n      - name: Install dependencies\n        run: pip install -r requirements.txt\n\n      - name: Run tests\n        run: pytest\n</code></pre> <ol> <li>GitLab CI/CD</li> </ol> <p>GitLab CI/CD utilise un fichier <code>.gitlab-ci.yml</code> pour d\u00e9finir les pipelines. Ce fichier d\u00e9crit les \u00e9tapes que GitLab doit suivre pour tester, construire, et d\u00e9ployer le code.</p> <p>Exemple de pipeline pour GitLab CI/CD :</p> <pre><code>stages:\n  - test\n  - build\n  - deploy\n\ntest:\n  stage: test\n  script:\n    - pytest\n\nbuild:\n  stage: build\n  script:\n    - python setup.py sdist\n\ndeploy:\n  stage: deploy\n  script:\n    - scp dist/* user@server:/path/to/deploy/\n</code></pre> <ol> <li>Visualisation du Processus CI/CD</li> </ol> <p>Pour mieux comprendre le processus CI/CD avec Git, GitHub, et GitLab, voici une visualisation avec Mermaid :</p> <pre><code>graph LR\n    A[Push du Code] --&gt; B[Pipeline CI/CD D\u00e9marr\u00e9]\n    B --&gt; C[Tests]\n    C --&gt; D{Tests R\u00e9ussis?}\n    D --&gt;|Oui| E[Build de l'Application]\n    E --&gt; F[D\u00e9ploiement sur Environnement de ProD]\n    D --&gt;|Non| G[Retourne les Erreurs au D\u00e9veloppeur]</code></pre> <p>\u00c9tapes du Processus CI/CD</p> <ol> <li>Push du Code : Le d\u00e9veloppeur pousse son code vers GitLab ou GitHub.</li> <li>Pipeline CI/CD D\u00e9marr\u00e9 : Le push d\u00e9clenche automatiquement un pipeline CI/CD.</li> <li>Tests : Le code est test\u00e9 pour d\u00e9tecter les erreurs ou les bugs.</li> <li>Build : Si les tests r\u00e9ussissent, l'application est construite (compilation, packaging, etc.).</li> <li>D\u00e9ploiement : Enfin, l'application est d\u00e9ploy\u00e9e sur l'environnement de production. Si les tests \u00e9chouent, les erreurs sont retourn\u00e9es au d\u00e9veloppeur pour correction.</li> </ol>"},{"location":"2024/08/24/all-things-start-with-git/#conclusion","title":"Conclusion","text":"<p>Git, GitLab, et GitHub sont des outils puissants pour g\u00e9rer et automatiser le d\u00e9veloppement logiciel. En utilisant Git pour le contr\u00f4le de version et GitLab/GitHub pour le CI/CD, vous pouvez cr\u00e9er un flux de travail robuste qui assure que votre code est test\u00e9, valid\u00e9, et d\u00e9ploy\u00e9 automatiquement \u00e0 chaque changement. Ces pratiques vous permettent de livrer du code de qualit\u00e9 plus rapidement et plus efficacement.</p>"},{"location":"2024/08/25/cicd-pour-les-data-scientists--quand-le-code-se-met-%C3%A0-danser/","title":"CI/CD pour les Data Scientists : Quand le Code Se Met \u00e0 Danser","text":"<p>La collaboration entre data scientists et d\u00e9veloppeurs peut parfois ressembler \u00e0 une partie de ping-pong chaotique : chacun fait rebondir des id\u00e9es et des bouts de code, mais rien ne semble vraiment s'assembler correctement. Heureusement, il existe une solution pour rendre cette danse collaborative plus harmonieuse : la CI/CD (Continuous Integration/Continuous Deployment). Oui, m\u00eame pour les data scientists\u202f! Alors, prenez vos notebooks, ajustez vos lunettes, et d\u00e9couvrons comment transformer cette pagaille en une symphonie bien orchestr\u00e9e :)</p>"},{"location":"2024/08/25/cicd-pour-les-data-scientists--quand-le-code-se-met-%C3%A0-danser/#quest-ce-que-la-cicd","title":"Qu'est-ce que la CI/CD ?","text":"<p>Avant de plonger dans les d\u00e9tails, clarifions ce que signifie CI/CD, surtout pour ceux qui, parmi nous, passent plus de temps \u00e0 jongler des notebooks qu\u2019\u00e0 jongler avec des pipelines. Ou travaillent : generalement en nombre reduits au pr\u00e8s des metiers:</p> <ul> <li>Continuous Integration (CI): une pratique qui consiste \u00e0 int\u00e9grer r\u00e9guli\u00e8rement les modifications du code dans un d\u00e9p\u00f4t central, o\u00f9 elles sont automatiquement test\u00e9es. Imaginez un petit robot qui v\u00e9rifie si chaque ligne de code que vous ajoutez fonctionne bien avec le reste du code, comme un danseur de tango qui s'assure que chaque pas est en harmonie avec le rythme. L'id\u00e9e est de d\u00e9tecter rapidement les erreurs afin qu'elles ne s'accumulent pas comme une pile de vaisselle sale (vous savez, celle qu\u2019on promet de faire plus tard, mais qui finit par devenir un Everest in\u00e9branlable).</li> </ul> <ul> <li>Continuous Deployment (CD): c' est comme la cerise sur le g\u00e2teau. Ici, chaque modification valid\u00e9e (apr\u00e8s les tests de CI) est automatiquement d\u00e9ploy\u00e9e en production. Oui, vous avez bien entendu : plus besoin d'appuyer sur un bouton pour d\u00e9ployer, c'est comme si votre code se d\u00e9ployait tout seul, un peu comme une machine \u00e0 caf\u00e9 qui se pr\u00e9pare elle-m\u00eame une nouvelle tasse de caf\u00e9 d\u00e8s que vous avez termin\u00e9 la pr\u00e9c\u00e9dente. Le r\u00eave, non ?</li> </ul>"},{"location":"2024/08/25/cicd-pour-les-data-scientists--quand-le-code-se-met-%C3%A0-danser/#cicd-pour-les-data-scientists-pourquoi","title":"CI/CD pour les Data Scientists : Pourquoi ?","text":"<p>Pendant longtemps, la cicd etait propre aux developpeurs : Bonne pratique de developpement (devops). Avec le developpement de la data science et la volont\u00e9 de maturer les projets data, on a donc commencer \u00e0 entendre parler de MLOPS. Disons que la CICD est une composante pour faire du MlOps.</p> <p>En tant que data scientist, vous pourriez vous demander : \"Pourquoi devrais-je me pr\u00e9occuper de tout ce bazar ? Mes notebooks fonctionnent tr\u00e8s bien tels quels !\" Certes, mais imaginez la sc\u00e8ne : vous travaillez sur un mod\u00e8le hyper complexe, vous l\u2019entra\u00eenez pendant des heures (ou des jours), et puis\u2026 Oups, un autre membre de l'\u00e9quipe modifie le code d'importation des donn\u00e9es, et votre magnifique mod\u00e8le ne fonctionne plus. Catastrophe. Ou plus simplement, vous voudriez suivre l'historique d'un code. Le code marchait -il avant? Difficilement de repondre \u00e0 ces questions \u00e0 priori sans CICD</p>"},{"location":"2024/08/25/cicd-pour-les-data-scientists--quand-le-code-se-met-%C3%A0-danser/#les-enjeux-de-la-collaboration","title":"Les Enjeux de la Collaboration","text":"<p>La collaboration entre plusieurs data scientists (et d\u00e9veloppeurs) sur un m\u00eame projet peut vite devenir compliqu\u00e9e. Chacun a son style, ses m\u00e9thodes, et son code. Comme une recette de cuisine o\u00f9 chaque cuisinier ajoute ses propres ingr\u00e9dients sans se concerter avec les autres, le r\u00e9sultat peut \u00eatre\u2026 surprenant, pour ne pas dire immangeable. </p>"},{"location":"2024/08/25/cicd-pour-les-data-scientists--quand-le-code-se-met-%C3%A0-danser/#la-solution-cicd-pour-data-scientists","title":"La Solution : CI/CD pour Data Scientists","text":"<p>Impl\u00e9menter une cha\u00eene CI/CD dans vos projets de data science permet d'assurer que :</p> <ol> <li> <p>Tous les changements sont test\u00e9s : Vous \u00e9vitez le fameux \"\u00e7a marche sur ma machine !\" en vous assurant que chaque modification est test\u00e9e dans un environnement standardis\u00e9.</p> </li> <li> <p>Le code est toujours pr\u00eat pour la production : Vous pouvez d\u00e9ployer vos mod\u00e8les en production rapidement et en toute confiance, sans avoir \u00e0 passer des jours \u00e0 les v\u00e9rifier manuellement.</p> </li> <li> <p>La documentation et le versionning sont automatiques : Chaque modification est document\u00e9e, et vous pouvez facilement revenir en arri\u00e8re en cas de probl\u00e8me (comme une machine \u00e0 remonter le temps pour votre code).</p> </li> <li> <p>Tout developpeur ou data scientist pourrait reprendre vos travaux sans perdre les cheveux.</p> </li> </ol> <p>C'est quoi git , gitlab ou github dans tout \u00e7a? Je vous invite \u00e0 faire un tour sur cet article au cas o\u00f9 vous n'etes pas tres familier avec des trois mots: github vs gitlab vs git</p>"},{"location":"2024/08/25/cicd-pour-les-data-scientists--quand-le-code-se-met-%C3%A0-danser/#exemple-pour-un-projet-data-science","title":"Exemple pour un Projet Data Science","text":"<p>Dans ce guide, nous allons cr\u00e9er un pipeline CI/CD pour un projet de data science sur GitLab. Nous aborderons la structure du projet, la configuration du pipeline avec GitLab CI/CD, les tests, le d\u00e9ploiement d'une application Dash, et un bonus sur l'utilisation des \u201cgit hooks\u201d pour tester localement avant de pousser les changements. Avant tout parlons de </p>"},{"location":"2024/08/25/cicd-pour-les-data-scientists--quand-le-code-se-met-%C3%A0-danser/#structure-du-projet","title":"Structure du Projet","text":"<p>Voici une structure typique pour un projet de data science utilisant GitLab CI/CD :</p> <pre><code>mon_projet_data_science/\n\u2502\n\u251c\u2500\u2500 .gitlab-ci.yml        # Fichier de configuration pour le pipeline CI/CD\n\u251c\u2500\u2500 requirements.txt      # Fichier listant les d\u00e9pendances Python\n\u251c\u2500\u2500 README.md             # Documentation du projet\n\u251c\u2500\u2500 setup.py              # Script d'installation pour le projet\n\u2502\n\u251c\u2500\u2500 data/                 # R\u00e9pertoire contenant les donn\u00e9es\n\u2502   \u251c\u2500\u2500 raw/              # Donn\u00e9es brutes non trait\u00e9es\n\u2502   \u2514\u2500\u2500 processed/        # Donn\u00e9es pr\u00e9-trait\u00e9es\n\u2502\n\u251c\u2500\u2500 src/                  # R\u00e9pertoire du code source principal\n\u2502   \u251c\u2500\u2500 __init__.py       # Fichier d'initialisation du package Python\n\u2502   \u251c\u2500\u2500 data_loader.py    # Script pour charger et traiter les donn\u00e9es\n\u2502   \u251c\u2500\u2500 model.py          # D\u00e9finition du mod\u00e8le de machine learning\n\u2502   \u2514\u2500\u2500 train_model.py    # Script pour l'entra\u00eenement du mod\u00e8le\n\u2502\n\u251c\u2500\u2500 tests/                # R\u00e9pertoire contenant les tests\n\u2502   \u251c\u2500\u2500 __init__.py       # Fichier d'initialisation du package de tests\n\u2502   \u251c\u2500\u2500 test_data_loader.py  # Tests pour le chargement des donn\u00e9es\n\u2502   \u2514\u2500\u2500 test_model.py     # Tests pour le mod\u00e8le de machine learning\n\u2502\n\u2514\u2500\u2500 notebooks/            # R\u00e9pertoire pour les notebooks Jupyter\n    \u251c\u2500\u2500 exploration.ipynb # Notebook pour l'exploration des donn\u00e9es\n    \u2514\u2500\u2500 analysis.ipynb    # Notebook pour l'analyse des r\u00e9sultats\n</code></pre>"},{"location":"2024/08/25/cicd-pour-les-data-scientists--quand-le-code-se-met-%C3%A0-danser/#exemple-avec-gitlab","title":"Exemple avec gitlab","text":"<p>Cela revient \u00e0 configurer le fichier <code>.gitlab-ci.yml</code> Voici un exemple de configuration pour le pipeline CI/CD :</p> <pre><code>stages:\n  - install\n  - test\n  - lint\n  - train\n  - deploy\n\nvariables:\n  VENV_PATH: .venv\n\nbefore_script:\n  - python3 -m venv $VENV_PATH\n  - source $VENV_PATH/bin/activate\n  - pip install --upgrade pip\n  - pip install -r requirements.txt\n\ninstall:\n  stage: install\n  script:\n    - pip install -r requirements.txt\n  cache:\n    paths:\n      - $VENV_PATH\n\ntest:\n  stage: test\n  script:\n    - pytest tests/\n\nlint:\n  stage: lint\n  script:\n    - flake8 .\n\ntrain:\n  stage: train\n  script:\n    - python src/train_model.py\n\ndeploy:\n  stage: deploy\n  script:\n    - echo \"D\u00e9ploiement de l'application Dash sur le serveur de production...\"\n    - scp -r * user@server:/path/to/deployment\n  only:\n    - main\n</code></pre> <p>Explications des \u00c9tapes du Pipeline :</p> <ol> <li>Install Stage : Installe les d\u00e9pendances Python d\u00e9finies dans <code>requirements.txt</code>.</li> <li>Test Stage : Ex\u00e9cute les tests unitaires avec <code>pytest</code> pour s'assurer que chaque composant fonctionne correctement.</li> <li>Lint Stage : Utilise <code>flake8</code> pour v\u00e9rifier la qualit\u00e9 du code et s'assurer qu'il respecte les bonnes pratiques de codage.</li> <li>Train Stage : Lance l'entra\u00eenement du mod\u00e8le de machine learning en ex\u00e9cutant le script <code>train_model.py</code>.</li> <li>Deploy Stage : D\u00e9ploie l'application Dash sur un serveur distant. Cette \u00e9tape est d\u00e9clench\u00e9e uniquement pour la branche <code>main</code>.</li> </ol>"},{"location":"2024/08/25/cicd-pour-les-data-scientists--quand-le-code-se-met-%C3%A0-danser/#utiliser-des-git-hooks","title":"Utiliser des Git Hooks","text":"<p>Dans le cas o\u00f9 vous ne disposer pas de serveur distant pour lancer vos codes, vous pourriez utiliser  git hooks pour ex\u00e9cuter les tests locaux. Par exemple, un hook <code>pre-push</code> peut \u00eatre utilis\u00e9 pour ex\u00e9cuter les tests avant chaque <code>git push</code>.</p> <ol> <li> <p>Cr\u00e9er un Hook <code>pre-push</code> :</p> <p>Dans le r\u00e9pertoire <code>.git/hooks</code>, cr\u00e9ez un fichier nomm\u00e9 <code>pre-push</code> :</p> <p><pre><code>touch .git/hooks/pre-push\n</code></pre> 2. Rendre le Hook Ex\u00e9cutable : 3. Ajouter le Script pour Ex\u00e9cuter les Tests :</p> <p>Ouvrez le fichier <code>pre-push</code> et ajoutez le script suivant :</p> </li> </ol> <p><pre><code>#!/bin/bash\n\necho \"Ex\u00e9cution des tests locaux avant le push...\"\nsource .venv/bin/activate\npytest tests/\n\nif [ $? -ne 0 ]; then\n    echo \"\u00c9chec des tests. Annulation du push.\"\n    exit 1\nfi\n\necho \"Tests r\u00e9ussis. Push en cours...\"\n</code></pre> Avec ce hook en place, chaque tentative de git push ex\u00e9cutera les tests locaux. Si les tests \u00e9chouent, le push sera annul\u00e9, garantissant ainsi que seul le code valide est pouss\u00e9 vers le d\u00e9p\u00f4t distant.</p>"},{"location":"2024/08/25/cicd-pour-les-data-scientists--quand-le-code-se-met-%C3%A0-danser/#conclusion","title":"Conclusion","text":"<p>En somme, int\u00e9grer la CI/CD dans vos projets de data science est comme apprendre \u00e0 danser le tango avec vos coll\u00e8gues : c'est au d\u00e9but un peu maladroit, mais une fois que vous avez le rythme, vous ne pouvez plus vous en passer. Cela transforme votre fa\u00e7on de travailler, rend vos collaborations plus fluides, et garantit que vos mod\u00e8les sont toujours au top de leur forme.</p> <p>Alors, chers data scientists, pr\u00eats \u00e0 chausser vos chaussures de danse et \u00e0 adopter la CI/CD ? Parce qu\u2019une fois que vous y aurez go\u00fbt\u00e9, vous ne reviendrez jamais en arri\u00e8re. Promis, jur\u00e9.</p> <p>References : - https://martinfowler.com/articles/continuousIntegration.html</p> <ul> <li>https://martinfowler.com/books/duvall.html</li> </ul>"},{"location":"2024/09/21/apache-arrow-pour-loptimisation-du-traitement-des-donn%C3%A9es/","title":"Apache Arrow pour l'optimisation du traitement des donn\u00e9es","text":"<p>La gestion des donn\u00e9es en m\u00e9moire ressemble parfois \u00e0 un tatonnement. Les  data engineers, scientists et analysts se retrouvent souvent \u00e0 jongler entre diff\u00e9rents formats de donn\u00e9es, calculs intensifs et besoins de performance. Jusqu\u2019\u00e0 r\u00e9cemment, nous \u00e9tions limit\u00e9s par des outils et formats con\u00e7us pour des volumes et des vitesses bien inf\u00e9rieurs \u00e0 ceux d\u2019aujourd'hui. C\u2019est l\u00e0 qu\u2019Apache Arrow entre en sc\u00e8ne, tel un champion pr\u00eat \u00e0 transformer ce marathon en un sprint ma\u00eetris\u00e9. Dans ce billet de blog, on ne va parler que de donn\u00e9es. Comment python g\u00e8re les dataframes en backend?</p>"},{"location":"2024/09/21/apache-arrow-pour-loptimisation-du-traitement-des-donn%C3%A9es/#representation-tabulaire-des-donnees","title":"Repr\u00e9sentation tabulaire des donn\u00e9es","text":""},{"location":"2024/09/21/apache-arrow-pour-loptimisation-du-traitement-des-donn%C3%A9es/#theoriquement","title":"Th\u00e9oriquement","text":"<p>On commence par un petit voyage dans la mani\u00e8re dont les donn\u00e9es sont stock\u00e9es. Imaginez que vous avez une \u00e9norme table remplie de donn\u00e9es :</p> <p></p> <p>Il existe deux fa\u00e7ons principales de l\u2019organiser en m\u00e9moire dans un logiciel :</p> <ul> <li> <p>Row-wise : ici, les donn\u00e9es sont stock\u00e9es ligne par ligne. Chaque ligne repr\u00e9sente un enregistrement complet. C\u2019est comme si vous lisiez un livre page par page, pratique pour \u00e9crire rapidement des donn\u00e9es.</p> <p></p> </li> </ul> <ul> <li> <p>Columnar : dans ce format, les donn\u00e9es sont stock\u00e9es par colonne. Chaque colonne contient les valeurs d\u2019un m\u00eame attribut, ce qui est tr\u00e8s efficace pour les op\u00e9rations de lecture et d'analyse, notamment lors du filtrage ou de l'agr\u00e9gation.</p> <p></p> </li> </ul>"},{"location":"2024/09/21/apache-arrow-pour-loptimisation-du-traitement-des-donn%C3%A9es/#en-pratique-stockage-sur-disque","title":"En pratique : stockage sur disque","text":"<p>La mani\u00e8re dont ces formats sont impl\u00e9ment\u00e9s peut avoir un impact majeur sur les performances d'analyse. Prenons l'exemple d'un data scientist travaillant sur de grands ensembles de donn\u00e9es.</p>"},{"location":"2024/09/21/apache-arrow-pour-loptimisation-du-traitement-des-donn%C3%A9es/#formats-row-wise","title":"Formats Row-wise","text":"<p>Les formats row-wise sont souvent utilis\u00e9s dans des bases de donn\u00e9es transactionnelles comme PostgreSQL ou MySQL. Ils permettent un acc\u00e8s rapide aux enregistrements complets, id\u00e9aux pour des op\u00e9rations telles que les insertions et mises \u00e0 jour. Toutefois, pour les analyses complexes, ce format peut ralentir les choses, car il est moins efficace pour les op\u00e9rations d'agr\u00e9gation ou de filtrage sur des colonnes sp\u00e9cifiques. Par exemple, interroger une base de donn\u00e9es pour des statistiques sur une colonne exige la lecture de toutes les lignes, ce qui entra\u00eene une surcharge.</p> <p>Les formats row-wise incluent \u00e9galement des formats comme CSV, Excel, et JSON :</p> <ul> <li>CSV (Comma-Separated Values) : Ce format, tr\u00e8s r\u00e9pandu, stocke les donn\u00e9es sous forme de texte brut. Bien qu'il soit pratique pour l'\u00e9change de donn\u00e9es, il se r\u00e9v\u00e8le souvent inefficace pour les analyses complexes de grands ensembles de donn\u00e9es, en raison de sa structure plate et non optimis\u00e9e.</li> </ul> <ul> <li>JSON (JavaScript Object Notation) : Structur\u00e9 de mani\u00e8re row-wise, JSON est id\u00e9al pour l'\u00e9change de donn\u00e9es, mais reste inefficace pour les analyses complexes.</li> </ul>"},{"location":"2024/09/21/apache-arrow-pour-loptimisation-du-traitement-des-donn%C3%A9es/#formats-columnar","title":"Formats Columnar","text":"<p>Les formats columnar sont optimis\u00e9s pour les performances lors des analyses de donn\u00e9es. Ils permettent des op\u00e9rations rapides sur les colonnes, notamment pour le filtrage et l'agr\u00e9gation. Les formats optimis\u00e9s incluent Apache Parquet, ORC, et Feather.</p> <ul> <li>Parquet : Un format de fichier compress\u00e9 et optimis\u00e9 pour le stockage de grands volumes de donn\u00e9es. Il est particuli\u00e8rement efficace pour les analyses car il stocke les donn\u00e9es par colonne et compresse les valeurs similaires, r\u00e9duisant ainsi l'espace de stockage et am\u00e9liorant les performances d'E/S.</li> </ul> <ul> <li>ORC (Optimized Row Columnar) : Similaire \u00e0 Parquet, ORC offre des performances de lecture rapides et une compression efficace, souvent utilis\u00e9 dans les environnements Hadoop.</li> </ul> <ul> <li>Feather : Format simplifi\u00e9 pour les \u00e9changes rapides de donn\u00e9es entre langages, comme Python et R. Il permet un acc\u00e8s extr\u00eamement rapide et r\u00e9duit le temps d'E/S gr\u00e2ce \u00e0 des buffers efficaces.</li> </ul>"},{"location":"2024/09/21/apache-arrow-pour-loptimisation-du-traitement-des-donn%C3%A9es/#backends-arrow-vs-numpy-array","title":"Backends Arrow vs NumPy Array","text":"<p>Lorsque nous travaillons sur des donn\u00e9es en Python, que ce soit pour des calculs ou du stockage, nous utilisons des biblioth\u00e8ques comme Pandas, NumPy, Polars ou PyArrow. Mais comment ces donn\u00e9es sont-elles g\u00e9r\u00e9es en backend ? </p>"},{"location":"2024/09/21/apache-arrow-pour-loptimisation-du-traitement-des-donn%C3%A9es/#gestion-des-formats-de-donnees","title":"Gestion des formats de donn\u00e9es","text":"<p>L'image ci-dessus illustre comment Apache Arrow s'est impos\u00e9 comme un standard pour le traitement de donn\u00e9es en m\u00e9moire. Autrefois, chaque outil devait r\u00e9aliser des conversions pour lire diff\u00e9rents formats de donn\u00e9es (CSV, Parquet, etc.), ce qui entra\u00eenait des pertes de temps et une inefficacit\u00e9 des ressources. Aujourd'hui, Arrow centralise le stockage en m\u00e9moire, permettant aux outils d'acc\u00e9der directement aux donn\u00e9es sans conversion interm\u00e9diaire.</p>"},{"location":"2024/09/21/apache-arrow-pour-loptimisation-du-traitement-des-donn%C3%A9es/#integration-progressive-du-backend-pyarrow","title":"Int\u00e9gration progressive du backend PyArrow","text":"<p>Par d\u00e9faut, Pandas utilise NumPy comme backend :</p> <pre><code>pd.read_csv('data.csv')\n</code></pre> <p>Depuis Pandas 2.0, il est d\u00e9sormais possible d'utiliser Apache Arrow comme backend pour am\u00e9liorer les performances, notamment pour les types de donn\u00e9es complexes comme les cha\u00eenes de caract\u00e8res et les dates.</p> <p>Voici comment lire un fichier CSV avec le backend PyArrow :</p> <pre><code>pd.read_csv(\"data.csv\", engine=\"pyarrow\", dtype_backend=\"pyarrow\")\n</code></pre> <p>Sur un fichier de test, cette m\u00e9thode a \u00e9t\u00e9 32 fois plus rapide que l'ex\u00e9cution standard avec NumPy.</p>"},{"location":"2024/09/21/apache-arrow-pour-loptimisation-du-traitement-des-donn%C3%A9es/#pourquoi","title":"Pourquoi ?","text":"<p>Pour comprendre la raison, r\u00e9sumons rapidement comment Pandas fonctionne. L'id\u00e9e g\u00e9n\u00e9rale est qu'avant de pouvoir faire quoi que ce soit dans Pandas, il est n\u00e9cessaire de charger en m\u00e9moire les donn\u00e9es d'int\u00e9r\u00eat (en utilisant des m\u00e9thodes comme read_csv, read_sql, read_parquet, etc.). Lors du chargement des donn\u00e9es en m\u00e9moire, il est n\u00e9cessaire de d\u00e9cider comment ces donn\u00e9es seront stock\u00e9es. Pour les simples donn\u00e9es comme les entiers ou les flottants, cela n'est g\u00e9n\u00e9ralement pas si compliqu\u00e9, car la repr\u00e9sentation d'un seul \u00e9l\u00e9ment est principalement standard, et nous avons juste besoin de tableaux du nombre d'\u00e9l\u00e9ments dans nos donn\u00e9es. Mais pour d'autres types (comme les cha\u00eenes, les dates et heures, les cat\u00e9gories, etc.),</p> <p>Python est capable de repr\u00e9senter presque tout, mais les structures de donn\u00e9es de Python (listes, dictionnaires, tuples, etc.) sont tr\u00e8s lentes et ne peuvent pas \u00eatre utilis\u00e9es. Ainsi, la repr\u00e9sentation des donn\u00e9es n'est ni en Python ni standard, et une impl\u00e9mentation doit \u00eatre r\u00e9alis\u00e9e via des extensions Python, g\u00e9n\u00e9ralement impl\u00e9ment\u00e9es en C (ou en C++, Rust, et autres). Pendant de nombreuses ann\u00e9es, l'extension principale pour repr\u00e9senter des tableaux et effectuer des op\u00e9rations sur eux de mani\u00e8re rapide a \u00e9t\u00e9 NumPy. Et c'est sur cette base que Pandas a initialement \u00e9t\u00e9 construit.</p> <p>Apache Arrow, quant \u00e0 lui, a \u00e9merg\u00e9 comme un standard pour le traitement en m\u00e9moire. Il permet de centraliser les donn\u00e9es, \u00e9liminant la n\u00e9cessit\u00e9 de conversions co\u00fbteuses entre diff\u00e9rents formats de stockage. Cela am\u00e9liore consid\u00e9rablement les performances des analyses.</p>"},{"location":"2024/09/21/apache-arrow-pour-loptimisation-du-traitement-des-donn%C3%A9es/#fonctionnement-des-backends-arrow-et-numpy","title":"Fonctionnement des backends Arrow et NumPy","text":"<p>Pour illustrer les diff\u00e9rences entre NumPy et Apache Arrow, g\u00e9n\u00e9rons des donn\u00e9es similaires \u00e0 celles que nous avons utilis\u00e9es pr\u00e9c\u00e9demment. </p> <p>Imaginons que nous avons une colonne avec les cha\u00eenes suivantes :</p> <ul> <li><code>\"numpy\"</code></li> </ul> <ul> <li><code>\"data\"</code></li> </ul> <ul> <li><code>\"analysis\"</code></li> </ul> <ul> <li><code>null</code></li> </ul> <ul> <li><code>\"performance\"</code></li> </ul>"},{"location":"2024/09/21/apache-arrow-pour-loptimisation-du-traitement-des-donn%C3%A9es/#numpy-backend","title":"NumPy Backend","text":"<p>Avec NumPy, on stockerait ces cha\u00eenes en utilisant un tableau de taille fixe, souvent moins optimal pour les cha\u00eenes de caract\u00e8res.</p> <ul> <li>Structure :<ul> <li>Tableau de cha\u00eenes de longueur maximale, par exemple 15 caract\u00e8res.</li> <li>Chaque cha\u00eene occupe 15 espaces, m\u00eame si elle est plus courte.</li> </ul> </li> </ul> <p>Repr\u00e9sentation hypoth\u00e9tique du tableau NumPy</p> Cha\u00eene Stockage <code>\"numpy\"</code> <code>\"numpy         \"</code> <code>\"data\"</code> <code>\"data         \"</code> <code>\"analysis\"</code> <code>\"analysis    \"</code> <code>null</code> <code>\"\"</code> (ou espace vide) <code>\"performance\"</code> <code>\"performance \"</code>"},{"location":"2024/09/21/apache-arrow-pour-loptimisation-du-traitement-des-donn%C3%A9es/#apache-arrow-backend","title":"Apache Arrow Backend","text":"<p>Avec Arrow, ces cha\u00eenes sont stock\u00e9es en utilisant plusieurs buffers (un buffer est une zone m\u00e9moire temporaire utilis\u00e9e pour stocker des donn\u00e9es en attente de traitement ou de transfert).</p> <ul> <li>Validity Bitmap Buffer : <code>00011101</code></li> <li>Offsets Buffer : <code>0 5 9 17 17 28</code></li> <li>Values Buffer : <code>numpydataanalysisperformance</code></li> </ul> <p>Repr\u00e9sentation Arrow</p> Composant Valeurs Validity Bitmap Buffer <code>00011101</code> Offsets Buffer <code>0 5 9 17 17 28</code> Values Buffer <code>numpydataanalysisperformance</code> <ul> <li>Validity Bitmap Buffer : Indique l'absence du 4\u00e8me \u00e9l\u00e9ment (<code>null</code>).</li> <li>Offsets Buffer : Montre les d\u00e9buts et fins de chaque cha\u00eene dans le Values Buffer.</li> <li>Values Buffer : Contient les donn\u00e9es r\u00e9elles, stock\u00e9es de mani\u00e8re compacte.</li> </ul> <p>Cela permet une gestion plus efficace de la m\u00e9moire et am\u00e9liore les temps de traitement, notamment pour des op\u00e9rations complexes sur de gros ensembles de donn\u00e9es.</p>"},{"location":"2024/09/21/apache-arrow-pour-loptimisation-du-traitement-des-donn%C3%A9es/#conclusion","title":"Conclusion","text":"<p>Gr\u00e2ce \u00e0 Apache Arrow, les op\u00e9rations de lecture et de calcul sont optimis\u00e9es, quel que soit le format de stockage. Le passage aux formats columnar est un atout majeur, surtout pour le traitement de grandes quantit\u00e9s de donn\u00e9es.</p>"},{"location":"2024/08/18/faire-son-blog-avec-mkdocs-et-github-actions/","title":"Faire son Blog avec MkDocs et github actions","text":"<p>Dans ce billet de blog, j'essayerai de retracer  cr\u00e9er et d\u00e9ployer un blog en utilisant MkDocs, un outil qui facilite la cr\u00e9ation de belles documentations gr\u00e2ce \u00e0 des fichiers <code>markdown</code>. Nous allons couvrir chaque \u00e9tape pour t'aider \u00e0 mettre ton blog en ligne.</p>"},{"location":"2024/08/18/faire-son-blog-avec-mkdocs-et-github-actions/#ce-dont-tu-as-besoin","title":"Ce dont tu as besoin","text":"<ol> <li>Compte GitHub : Tu auras besoin d\u2019un compte GitHub pour stocker et d\u00e9ployer ton blog.</li> <li>Connaissances de Base : Une familiarit\u00e9 avec GitHub, Docker et quelques bases de la ligne de commande sera utile.</li> </ol>"},{"location":"2024/08/18/faire-son-blog-avec-mkdocs-et-github-actions/#etape-1-configuration-de-ton-blog","title":"\u00c9tape 1 : Configuration de ton blog","text":"<ol> <li> <p>Cr\u00e9er un r\u00e9pertoire gitHub :</p> <ul> <li>Va sur GitHub et connecte-toi.</li> <li>Clique sur New pour cr\u00e9er un nouveau r\u00e9pertoire.</li> <li>Nomme ton r\u00e9pertoire, par exemple <code>mon-blog</code>.</li> <li>Choisis Public ou Private selon ta pr\u00e9f\u00e9rence.</li> <li>Clique sur Create repository.</li> </ul> </li> <li> <p>Installer MkDocs localement :</p> <ul> <li>Ouvre ton terminal ou la ligne de commande.</li> <li>Installe MkDocs avec pip :    <pre><code>pip install mkdocs\n</code></pre></li> </ul> </li> <li> <p>Configurer ton blog :</p> <ul> <li>Navigue jusqu\u2019au dossier o\u00f9 tu souhaites cr\u00e9er ton blog.</li> <li>Ex\u00e9cute :    <pre><code>mkdocs new mon-blog\n</code></pre></li> <li>Cela cr\u00e9e un dossier nomm\u00e9 <code>mon-blog</code> avec les fichiers de base pour ton blog.</li> </ul> </li> <li> <p>Personnaliser Ton Blog :</p> <ul> <li>Ouvre le fichier <code>mkdocs.yml</code> dans le dossier de ton blog. C\u2019est ici que tu d\u00e9finis le nom et le th\u00e8me de ton blog.</li> <li> <p>Modifie le fichier <code>mkdocs.yml</code> pour qu\u2019il ressemble \u00e0 ceci :</p> <pre><code>site_name: Mon Blog\ntheme:\n  name: material\nnav:\n  - Accueil: index.md\n</code></pre> </li> </ul> <ul> <li>Ajoute du contenu en modifiant <code>index.md</code> ou en cr\u00e9ant de nouveaux fichiers Markdown dans le dossier <code>docs</code>.</li> </ul> </li> </ol>"},{"location":"2024/08/18/faire-son-blog-avec-mkdocs-et-github-actions/#etape-2-construire-et-deployer-avec-docker","title":"\u00c9tape 2 : Construire et d\u00e9ployer avec docker","text":"<ol> <li> <p>Cr\u00e9er une Image Docker :</p> <ul> <li>R\u00e9dige un Dockerfile pour inclure MkDocs et les outils n\u00e9cessaires.</li> <li> <p>Voici un Dockerfile de base :</p> <pre><code># Utiliser l'image Python 3.12\nFROM python:3.12-slim\n\n# D\u00e9finir le r\u00e9pertoire de travail\nWORKDIR /app\n\n# Installer MkDocs et les plugins\nRUN pip install mkdocs mkdocs-material ghp-import\n\n# Copier les fichiers du blog dans l'image Docker\nCOPY . /app\n\n# D\u00e9finir la commande pour construire MkDocs\nCMD [\"mkdocs\", \"build\", \"--verbose\", \"--site-dir\", \"site\"]\n</code></pre> </li> </ul> <ul> <li>Construis l'image Docker avec :    <pre><code>docker build -t mon-blog-image .\n</code></pre></li> </ul> </li> <li> <p>D\u00e9ployer ton blog :</p> <ul> <li> <p>Cr\u00e9e un fichier workflow GitHub Actions pour automatiser le d\u00e9ploiement. Sauvegarde le fichier suivant sous <code>.github/workflows/deploy.yml</code> dans ton r\u00e9pertoire :</p> <pre><code>name: Build and Deploy MkDocs Site\n\non:\n  push:\n    branches:\n      - main\n  pull_request:\n    branches:\n      - main\n  workflow_dispatch:\n\nenv:\n  IMAGE_NAME: mon-blog-image\n  IMAGE_TAG: latest\n\njobs:\n  build-and-deploy:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v3\n\n      - name: Build MkDocs site\n        run: |\n          docker run --rm -v ${{ github.workspace }}:/app -w /app ${{ env.IMAGE_NAME }}:${{ env.IMAGE_TAG }} mkdocs build --verbose --site-dir site\n\n      - name: Deploy to GitHub Pages\n        if: github.ref == 'refs/heads/main'\n        run: |\n          docker run --rm -v ${{ github.workspace }}:/app -w /app ${{ env.IMAGE_NAME }}:${{ env.IMAGE_TAG }} /bin/bash -c \"\n            ghp-import -n -p -f site -r https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git -b gh-pages\"\n\n      - name: Clean up Docker resources\n        run: docker system prune -f\n</code></pre> </li> </ul> </li> </ol>"},{"location":"2024/08/18/faire-son-blog-avec-mkdocs-et-github-actions/#etapes-finales","title":"\u00c9tapes finales","text":"<ol> <li> <p>Pousser tes modifications :</p> <ul> <li>Commit et pousse tes modifications vers GitHub :    <pre><code>git add .\ngit commit -m \"Configuration du site MkDocs\"\ngit push origin main\n</code></pre></li> </ul> </li> <li> <p>Assurer les permissions correctes pour le d\u00e9ploiement :</p> <ul> <li>Pour \u00e9viter les probl\u00e8mes de permission avec GitHub Actions, assure-toi que le <code>GITHUB_TOKEN</code> dispose des permissions n\u00e9cessaires.</li> <li>V\u00e9rifie les param\u00e8tres du r\u00e9pertoire :<ul> <li>Va sur ton r\u00e9pertoire GitHub.</li> <li>Navigue vers Settings &gt; Actions &gt; General.</li> <li>Sous Workflow permissions, assure-toi que Read and write permissions sont s\u00e9lectionn\u00e9es.</li> </ul> </li> </ul> </li> <li> <p>V\u00e9rifier le d\u00e9ploiement :</p> <ul> <li>Va sur ton r\u00e9pertoire GitHub.</li> <li>Navigue vers Settings &gt; Pages.</li> <li>Assure-toi que la source est d\u00e9finie sur la branche <code>gh-pages</code>.</li> </ul> <p>Ton blog devrait maintenant \u00eatre en ligne ! Visite l\u2019URL fournie dans les param\u00e8tres de GitHub Pages pour voir ton site.</p> </li> </ol>"},{"location":"2024/08/18/faire-son-blog-avec-mkdocs-et-github-actions/#conclusion","title":"Conclusion","text":"<p>F\u00e9licitations ! Tu as construit et d\u00e9ploy\u00e9 un blog en utilisant MkDocs et Docker. Ce guide vise \u00e0 simplifier le processus pour que tu puisses facilement partager ton contenu en ligne. Bon blogging !</p> <p>Pour un exemple complet, jette un \u0153il \u00e0 mon projet : R\u00e9pertoire GitHub.</p>"},{"location":"2024/08/18/faire-son-blog-avec-mkdocs-et-github-actions/#references","title":"R\u00e9f\u00e9rences:","text":"<ul> <li>https://squidfunk.github.io/mkdocs-material/getting-started/</li> </ul>"},{"location":"2024/08/18/faire-son-blog-avec-mkdocs-et-github-actions/#maj","title":"MAJ","text":"<ul> <li>08/09/2024 : Traduire en fran\u00e7ais</li> <li>14/09/2024 : Ajout de r\u00e9f\u00e9rences</li> </ul>"},{"location":"2024/09/28/faites-du-dask-plut%C3%B4t-que-du-spark-si-vous-avez-juste-de-grosses-tables-de-donn%C3%A9es/","title":"Faites du Dask plut\u00f4t que du Spark si vous avez juste de grosses tables de donn\u00e9es","text":"<p>Dans le monde actuel des donn\u00e9es, traiter de grands volumes n\u00e9cessite des solutions performantes et \u00e9volutives. Le calcul distribu\u00e9 permet de g\u00e9rer efficacement ces donn\u00e9es en les r\u00e9partissant sur plusieurs machines/cores/workers (selon votre cas). Cependant, toutes les entreprises n'ont pas besoin d'une infrastructure lourde et co\u00fbteuse telle qu'un cluster Spark. Dask, un framework Python, est une alternative l\u00e9g\u00e8re et flexible, particuli\u00e8rement adapt\u00e9e aux environnements d\u00e9j\u00e0 bas\u00e9s sur Python.</p> <p>Ce billet de blog explore pourquoi Dask est une excellente option pour les data scientists qui manipulent de gros volumes de donn\u00e9es, sans pour autant entrer dans la cat\u00e9gorie du Big Data.</p>"},{"location":"2024/09/28/faites-du-dask-plut%C3%B4t-que-du-spark-si-vous-avez-juste-de-grosses-tables-de-donn%C3%A9es/#prerequis","title":"Pr\u00e9requis","text":"<p>Si vous ne les avez pas d\u00e9j\u00e0 install\u00e9s, vous aurez besoin des paquets suivants : <pre><code>pip install dask[distributed]\npip install \"bokeh&gt;=3.1.0\"  # Pour visualiser le tableau de bord du cluster\n</code></pre></p>"},{"location":"2024/09/28/faites-du-dask-plut%C3%B4t-que-du-spark-si-vous-avez-juste-de-grosses-tables-de-donn%C3%A9es/#1-donnees-volumineuses-et-le-besoin-de-calcul-distribue","title":"1. Donn\u00e9es volumineuses et le besoin de calcul distribu\u00e9","text":"<p>Lorsque le volume de donn\u00e9es devient trop important pour \u00eatre trait\u00e9 en m\u00e9moire sur une seule machine, les performances diminuent rapidement. Des t\u00e2ches courantes comme l'entra\u00eenement de mod\u00e8les de machine learning, l'analyse de donn\u00e9es, ou encore la cr\u00e9ation de visualisations interactives peuvent \u00eatre ralenties. \u00c0 ce stade, le besoin de calcul distribu\u00e9 devient critique. Le principe est simple : r\u00e9partir la charge de travail sur plusieurs machines pour parall\u00e9liser les calculs, et ainsi r\u00e9duire consid\u00e9rablement le temps de traitement. Lorsqu'on ne poss\u00e8de pas de serveur, il s'agit de parall\u00e9liser sur le nombre de c\u0153urs du PC.</p>"},{"location":"2024/09/28/faites-du-dask-plut%C3%B4t-que-du-spark-si-vous-avez-juste-de-grosses-tables-de-donn%C3%A9es/#2-deux-frameworks-de-calcul-distribue-spark-et-dask","title":"2. Deux frameworks de calcul distribu\u00e9 : Spark et Dask","text":"<p>Pour r\u00e9pondre \u00e0 ces d\u00e9fis, deux frameworks majeurs sont souvent utilis\u00e9s : Spark et Dask. Alors que Spark a gagn\u00e9 en popularit\u00e9 dans les infrastructures Big Data, Dask est particuli\u00e8rement appr\u00e9ci\u00e9 dans les environnements Python. Voici un tableau comparatif pour mieux comprendre leurs diff\u00e9rences :</p> Caract\u00e9ristiques Spark Dask Langage de programmation Majoritairement Scala et Java Python API de haut niveau DataFrame API Dask DataFrame Scalabilit\u00e9 Hautement scalable, con\u00e7u pour le Big Data Scalabilit\u00e9 adapt\u00e9e aux workflows Python Type de cluster Clusters distribu\u00e9s (Hadoop, Kubernetes) Clusters distribu\u00e9s ou local sur une machine \u00c9cosyst\u00e8me Int\u00e9gr\u00e9 avec des outils Big Data (HDFS, Hive) Int\u00e9gr\u00e9 avec NumPy, Pandas, Scikit-learn Simplicit\u00e9 d'installation Plus complexe, n\u00e9cessite souvent un cluster Hadoop Plus simple, peut \u00eatre ex\u00e9cut\u00e9 localement Machine Learning Biblioth\u00e8que int\u00e9gr\u00e9e (MLlib) Int\u00e9gr\u00e9 avec Scikit-learn, XGBoost, etc. Taille des donn\u00e9es Con\u00e7u pour des p\u00e9taoctets Convient pour des volumes moyens \u00e0 grands Maturit\u00e9 Plus mature, utilis\u00e9 \u00e0 grande \u00e9chelle Moins mature, mais en pleine expansion Co\u00fbt Tr\u00e8s cher \u00e0 mettre en place un cluster spark Pas besoin de cluster d\u00e9di\u00e9 pour le faire tourner efficacement"},{"location":"2024/09/28/faites-du-dask-plut%C3%B4t-que-du-spark-si-vous-avez-juste-de-grosses-tables-de-donn%C3%A9es/#3-attention-tout-nest-pas-du-big-data","title":"3. Attention, tout n'est pas du Big Data","text":"<p>Il ne faut pas confondre donn\u00e9es volumineuses et Big Data. Beaucoup d'entreprises ont de gros ensembles de donn\u00e9es, mais cela ne suffit pas pour dire qu'elles font du Big Data. Pour m\u00e9riter ce titre, il faut r\u00e9pondre aux crit\u00e8res des 5V :</p> <ul> <li>Volume : Une quantit\u00e9 massive de donn\u00e9es.</li> <li>V\u00e9locit\u00e9 : La vitesse \u00e0 laquelle ces donn\u00e9es sont g\u00e9n\u00e9r\u00e9es.</li> <li>Vari\u00e9t\u00e9 : Diff\u00e9rents types de donn\u00e9es (structur\u00e9es, non structur\u00e9es, semi-structur\u00e9es).</li> <li>V\u00e9racit\u00e9 : La qualit\u00e9 et la fiabilit\u00e9 des donn\u00e9es.</li> <li>Valeur : Les insights et b\u00e9n\u00e9fices que l'on peut tirer de ces donn\u00e9es.</li> </ul> <p>Les g\u00e9ants comme Google, Amazon, et Netflix op\u00e8rent v\u00e9ritablement dans le domaine du Big Data. Ils g\u00e8rent des volumes immenses, des flux continus de donn\u00e9es, et disposent d'une infrastructure optimis\u00e9e pour tout ce traitement de masse. Cependant, si une entreprise a quelques dizaines de millions de lignes avec une vingtaine de colonnes, elle n\u2019a pas n\u00e9cessairement besoin d\u2019un cluster Spark. Souvent, une solution comme Dask, plus l\u00e9g\u00e8re et flexible, est beaucoup mieux adapt\u00e9e \u00e0 ses besoins.</p> <p>Vous avez compris d\u00e9j\u00e0. Dans ce billet de blog, je ne veux pas vous convaincre d'utiliser Spark. Je vous presenterai plut\u00f4t Dask. Ce dernier est compos\u00e9 de plusieurs API :</p> <ul> <li>Arrays (s\u2019appuie sur NumPy)</li> <li>DataFrames (repose sur Pandas)</li> <li>Bag (suit map/filter/groupby/reduce)</li> <li>Dask-ML (fonctionne avec Scikit-Learn)</li> <li>Delayed (couvre de mani\u00e8re g\u00e9n\u00e9rale Python)</li> </ul> <p>Puisque nous sommes en data science, on parlera de dataframe et de Dask-ML. Pour plus de r\u00e9f\u00e9rences, consultez les liens dans les r\u00e9f\u00e9rences.</p>"},{"location":"2024/09/28/faites-du-dask-plut%C3%B4t-que-du-spark-si-vous-avez-juste-de-grosses-tables-de-donn%C3%A9es/#4-le-cluster-dask","title":"4. Le Cluster Dask","text":"<p>Dask est un outil hyper flexible qui s'adapte aussi bien sur ta machine locale que sur un cluster distribu\u00e9. Cela signifie que tu peux commencer avec une configuration l\u00e9g\u00e8re et \u00e9voluer au fur et \u00e0 mesure que tes besoins augmentent. Le noyau de sa magie r\u00e9side dans le Dask Scheduler, qui g\u00e8re l\u2019ex\u00e9cution des calculs de mani\u00e8re dynamique, m\u00eame sur plusieurs machines.</p> <p>Les clusters Dask sont faciles \u00e0 d\u00e9ployer sur des services comme Kubernetes, AWS, ou m\u00eame avec un simple SSH sur des machines distantes. Compar\u00e9 \u00e0 des solutions plus lourdes comme Spark, cette flexibilit\u00e9 aide \u00e0 r\u00e9duire les co\u00fbts d\u2019infrastructure. Dans mon projet, je l'ex\u00e9cute en local, mais ce n\u2019est pas une obligation. Cela permet de suivre de mani\u00e8re efficace les ressources utilis\u00e9es.</p> <p>Lorsque tu cr\u00e9es ton client Dask, un lien vers le tableau de bord s'affiche. Je te conseille de le garder ouvert d\u2019un c\u00f4t\u00e9 de ton \u00e9cran pendant que tu travailles sur ton notebook, m\u00eame si c\u2019est parfois un peu compliqu\u00e9 de jongler avec les fen\u00eatres. C\u2019est extr\u00eamement utile lorsque tu commences \u00e0 apprendre \u00e0 utiliser Dask.</p> <p>Voici comment cela fonctionne en local : </p> <p>On peut voir comment le client est configur\u00e9 et o\u00f9 le dashboard tourne. Je recommande d\u2019y jeter un \u0153il pour voir comment les workers ex\u00e9cutent ton code en coulisses. </p> <p>Encore une fois, ce n\u2019est pas obligatoire, mais c\u2019est surtout pratique pour suivre les t\u00e2ches, particuli\u00e8rement lorsqu'on veut monitorer des processus ETL.</p>"},{"location":"2024/09/28/faites-du-dask-plut%C3%B4t-que-du-spark-si-vous-avez-juste-de-grosses-tables-de-donn%C3%A9es/#5-composant-dataframes-dask","title":"5. Composant DataFrames Dask","text":"<p>Le Dask DataFrame est la version parall\u00e8le du c\u00e9l\u00e8bre Pandas DataFrame. Donc, si tu ma\u00eetrises d\u00e9j\u00e0 Pandas, tu n'auras besoin que de petits ajustements pour te mettre \u00e0 Dask. Ce qui est cool, c\u2019est qu\u2019il est con\u00e7u pour g\u00e9rer des datasets plus gros que ce que Pandas peut traiter en m\u00e9moire, tout en gardant une interface famili\u00e8re.</p> <p>Pandas charge tout en m\u00e9moire, ce qui n\u2019est pas toujours id\u00e9al. Dask, en revanche, divise tes donn\u00e9es en partitions, trait\u00e9es en parall\u00e8le. R\u00e9sultat, tu peux travailler avec des datasets plus volumineux que ta RAM sans trop de stress, tout en continuant d'utiliser des op\u00e9rations classiques comme groupby, merge, ou apply. Un DataFrame Dask est essentiellement un ensemble de DataFrames Pandas.</p> <p></p> <p>Une Petite Histoire...</p> <p>Pour l'anecdote, une amie statisticienne se battait avec un fichier CSV de 7 gigas sur son PC avec 64 gigas de RAM. Elle souhaitait pr\u00e9parer ses donn\u00e9es pour de la mod\u00e9lisation statistique, mais le traitement prenait 4 heures. Avec Dask, nous avons pu r\u00e9duire ce temps \u00e0... 19 minutes ! Elle \u00e9tait abasourdie. La cl\u00e9 \u00e9tait d'utiliser Dask pour son ETL, puis de sauvegarder sa table interm\u00e9diaire en format Parquet. Avec suffisamment de RAM et de c\u0153urs, nous avons pu ex\u00e9cuter tout cela directement sur son PC.</p> <p>Regarde mon code ci-dessous : </p> <p>Comme tu vois, c\u2019est du Pandas classique, non ? La seule diff\u00e9rence, c\u2019est que les r\u00e9sultats ne s'affichent qu'apr\u00e8s avoir appel\u00e9 la m\u00e9thode <code>.compute()</code>. Pourquoi ? Parce que Dask fonctionne en mode \"lazy execution\". Il pr\u00e9pare toutes les op\u00e9rations, et c\u2019est seulement quand tu appelles <code>.compute()</code> qu\u2019il ex\u00e9cute tout d'un coup, optimisant ainsi le processus.</p> <p>Ce calcul \u00e9tait dix fois plus rapide qu\u2019avec du Pandas classique. Impressionnant, non ? Si tu veux en savoir plus, jette un \u0153il ici : Dask DataFrame Examples.</p>"},{"location":"2024/09/28/faites-du-dask-plut%C3%B4t-que-du-spark-si-vous-avez-juste-de-grosses-tables-de-donn%C3%A9es/#6-composant-dask-ml","title":"6. Composant Dask-ML","text":"<p>Dask ne propose pas de biblioth\u00e8que int\u00e9gr\u00e9e comme MLlib dans Spark, mais il s\u2019int\u00e8gre parfaitement avec les biblioth\u00e8ques populaires de machine learning de l\u2019\u00e9cosyst\u00e8me Python, telles que Scikit-learn, XGBoost, et TensorFlow. Gr\u00e2ce au module Dask-ML, tu peux \u00e9tendre l\u2019entra\u00eenement de mod\u00e8les sur plusieurs machines, ce qui facilite le traitement de datasets volumineux sans forc\u00e9ment recourir \u00e0 des solutions lourdes.</p> <p>Dans un environnement classique, Scikit-learn utilise <code>joblib</code> pour le parall\u00e9lisme sur une seule machine. Cela permet d'entra\u00eener la plupart des estimateurs (ceux qui acceptent un param\u00e8tre <code>n_jobs</code>) en utilisant tous les c\u0153urs de ton ordinateur portable ou de ta station de travail. Mais avec Dask, tu peux aller plus loin.</p> <p>Dask te permet de parall\u00e9liser ton code Scikit-learn sur un cluster sans avoir \u00e0 modifier significativement ton code existant. Par exemple, des fonctions comme GridSearchCV ou RandomizedSearchCV, tr\u00e8s utilis\u00e9es pour la recherche d\u2019hyperparam\u00e8tres, peuvent \u00eatre parall\u00e9lis\u00e9es avec Dask. R\u00e9sultat : tu gagnes en efficacit\u00e9 tout en gardant le m\u00eame code que celui que tu ex\u00e9cuterais localement.</p> <p>Voici un exemple avec XGBoost et Dask : <pre><code>import dask_xgboost as dxgb\nimport dask.dataframe as dd\nfrom dask_ml.model_selection import train_test_split\n\nparams = {'objective': 'binary:logistic', 'max_depth': 4, \n          'eta': 0.01, 'subsample': 0.5, 'min_child_weight': 0.5}\n\ndf = dd.read_csv('data*.csv')\n\nX = df.drop('target', axis=1)\ny = df['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n\nbst = dxgb.train(client, params, X_train, y_train, num_boost_round=10)\n\ny_hat = dxgb.predict(client, bst, X_test).persist()\n</code></pre></p> <p>Ce qui est int\u00e9ressant, c\u2019est que tout ce code peut tourner sur un cluster distribu\u00e9, ce qui booste la vitesse et la performance lorsque tu traites de gros datasets. Plus besoin de se battre contre les limites d'une seule machine.</p> <p>Dask-ML offre aussi des extensions pour distribuer les mod\u00e8les sur plusieurs machines. Que tu utilises Scikit-learn, TensorFlow, ou XGBoost, tu peux profiter de cette int\u00e9gration fluide pour acc\u00e9l\u00e9rer ton travail de data science. La flexibilit\u00e9 est vraiment l\u2019atout majeur de Dask, surtout si tu \u00e9volues d\u00e9j\u00e0 dans un environnement Python.</p> <p>Pour plus de d\u00e9tails et d'exemples sur l'utilisation de Dask avec le machine learning, consulte le guide officiel : Dask XGBoost Example.</p>"},{"location":"2024/09/28/faites-du-dask-plut%C3%B4t-que-du-spark-si-vous-avez-juste-de-grosses-tables-de-donn%C3%A9es/#conclusion","title":"Conclusion","text":"<p>En conclusion, Dask est un excellent choix pour les entreprises souhaitant int\u00e9grer le calcul distribu\u00e9 dans leur workflow de data science, tout en restant dans un \u00e9cosyst\u00e8me Python. Pour celles qui ne font pas du v\u00e9ritable Big Data et qui n'ont pas besoin de l'infrastructure massive d'un cluster Spark, Dask repr\u00e9sente une solution plus l\u00e9g\u00e8re, facile \u00e0 d\u00e9ployer, et suffisamment puissante pour traiter de grandes quantit\u00e9s de donn\u00e9es dans un environnement distribu\u00e9.</p>"},{"location":"2024/09/28/faites-du-dask-plut%C3%B4t-que-du-spark-si-vous-avez-juste-de-grosses-tables-de-donn%C3%A9es/#references","title":"R\u00e9f\u00e9rences","text":"<ul> <li>Dask Overview</li> <li>Parallel Computing with Dask</li> <li>Dask XGBoost Example</li> <li>From Pandas to Dask</li> </ul>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/","title":"Encodage efficace des variables cat\u00e9gorielles pour du ML","text":"<p>Les variables cat\u00e9gorielles, on les croise partout dans nos datasets, mais les algorithmes de machine learning, eux, pr\u00e9f\u00e8rent les chiffres. Dans ce billet, nous allons explorer plusieurs techniques d'encodage pour transformer ces variables, le tout agr\u00e9ment\u00e9 d'explications claires, de formulations math\u00e9matiques, et quelques exemples pratiques. Nous aborderons aussi les avantages et les limites de chaque technique, que ce soit les \"Classic Encoders\", le \"Contrast Encoder\", ou les \"Bayesian Encoders\".</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#prerequis","title":"Pr\u00e9requis","text":"<p>Assurez-vous d'avoir pandas, scikit-learn, et category_encoders install\u00e9s.</p> <p>Pour illustrer nos exemples, voici un petit jeu de donn\u00e9es :</p> x1 x2 x3 x4 y 5.6 3.4 1.5 B 0 7.8 2.7 6.9 A 1 4.9 3.1 1.5 A 0 6.4 3.2 5.3 C 1 5.1 3.8 1.6 A 0 6.0 2.9 4.5 B 0 6.5 3.0 5.8 C 1 5.3 3.7 1.5 A 0 7.1 3.0 5.9 B 0 5.9 3.0 5.1 C 1 <p>Pour mod\u00e9liser cette table de donn\u00e9es, il nous faut transformer la colonne x4 en variable num\u00e9rique. En gros, nous allons discuter des diff\u00e9rentes approches qui s'offrent \u00e0 nous.</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#i-classic-encoders","title":"I. Classic Encoders","text":""},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#1-label-encoding","title":"1. Label Encoding","text":""},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#description","title":"Description","text":"<p>L'encodage par \u00e9tiquette attribue un entier unique \u00e0 chaque cat\u00e9gorie d'une variable cat\u00e9gorielle. Cependant, attention ! Cette m\u00e9thode peut insuffler une notion d'ordre qui pourrait \u00eatre inappropri\u00e9e pour des donn\u00e9es purement nominales.</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#expression-mathematique","title":"Expression Math\u00e9matique","text":"<p>Pour des cat\u00e9gories \\(C_1, C_2, \\ldots, C_n\\), l'encodage se fait comme suit : $$ \\text{Valeur Encod\u00e9e} = \\text{index}(C_i) \\quad \\text{pour} \\; i = 1, 2, \\ldots, n $$ o\u00f9 \\(\\text{index}(C_i)\\) repr\u00e9sente un entier unique associ\u00e9 \u00e0 chaque cat\u00e9gorie.</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#pratiquement","title":"Pratiquement","text":"<p>Voici un petit extrait de code :</p> <p><pre><code>from sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\ndata[\"x4_LabelEncoder\"] = encoder.fit_transform(data[\"x4\"])\ndata.head()\n</code></pre> </p> <p>N\u2019oublions pas l\u2019inconv\u00e9nient : cela peut introduire des valeurs qui n\u2019ont pas de sens statistique.</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#2-ordinal-encoder","title":"2. Ordinal Encoder","text":""},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#description_1","title":"Description","text":"<p>Il arrive que certaines cat\u00e9gories aient un sens d'ordre. Dans ce cas, un Label Encoder ne sera pas tr\u00e8s utile et pourrait m\u00eame causer des dommages dans les donn\u00e9es. L'encodage ordinal attribue aussi un entier unique \u00e0 chaque cat\u00e9gorie, mais cela se fait lorsque les cat\u00e9gories ont un ordre naturel. Pensez \u00e0 des cat\u00e9gories telles que faible, moyen, et \u00e9lev\u00e9 ; cet ordre doit \u00eatre respect\u00e9.</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#mathematiquement","title":"Math\u00e9matiquement","text":"<p>Pour des cat\u00e9gories ordonn\u00e9es \\(C_1, C_2, \\ldots, C_n\\) o\u00f9 l'ordre naturel est \\(C_1 &lt; C_2 &lt; \\ldots &lt; C_n\\), l'encodage ordinal se fait par :  $$  \\text{Valeur Encod\u00e9e} = \\text{position}(C_i) \\quad \\text{pour} \\; i = 1, 2, \\ldots, n  $$ </p> <p>o\u00f9 : </p> <ul> <li>\\(\\text{position}(C_i)\\) repr\u00e9sente la position ordinale de la cat\u00e9gorie \\(C_i\\) dans l'ordre naturel. Si \\(C_1\\) est la premi\u00e8re, alors \\(\\text{position}(C_1) = 1\\) et ainsi de suite.</li> </ul>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#pratiquement_1","title":"Pratiquement","text":"<p>Pour une variable cat\u00e9gorielle x4 repr\u00e9sentant \"Niveau de risque\", avec les cat\u00e9gories suivantes :</p> <ul> <li>C \u2192 Faible </li> <li>B \u2192 Moyen </li> <li>A \u2192 \u00c9lev\u00e9 </li> </ul> <p>Si l'ordre naturel est Faible &lt; Moyen &lt; \u00c9lev\u00e9, alors l'encodage ordinal sera : </p> <ul> <li>C \u2192 0 </li> <li>B \u2192 1 </li> <li>A \u2192 2</li> </ul> <p><pre><code>from sklearn.preprocessing import OrdinalEncoder\nencoder = OrdinalEncoder(categories=[['C', 'B', 'A']])\ndata[\"x4_OrdinalEncoder\"] = encoder.fit_transform(data[[\"x4\"]])\ndata.head()\n</code></pre> </p> <p>Cette m\u00e9thode pr\u00e9serve l'ordre des cat\u00e9gories, crucial pour certaines analyses statistiques. En revanche, il faut s'assurer que cet ordre est bien d\u00e9fini dans le code <code>OrdinalEncoder(categories=[['C', 'B', 'A']])</code>.</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#3-one-hot-encoder","title":"3. One-Hot Encoder","text":""},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#description_2","title":"Description","text":"<p>Ce proc\u00e9d\u00e9 cr\u00e9e des colonnes binaires, ou indicatrices, pour chaque cat\u00e9gorie. Pour chaque observation, la colonne correspondant \u00e0 la cat\u00e9gorie pr\u00e9sente prend la valeur 1, et les autres sont \u00e0 0.</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#mathematiquement_1","title":"Math\u00e9matiquement","text":"<p>Soit \\(C = \\{ C_1, C_2, ..., C_n \\}\\) les cat\u00e9gories d'une variable. Une observation appartenant \u00e0 \\(C_i\\) est repr\u00e9sent\u00e9e par : $$ \\mathbf{x} = [0, 0, \\ldots, 1, \\ldots, 0] \\quad \\text{o\u00f9 la position } i \\text{ est \u00e0 1} $$</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#pratiquement_2","title":"Pratiquement","text":"<p>Il existe plusieurs outils, mais restons simples avec la m\u00e9thode <code>get_dummies</code> de pandas, que je trouve bien pratique.</p> <pre><code>import pandas as pd\ndata = pd.get_dummies(data, columns=[\"x4\"], drop_first=False, prefix=\"x4_OneHotEncoder\", dtype=int)\ndata.head()\n</code></pre> <p></p> <p>On peut jouer avec pas mal de param\u00e8tres. Pour les mod\u00e8les statistiques, on a souvent tendance \u00e0 fixer <code>drop_first=True</code> afin d'\u00e9viter le probl\u00e8me de colin\u00e9arit\u00e9 parfaite. Vous l'avez vu, on a transform\u00e9 la variable x4 en plusieurs nouvelles caract\u00e9ristiques. Cela peut poser probl\u00e8me si on a un grand nombre de cat\u00e9gories, ce qui pourrait mener \u00e0 des matrices creuses. Dans une situation de ML training, cela peut entra\u00eener du surapprentissage. Parfois, une s\u00e9lection de caract\u00e9ristiques devient alors in\u00e9vitable.</p> <p>https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#autres-encodeurs","title":"Autres encodeurs","text":"<p>Je vous encourage aussi \u00e0 jeter un \u0153il sur deux encodeurs int\u00e9ressants : - Hashing encoder  - Count encoder</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#ii-contrast-encoder","title":"II. Contrast Encoder","text":"<p>Les encodeurs de contraste transforment les variables cat\u00e9gorielles en format num\u00e9rique en cr\u00e9ant des codes de contraste qui permettent aux algorithmes d'interpr\u00e9ter efficacement les resultats des mod\u00e8les de regression. Voici quelques m\u00e9thodes courantes pour encoder les contrastes :</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#1-sum-encoder","title":"1. Sum Encoder","text":""},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#description_3","title":"Description","text":"<p>Cette m\u00e9thode encode les variables de mani\u00e8re \u00e0 ce que la somme des vecteurs encod\u00e9s soit \u00e9gale \u00e0 z\u00e9ro, \u00e9vitant ainsi la multicolin\u00e9arit\u00e9. Dans un mod\u00e8le One Hot Encoding, on doit supprimer une cat\u00e9gorie et la garder comme r\u00e9f\u00e9rence. Ainsi :</p> <ul> <li>Dans ce mod\u00e8le, l'intercept repr\u00e9sente la moyenne de la condition de r\u00e9f\u00e9rence.</li> </ul> <ul> <li>Les coefficients repr\u00e9sentent les effets simples, c'est-\u00e0-dire la diff\u00e9rence entre une condition particuli\u00e8re et la condition de r\u00e9f\u00e9rence.</li> </ul> <p>Cela n'est pas toujours du go\u00fbt des statisticiens ! Ils ont donc introduit l'encodage par somme. Dans les mod\u00e8les de r\u00e9gression :</p> <ul> <li>L'intercept repr\u00e9sente la moyenne g\u00e9n\u00e9rale du target \u00e0 travers toutes les conditions.</li> </ul> <ul> <li>Les coefficients des cat\u00e9gories sont alors interpr\u00e9t\u00e9s comme la variation de la moyenne du target pour chaque cat\u00e9gorie par rapport \u00e0 cette moyenne g\u00e9n\u00e9rale.</li> </ul>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#mathematiquement_2","title":"Math\u00e9matiquement","text":"<p>Pour des cat\u00e9gories \\(C = \\{ C_1, C_2, \\ldots, C_n \\}\\), si nous choisissons \\(C_k\\) comme cat\u00e9gorie de r\u00e9f\u00e9rence, une observation appartenant \u00e0 \\(C_i\\) (o\u00f9 $ i \\neq k $) se repr\u00e9sente par :</p> \\[ \\mathbf{x} = \\begin{bmatrix} 1 &amp; 0 &amp; \\ldots &amp; -1 &amp; -1 \\end{bmatrix} \\] <p>o\u00f9 les valeurs sont : -  \\(1\\) pour la cat\u00e9gorie \\(C_1\\)  -  \\(1\\) pour la cat\u00e9gorie \\(C_2\\)  -  \\(-1\\) pour la cat\u00e9gorie de r\u00e9f\u00e9rence \\(C_k\\)  -  \\(0\\) pour les autres cat\u00e9gories.</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#pratiquement_3","title":"Pratiquement","text":"<p>Pour appliquer l'encodage Sum avec Pandas, on pourrait le faire directement, mais je vous conseille d'utiliser le package category_encoders, notamment la classe SumEncoder.</p> <p><pre><code>from category_encoders.sum_coding import SumEncoder\nSE_encoder = SumEncoder(drop_invariant=True)\nSE_encoder.fit_transform(data).head()\n</code></pre> </p> <p>Premi\u00e8re remarque : il n\u2019y a pas de cat\u00e9gorie de r\u00e9f\u00e9rence, car par d\u00e9faut, c\u2019est la derni\u00e8re par ordre alphab\u00e9tique. On ne peut pas choisir la cat\u00e9gorie de r\u00e9f\u00e9rence directement ici, mais une fois que l\u2019on a compris le principe, on peut s\u2019en charger par nous-m\u00eames. Pour obtenir les coefficients de la cat\u00e9gorie de r\u00e9f\u00e9rence, il suffit de prendre -1 et -1 pour <code>x4_0</code> et <code>x4_1</code>.</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#2-helmert-coding","title":"2. Helmert Coding","text":"<p>Pour plus de d\u00e9tails, consultez ce lien.</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#iii-bayesian-target-encoders","title":"III. Bayesian Target Encoders","text":"<p>Les methodes class\u00e9es comme Bayesiennes  sont des  technique utile pour encoder les variables cat\u00e9gorielles en tenant compte de la distribution du target. Cette approche int\u00e8gre des informations a priori sur la variable cible, ce qui la rend particuli\u00e8rement efficace pour am\u00e9liorer la performance des mod\u00e8les d'apprentissage automatique. Leurs Caract\u00e9ristiques cl\u00e9s sont les suivantes.</p> <ol> <li> <p>Cadre Bay\u00e9sien : Cette m\u00e9thode recourt \u00e0 l\u2019approche bay\u00e9sienne pour estimer la moyenne du target pour chaque cat\u00e9gorie tout en consid\u00e9rant les informations provenant de l'ensemble de donn\u00e9es global. Cela aide \u00e0 att\u00e9nuer les soucis li\u00e9s au surajustement, surtout quand les cat\u00e9gories ont peu d'observations.</p> </li> <li> <p>R\u00e9duction (Shrinkage) : L'encodage cible bay\u00e9sien applique une technique de r\u00e9duction, o\u00f9 la moyenne de la cat\u00e9gorie est ajust\u00e9e vers la moyenne g\u00e9n\u00e9rale du target, rendant l'encoding plus robuste.</p> </li> <li> <p>Gestion des Donn\u00e9es Manquantes : Cette m\u00e9thode s\u2019accommode bien des donn\u00e9es manquantes dans la caract\u00e9ristique cat\u00e9gorique en fournissant une estimation significative bas\u00e9e sur les donn\u00e9es accessibles.</p> </li> <li> <p>Cas d'Utilisation : L'encoding cible bay\u00e9sien est particuli\u00e8rement pr\u00e9conis\u00e9 pour les variables cat\u00e9gorielles \u00e0 forte cardinalit\u00e9, o\u00f9 l'encoding one-hot entra\u00eenerait trop de caract\u00e9ristiques.</p> </li> </ol> <p>Avant d\u2019explorer les formules, voici quelques notations cruciales :</p> <ul> <li>\\(y\\) et \\(y^+\\) : Le nombre total d'observations et le nombre total d'observations positives (o\u00f9 \\(y = 1\\)).</li> <li>\\(x_i, y_i\\) : La valeur de la cat\u00e9gorie et du target pour l'observation $ i $.</li> <li>\\(n\\) et \\(n^+\\) : Le nombre d'observations et le nombre d'observations positives pour une valeur sp\u00e9cifique d'une colonne cat\u00e9gorielle.</li> <li>\\(a\\) : Un hyperparam\u00e8tre de r\u00e9gularisation.</li> <li>\\(prior\\) : La valeur moyenne du target sur l'ensemble du dataset.</li> <li>\\(x^k_i\\) est la valeur encod\u00e9e pour l'observation \\(i\\) de la cat\u00e9gorie \\(k\\)</li> </ul>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#1-target-encoder","title":"1. Target encoder","text":"<p>Le target encoder est une technique de transformation de variables cat\u00e9gorielles fond\u00e9e sur la variable cible, souvent utilis\u00e9e dans les mod\u00e8les de machine learning supervis\u00e9. L'id\u00e9e est de remplacer chaque cat\u00e9gorie par une valeur calcul\u00e9e \u00e0 partir de la moyenne du target, avec un m\u00e9canisme de lissage pour pr\u00e9venir le surajustement.</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#mathematiquement_3","title":"Math\u00e9matiquement","text":"<ol> <li> <p>Calcul du Param\u00e8tre de lissage (\\(s\\))</p> <p>Le param\u00e8tre de lissage est utilis\u00e9 pour \u00e9quilibrer la contribution entre la moyenne g\u00e9n\u00e9rale (prior) et la moyenne par cat\u00e9gorie :  $$  s = \\frac{1}{1 + \\exp\\left(-\\frac{n - mdl}{a}\\right)}  $$</p> <p>o\u00f9 :     - \\(mdl\\) est la valeur minimale de donn\u00e9es par feuille,</p> </li> <li> <p>Calcul de la valeur encod\u00e9e ($ \\hat{x}^k $)</p> <p>La valeur encod\u00e9e pour chaque cat\u00e9gorie $ k $ est donn\u00e9e par :  $$  \\hat{x}^k = prior \\cdot (1 - s) + s \\cdot \\frac{n^+}{n}  $$</p> <p>o\u00f9 :     - \\(s\\) est le param\u00e8tre de lissage calcul\u00e9,     - \\(\\frac{n^+}{n}\\) est la moyenne des cibles positives pour la cat\u00e9gorie \\(k\\).</p> </li> </ol>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#pratiquement_4","title":"Pratiquement","text":"<p>On utilisera encore le package category_encoders, avec les valeurs par d\u00e9faut : </p> <p><pre><code>from category_encoders import TargetEncoder\nencoder = TargetEncoder()\nencoder.fit_transform(data.drop(columns=[\"y\"]), data[\"y\"]).head()\n</code></pre> </p> Avantages Inconv\u00e9nient Capture la relation avec le target : Directement int\u00e9gr\u00e9e, permettant d'am\u00e9liorer la performance. Surajustement potentiel : Surtout pour les cat\u00e9gories avec peu de donn\u00e9es. R\u00e9duit la dimensionnalit\u00e9 : \u00c9vite une explosion de dimensions compar\u00e9 \u00e0 l'encoding one-hot. Le m\u00e9canisme de r\u00e9gularisation aide, mais n\u00e9cessite un bon ajustement des hyperparam\u00e8tres pour \u00e9viter le surapprentissage. G\u00e8re les cat\u00e9gories rares : Le lissage minimise le risque de surajustement pour les valeurs peu fr\u00e9quentes. Facile \u00e0 interpr\u00e9ter : Les valeurs encod\u00e9es refl\u00e8tent des probabilit\u00e9s moyennes pond\u00e9r\u00e9es, simplifiant l'analyse."},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#2-m-estimate-coding","title":"2. M-Estimate coding","text":"<p>M-Estimate encoder est une version simplifi\u00e9e du target encoder qui a un seul param\u00e8tre de lissage, ce qui facilite sa mise en place et son ajustement. Con\u00e7u pour estimer la probabilit\u00e9 d'appartenance \u00e0 une cat\u00e9gorie en utilisant une moyenne pond\u00e9r\u00e9e.</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#description_4","title":"Description","text":"<p>M-Estimate coding est une technique simplifi\u00e9e qui utilise un seul param\u00e8tre de lissage, facilitant son adaptation. Il est destin\u00e9 \u00e0 estimer la probabilit\u00e9 d'appartenance \u00e0 une cat\u00e9gorie en s'appuyant sur une moyenne pond\u00e9r\u00e9e.</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#mathematiquement_4","title":"Math\u00e9matiquement","text":"<p>La formule de  M-Estimate coding est :</p> \\[ \\hat{x}^k = \\frac{n^+ + \\text{prior} \\cdot m}{n + m} \\] <p>o\u00f9 :    - \\(m\\) : param\u00e8tre de lissage.</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#pratiquement_5","title":"Pratiquement","text":"<p>Voici comment on peut impl\u00e9menter ce type d'encoding en Python :</p> <p><pre><code>from category_encoders import MEstimateEncoder\nencoder = MEstimateEncoder()\nencoder.fit_transform(data.drop(columns=[\"y\"]), data[\"y\"]).head()\n</code></pre> </p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#avantages-et-inconvenients","title":"Avantages et inconv\u00e9nients","text":"Avantages Inconv\u00e9nients Simple et efficace : Un seul param\u00e8tre de lissage \u00e0 ajuster. R\u00e9gularisation limit\u00e9e : Moins flexible que le target encoder classique. R\u00e9duction du surajustement : Le param\u00e8tre $ m $ stabilise les valeurs, r\u00e9duisant l'impact des cat\u00e9gories rares. Pas id\u00e9al pour les cibles cat\u00e9gorielles multiples : Un wrapper polynomial est n\u00e9cessaire, complexifiant la m\u00e9thode. Performance \u00e9lev\u00e9e : Pratique \u00e0 impl\u00e9menter et efficace pour les cibles binaires et continues."},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#3-leave-one-out-encoder","title":"3. Leave-One-Out encoder","text":"<p>Le Leave-One-Out encoder (LOO) est une autre m\u00e9thode tir\u00e9e de target encoder, mais avec une variation importante pour minimiser la fuite d'information.</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#description_5","title":"Description","text":"<p>L'id\u00e9e est de calculer la moyenne du target pour chaque cat\u00e9gorie, mais sans inclure l'observation actuelle. Cela aide \u00e0 limiter la fuite d'information puisque la valeur cible de l'observation en cours n'est pas int\u00e9gr\u00e9e dans sa propre transformation.</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#mathematiquement_5","title":"Math\u00e9matiquement","text":"<p>Le calcul se fait en deux etapes.</p> <ol> <li> <p>Calcul de la moyenne du target en excluant l'observation actuelle</p> <p>Pour chaque observation $ i $ appartenant \u00e0 la cat\u00e9gorie $ k $, la moyenne est calcul\u00e9e sans l\u2019observation en cours par :  $$  x^k_i = \\frac{\\sum_{j \\neq i} (y_j \\cdot (x_j == k)) - y_i}{\\sum_{j \\neq i} (x_j == k)}  $$</p> <p>En excluant $ y_i $, on \u00e9vite que le mod\u00e8le \"voit\" sa propre valeur cible, ce qui r\u00e9duit le risque de surapprentissage.</p> </li> <li> <p>Encodage des donn\u00e9es de test</p> <p>Pour les donn\u00e9es de test, chaque cat\u00e9gorie est remplac\u00e9e par la moyenne du target calcul\u00e9e sur l'ensemble des donn\u00e9es d'entra\u00eenement :  $$  x^k = \\frac{\\sum y_j \\cdot (x_j == k)}{\\sum (x_j == k)}  $$</p> </li> </ol>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#pratiquement_6","title":"Pratiquement","text":"<p><pre><code>import pandas as pd\nfrom category_encoders import LeaveOneOutEncoder\nencoder = LeaveOneOutEncoder()\nencoder.fit_transform(data.drop(columns=[\"y\"]), data[\"y\"])\n</code></pre> </p> <p>D\u00e9composons le calcul pour chaque observation dans la colonne <code>x4</code> :</p> <p>Nous allons expliquer les calculs dans le tableau ci-dessous pour les trois premi\u00e8res lignes : Calculer la moyenne du target pour chaque cat\u00e9gorie en excluant l'observation actuelle. La valeur trouv\u00e9e est la nouvelle valeur.</p> Index Cat\u00e9gorie Cibles (sans l'observation actuelle) Moyenne du target 0 B [0, 0] \\(\\frac{0 + 0}{2} = 0\\) 1 A [0, 0, 0] \\(\\frac{0 + 0 + 0}{3} = 0\\) 2 A [1, 0, 0] \\(\\frac{1 + 0 + 0}{3} \\approx 0.33\\) <p>Si tu as besoin de modifications ou d'ajouts, n'h\u00e9site pas \u00e0 me le faire savoir ! Chaque observation est maintenant encod\u00e9e avec la moyenne des cibles des autres observations de la m\u00eame cat\u00e9gorie.</p> Avantages Inconv\u00e9nients R\u00e9duction de la fuite d'information : Sa m\u00e9thode minimise les risques de biais. Complexit\u00e9 : Peut \u00eatre co\u00fbteux en calculs sur de grands ensembles de donn\u00e9es. Capture des relations complexes : Comme target encoder, utile pour des relations non lin\u00e9aires. Variabilit\u00e9 : Peut introduire de la variance avec de petites cat\u00e9gories, n\u00e9cessitant une r\u00e9gularisation suppl\u00e9mentaire."},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#5-james-stein-encoder","title":"5. James-Stein encoder","text":""},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#description_6","title":"Description","text":"<p>James-Stein encoder est  bas\u00e9 sur le target. Son id\u00e9e fondatrice est d'estimer la moyenne du target pour une cat\u00e9gorie donn\u00e9e \\(k\\) selon la formule suivante :</p> \\[ JS_i = (1-B) \\cdot \\text{mean}(y_i) + B \\cdot \\text{mean}(y) \\] <p>o\u00f9 :     - \\(JS_i\\) est l\u2019estimation pour la cat\u00e9gorie \\(C_i\\)</p> <p>- \\(\\text{mean}(y_i)\\) est la moyenne des valeurs cibles pour la cat\u00e9gorie \\(C_i\\)</p> <p>- \\(\\text{mean}(y)\\) est la moyenne g\u00e9n\u00e9rale des cibles</p> <p>- \\(B\\) est un poids calcul\u00e9 qui \u00e9quilibre l\u2019influence de la moyenne conditionnelle et de la moyenne globale.</p> <p>Cela semble tr\u00e8s sens\u00e9. Nous cherchons une estimation qui se situe entre la moyenne de l'\u00e9chantillon (risquant d'\u00eatre extr\u00eame) et la moyenne globale.</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#mathematiquement_6","title":"Math\u00e9matiquement","text":"<p>Le poids $ B $ est d\u00e9fini par :</p> \\[ B = \\frac{\\text{var}(y_i)}{\\text{var}(y_i) + \\text{var}(y)} \\] <p>On se demande quel devrait \u00eatre ce poids. Si on accorde trop de poids \u00e0 la moyenne conditionnelle, on risque le surajustement, tandis qu'en privil\u00e9giant la moyenne globale, on peut sous-ajuster. Une approche canonique en apprentissage machine serait de passer par une validation crois\u00e9e. Cependant, Charles Stein a propos\u00e9 une solution en forme ferm\u00e9e. L'id\u00e9e : ajuster la qualit\u00e9 des estimations selon la variance.</p> <p>Cet estimateur est limit\u00e9 aux distributions normales, ce qui ne convient pas \u00e0 toutes les t\u00e2ches de classification. Ainsi, on retrouve :</p> \\[ SE^2 = \\frac{\\text{var}(y)}{\\text{count}(y)} \\] <p>Un d\u00e9fi majeur est que nous ne connaissons pas \\(\\text{var}(y)\\). Il nous faudra donc estimer ces variances. Voici quelques solutions :</p> <ol> <li> <p>Mod\u00e8le Pooled : Si toutes les observations sont semblables et prennent un nombre commun d'observations pour chaque valeur.</p> </li> <li> <p>Mod\u00e8le Ind\u00e9pendant : Si les comptes d'observation diff\u00e8rent, il est plus judicieux de remplacer les variances par des erreurs standard, p\u00e9nalisant ainsi les petites observations :</p> </li> </ol> \\[ SE^2 = \\frac{\\text{var}(y)}{\\text{count}(y)} \\]"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#application-pour-la-classification-binaire","title":"Application pour la classification binaire","text":"<p>Cet estimateur a une limitation pratique dans les mod\u00e8les de classification binaire, o\u00f9 les cibles ne sont que \\(0\\) ou \\(1\\). Pour l'appliquer, on doit convertir lamoyenne du target dans l'intervalle born\u00e9 \\(&lt;0,1&gt;\\) en rempla\u00e7ant \\(\\text{mean}(y)\\) par le ratio des cotes logarithmique :</p> \\[ \\text{log-odds\\_ratio}_i = \\log\\left(\\frac{\\text{mean}(y_i)}{\\text{mean}(y_{\\text{not} \\, i})}\\right) \\] <p>Cela s'appelle mod\u00e8le binaire. C\u2019est d\u00e9licat d'estimer les param\u00e8tres de ce mod\u00e8le, et parfois cela \u00e9choue. Il est souvent plus judicieux de recourir \u00e0 un mod\u00e8le b\u00eata, souvent plus stable malgr\u00e9 une pr\u00e9cision l\u00e9g\u00e8rement inf\u00e9rieure.</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#pratiquement_7","title":"Pratiquement","text":"<p>Pour utiliser l'encodeur James-Stein, allez-y avec la classe <code>JamesSteinEncoder</code> de <code>category_encoders</code> :</p> <p><pre><code>import pandas as pd\nfrom category_encoders import JamesSteinEncoder\n\nencoder = JamesSteinEncoder()\nencoder.fit_transform(data.drop(columns=[\"y\"]), data[\"y\"])\n</code></pre> </p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#4-catboost-encoding","title":"4. CatBoost Encoding","text":""},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#description_7","title":"Description","text":"<p>Il s'agit  d'une m\u00e9thode d'encodage bas\u00e9e sur la cible, d\u00e9velopp\u00e9e \u00e0 l'origine pour \u00eatre utilis\u00e9e avec l'algorithme CatBoost, mais qui est applicable \u00e0 d'autres mod\u00e8les. Cet encodeur utilise une m\u00e9thode particuli\u00e8re pour \u00e9viter la fuite d'information tout en exploitant les relations entre les cat\u00e9gories et ld target y.</p> <p>L'id\u00e9e principale est d'utiliser les informations du target de mani\u00e8re ordonn\u00e9e. Plut\u00f4t que de d\u00e9terminer la moyenne du target pour chaque cat\u00e9gorie sur l'ensemble des donn\u00e9es (qui peut introduire des fuites), CatBoost effectue une mise \u00e0 jour de l'encodage de mani\u00e8re s\u00e9quentielle.</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#mathematiquement_7","title":"Math\u00e9matiquement","text":"<p>Le calcul se fait en deux etapes.</p> <ol> <li> <p>Ordre des observations</p> <ul> <li>L'algorithme parcourt les donn\u00e9es de mani\u00e8re ordonn\u00e9e.</li> <li>L'encodage pour chaque observation est bas\u00e9 sur les informations des observations pr\u00e9c\u00e9dentes seulement, emp\u00eachant ainsi la valeur du target actuelle d'affecter son propre encodage.</li> </ul> </li> <li> <p>Calcul progressif de la moyenne du target</p> <ul> <li>Pour chaque observation \\(i\\) dans la cat\u00e9gorie $ k $, la moyenne du target est calcul\u00e9e avec les observations pr\u00e9c\u00e9dentes. La formule est :  $$  x^k_i = \\frac{\\sum_{j &lt; i} (y_j \\cdot (x_j == k)) + \\text{prior} \\cdot \\alpha}{\\sum_{j &lt; i} (x_j == k) + \\alpha}  $$</li> </ul> </li> <li> <p>Encodage des donn\u00e9es de test</p> <ul> <li>Pour les donn\u00e9es de test, l'encodage est bas\u00e9 sur les moyennes calcul\u00e9es \u00e0 partir des donn\u00e9es d'entra\u00eenement, sans fuite d'information.</li> </ul> </li> </ol>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#pourquoi-catboost-encoder-est-il-efficace","title":"Pourquoi CatBoost encoder est-il Efficace ?","text":"<p>CatBoost encoder r\u00e9duit efficacement la fuite d'information gr\u00e2ce \u00e0 sa m\u00e9thode de calcul s\u00e9quentiel. Voici quelques atouts : - S\u00e9quentiel et Progressif : En n'utilisant que les observations pr\u00e9c\u00e9dentes, il \u00e9vite que la valeur actuelle influence son encodage. - R\u00e9gularisation : L'ajout d'un terme de r\u00e9gularisation permet de contr\u00f4ler la variance.</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#pratiquement_8","title":"Pratiquement","text":"<p><pre><code>from category_encoders import CatBoostEncoder\nencoder = CatBoostEncoder()\nencoder.fit_transform(data.drop(columns=[\"y\"]), data[\"y\"]).head()\n</code></pre> </p> <p>Et voil\u00e0 ! Pour appliquer l'encodeur CatBoost \u00e0 la variable cat\u00e9gorielle <code>x4</code>, nous avons vu le calcul \u00e9tape par \u00e9tape. </p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#conclusion","title":"Conclusion","text":"<p>Le choix de la meilleure m\u00e9thode d\u00e9pendra de votre cas d'utilisation et de la cardinalit\u00e9 des cat\u00e9gories. Est-on \u00e0 la recherche d'un mod\u00e8le explicatif ou pr\u00e9dictif ?  Dans le billet de blog de la semaine prochaine, je vais essayer de mesurer les performances de ces m\u00e9thodes avec un mod\u00e8le simple et voir qui s\u2019en sort le mieux.</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#references","title":"R\u00e9f\u00e9rences","text":"<p>doc officiel de category_encoder</p>"},{"location":"2024/09/07/ia-g%C3%A9n%C3%A9rative--une-%C3%A9volution-plut%C3%B4t-que-r%C3%A9volution/","title":"IA g\u00e9n\u00e9rative : une \u00c9volution plut\u00f4t que R\u00e9volution","text":"<p>Aujourd\u2019hui, on plonge dans l\u2019univers fascinant de l\u2019IA g\u00e9n\u00e9rative. On va retracer les grandes \u00e9tapes qui ont marqu\u00e9 son \u00e9volution. Pas de panique, on ne va pas s\u2019embourber dans une liste exhaustive \u2013 on va plut\u00f4t se concentrer sur les moments cl\u00e9s qui ont fa\u00e7onn\u00e9 l\u2019IA telle qu\u2019on la conna\u00eet aujourd'hui. J\u2019avais depuis un moment envie de faire le point sur les avanc\u00e9es dans ce domaine, mais avec le rythme effr\u00e9n\u00e9 des innovations, je ne savais pas comment illustrer cela jusqu'\u00e0 ce que je tombe sur les travaux de l'Innovation Makers Alliance.</p> <p>Comme vous l'avez bien remarqu\u00e9, j'adore sch\u00e9matiser avant de d\u00e9crire. Donc voici un sch\u00e9ma chronologique qui r\u00e9sume tout. </p>"},{"location":"2024/09/07/ia-g%C3%A9n%C3%A9rative--une-%C3%A9volution-plut%C3%B4t-que-r%C3%A9volution/#1950-1960-les-premiers-pas-avec-le-bag-of-words","title":"1950-1960 : Les Premiers Pas avec le \"Bag of Words\"","text":"<p>L'histoire de l'IA commence dans les ann\u00e9es 1950 avec le mod\u00e8le \"Bag of Words\". Cette approche traite le texte comme une collection de mots ind\u00e9pendants, sans tenir compte de leur ordre ou de leur contexte.</p> <p>Repr\u00e9sentation Matricielle :</p> <p>Le mod\u00e8le Bag of Words convertit le texte en une matrice o\u00f9 chaque ligne repr\u00e9sente un document et chaque colonne un mot unique. Les valeurs indiquent la fr\u00e9quence des mots dans chaque document.</p> <p>Exemple :</p> IA est l' avenir fa\u00e7onnera Doc 1 1 1 1 1 0 Doc 2 1 0 1 1 1 <p>TF-IDF :</p> <p>La m\u00e9thode TF-IDF (Term Frequency-Inverse Document Frequency) \u00e9value l'importance d'un mot dans un document en comparant sa fr\u00e9quence \u00e0 celle dans l'ensemble des documents :</p> \\[ \\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t) \\] <p>O\u00f9 : - TF (Term Frequency) mesure combien de fois un terme appara\u00eet dans un document :</p> \\[ \\text{TF}(t, d) = \\frac{\\text{Nombre de fois que le terme } t \\text{ appara\u00eet dans le document } d}{\\text{Nombre total de termes dans le document } d} \\] <ul> <li>IDF (Inverse Document Frequency) \u00e9value l'importance d'un terme dans tous les documents :</li> </ul> \\[ \\text{IDF}(t) = \\log \\frac{\\text{Nombre total de documents}}{\\text{Nombre de documents contenant le terme } t} \\] <p>TF-IDF et le Bag of Words ont longtemps domin\u00e9 l'analyse de texte (d'ailleurs toujours utilis\u00e9s pour des analyses textuelles), mais ils pr\u00e9sentent des limites majeures : ils ignorent le contexte et l'ordre des mots, produisent des vecteurs tr\u00e8s grands et sparses, et ne capturent pas les relations s\u00e9mantiques entre les termes. Ces approches ne permettent pas de saisir les nuances du langage, comme les relations de longue port\u00e9e ou les variations de sens. Ces insuffisances ont conduit au d\u00e9veloppement des r\u00e9seaux de neurones.</p>"},{"location":"2024/09/07/ia-g%C3%A9n%C3%A9rative--une-%C3%A9volution-plut%C3%B4t-que-r%C3%A9volution/#1986-reseaux-de-neurones-recurrents-rnns","title":"1986 : R\u00e9seaux de Neurones R\u00e9currents (RNNs)","text":"<p>En 1986, les RNNs offrent une solution pour traiter les donn\u00e9es s\u00e9quentielles comme le texte et la parole. Ils se souviennent des informations des \u00e9tapes pr\u00e9c\u00e9dentes, mais ont des difficult\u00e9s avec les d\u00e9pendances \u00e0 long terme en raison du probl\u00e8me du \"gradient qui s\u2019\u00e9vanouit\".</p>"},{"location":"2024/09/07/ia-g%C3%A9n%C3%A9rative--une-%C3%A9volution-plut%C3%B4t-que-r%C3%A9volution/#1997-les-reseaux-a-long-terme-lstm","title":"1997 : Les R\u00e9seaux \u00e0 Long Terme (LSTM)","text":"<p>Pour rem\u00e9dier aux limitations des RNNs, les r\u00e9seaux LSTM, introduits en 1997, am\u00e9liorent la capture des d\u00e9pendances \u00e0 long terme. Malgr\u00e9 cette avanc\u00e9e, ils restent complexes \u00e0 entra\u00eener et difficiles \u00e0 parall\u00e9liser.</p>"},{"location":"2024/09/07/ia-g%C3%A9n%C3%A9rative--une-%C3%A9volution-plut%C3%B4t-que-r%C3%A9volution/#2012-la-revolution-des-gpus-avec-alexnet","title":"2012 : La R\u00e9volution des GPUs avec AlexNet","text":"<p>En 2012, l'utilisation des GPUs pour entra\u00eener les mod\u00e8les devient une r\u00e9volution avec AlexNet, d\u00e9velopp\u00e9 par Ilya Sutskever, Alex Krizhevsky et Geoffrey Hinton. AlexNet r\u00e9duit drastiquement le temps d'entra\u00eenement et am\u00e9liore les performances dans la vision par ordinateur.</p>"},{"location":"2024/09/07/ia-g%C3%A9n%C3%A9rative--une-%C3%A9volution-plut%C3%B4t-que-r%C3%A9volution/#2013-word2vec-et-la-representation-vectorielle-des-mots","title":"2013 : Word2Vec et la Repr\u00e9sentation Vectorielle des Mots","text":"<p>En 2013, Word2Vec introduit une avanc\u00e9e majeure en am\u00e9liorant la repr\u00e9sentation vectorielle des mots avec les techniques CBOW et Skip-gram. Il montre que les mots apparaissant dans des contextes similaires ont des vecteurs proches, am\u00e9liorant l'apprentissage s\u00e9mantique.</p>"},{"location":"2024/09/07/ia-g%C3%A9n%C3%A9rative--une-%C3%A9volution-plut%C3%B4t-que-r%C3%A9volution/#2017-lessor-des-transformers","title":"2017 : L\u2019Essor des Transformers","text":"<p>L'ann\u00e9e 2017 marque l'introduction des Transformers par Google avec \"Attention is All You Need\". Les Transformers int\u00e8grent un m\u00e9canisme d'attention qui permet de capturer les relations complexes entre les mots, ind\u00e9pendamment de leur position. Cette architecture facilite le traitement parall\u00e8le des donn\u00e9es et pave la voie aux grands mod\u00e8les comme GPT. J'ai d'ailleurs commenc\u00e9 \u00e0 r\u00e9fl\u00e9chir comment illustrer les diff\u00e9rents types de Transformers qui existent de nos jours.</p>"},{"location":"2024/09/07/ia-g%C3%A9n%C3%A9rative--une-%C3%A9volution-plut%C3%B4t-que-r%C3%A9volution/#2018-2022-les-grands-modeles-de-langage","title":"2018-2022 : Les Grands Mod\u00e8les de Langage","text":"<p>Apr\u00e8s l\u2019arriv\u00e9e des Transformers, les grands mod\u00e8les de langage se d\u00e9veloppent rapidement :</p> <ul> <li>GPT-3 (2020) : D\u00e9velopp\u00e9 par OpenAI, GPT-3, avec ses 175 milliards de param\u00e8tres, est capable de g\u00e9n\u00e9rer du texte avec une coh\u00e9rence impressionnante et de r\u00e9aliser diverses t\u00e2ches sans formation sp\u00e9cifique. Il repr\u00e9sente une avanc\u00e9e majeure en augmentant la taille des mod\u00e8les.</li> </ul> <ul> <li>DALL-E (2021) : DALL-E, \u00e9galement d'OpenAI, g\u00e9n\u00e8re des images \u00e0 partir de descriptions textuelles, fusionnant cr\u00e9ativit\u00e9 et technique.</li> </ul> <ul> <li>Midjourney (2022) : Ce mod\u00e8le se concentre sur la cr\u00e9ation artistique d\u2019images, permettant aux utilisateurs de concevoir des \u0153uvres uniques \u00e0 partir de prompts.</li> </ul> <ul> <li>ChatGPT (fin 2022) : Bas\u00e9 sur l\u2019architecture GPT et InstructGPT, ChatGPT offre des capacit\u00e9s de dialogue interactives, un tournant majeur des potentialit\u00e9s de l'IA g\u00e9n\u00e9rative. C\u2019est \u00e0 ce moment pr\u00e9cis que le grand public commence \u00e0 se rendre compte des derni\u00e8res avanc\u00e9es en IA et deep learning.</li> </ul>"},{"location":"2024/09/07/ia-g%C3%A9n%C3%A9rative--une-%C3%A9volution-plut%C3%B4t-que-r%C3%A9volution/#2023-les-nouveaux-acteurs-en-plus-de-openai","title":"2023 : Les Nouveaux Acteurs en Plus de OpenAI","text":"<ul> <li>LLaMA (2023) : Meta introduit la s\u00e9rie LLaMA pour fournir des alternatives efficaces avec diff\u00e9rentes tailles de param\u00e8tres, favorisant une recherche plus accessible et performante. Ainsi, la porte \u00e0 l'open source est ouverte.</li> </ul> <ul> <li>Juste apr\u00e8s LLaMA, OpenAI entre encore en jeu avec DALL-E 3 et GPT-4, qui est, jusqu'\u00e0 nos jours, l'un des mod\u00e8les les plus performants, bien que tr\u00e8s co\u00fbteux.</li> </ul> <ul> <li>Mistral 7B (2023) : Un mod\u00e8le LLM fran\u00e7ais avec 7 milliards de param\u00e8tres, con\u00e7u pour r\u00e9pondre aux besoins sp\u00e9cifiques tout en offrant une performance de haut niveau.</li> </ul> <ul> <li>Claude 3 (2023) : La famille Claude 3 d'Anthropic, comprenant Claude 3 Haiku, Claude 3 Sonnet, et Claude 3 Opu, \u00e9tablit de nouveaux standards pour les t\u00e2ches cognitives complexes.</li> </ul> <ul> <li>Bard puis Gemini (2023) : Google rattrape son retard en annon\u00e7ant Gemini, un LLM multimodal int\u00e9grant des capacit\u00e9s avanc\u00e9es pour le texte, les images, et le son, apr\u00e8s un premier Bard qui n'a pas convaincu.</li> </ul>"},{"location":"2024/09/07/ia-g%C3%A9n%C3%A9rative--une-%C3%A9volution-plut%C3%B4t-que-r%C3%A9volution/#2024-les-dernieres-innovations","title":"2024 : Les Derni\u00e8res Innovations","text":"<ul> <li>Mistral Large (f\u00e9vrier 2024) : Ce mod\u00e8le, nativement multilingue, am\u00e9liore les capacit\u00e9s de traitement et d'interaction dans plusieurs langues. La France ne veut pas rester en reste de cette course.</li> </ul> <ul> <li>GPT-4O (mai 2024) : Ce mod\u00e8le \"omnimodal\" de OpenAI int\u00e8gre des capacit\u00e9s avanc\u00e9es pour le texte, les images, et le son. Il repr\u00e9sente un progr\u00e8s significatif en mati\u00e8re d'IA g\u00e9n\u00e9rative.</li> </ul> <ul> <li>SearchGPT (2024) : Un prototype de moteur de recherche d'OpenAI, lanc\u00e9 le 26 juillet, combinant fonctionnalit\u00e9s traditionnelles et capacit\u00e9s d'IA g\u00e9n\u00e9rative pour une exp\u00e9rience de recherche enrichie.</li> </ul> <ul> <li>NVIDIA Blackwell (2024) : Annonc\u00e9 en juillet, ce processeur acc\u00e9l\u00e8re les calculs n\u00e9cessaires pour l'entra\u00eenement des mod\u00e8les d'IA, facilitant les avanc\u00e9es technologiques. NVIDIA se montre pr\u00eat \u00e0 \u00e9pauler cette course vers l'IAG (Intelligence Artificielle G\u00e9n\u00e9ralis\u00e9e).</li> </ul>"},{"location":"2024/09/07/ia-g%C3%A9n%C3%A9rative--une-%C3%A9volution-plut%C3%B4t-que-r%C3%A9volution/#conclusion","title":"Conclusion","text":"<p>D\u00e9sormais, les data scientists, ou si vous pr\u00e9f\u00e9rez, AI engineers ou LLM engineers (on est toujours tr\u00e8s dou\u00e9s pour inventer de nouveaux titres, rires), dans les diff\u00e9rentes entreprises s'approprient ces technologies pour acc\u00e9l\u00e9rer la cr\u00e9ation de nouvelles sources de croissance, tout comme ils le faisaient d\u00e9j\u00e0 avec la data. Mais cette fois-ci, le traitement des donn\u00e9es non structur\u00e9es devient \u00e0 la fois plus simple et plus stimulant gr\u00e2ce aux nouvelles opportunit\u00e9s. L'IA g\u00e9n\u00e9rative continuera ainsi de transformer les industries avec des cas d'usage comme les chatbots, ou centre d'aide ou encore d'un centre de base de connaissance, les moteurs de recherche vectoriels, et bien d'autres.</p> <p>If you're a data scientist unfamiliar with AI, all of a sudden you're going to have to step up your game in to iagen.</p>"},{"location":"2024/08/11/iam-for-eks/","title":"Iam for eks","text":""},{"location":"2024/08/11/iam-for-eks/#understanding-iam-roles-and-users-in-aws-with-a-practical-example-running-an-application-on-eks","title":"Understanding IAM Roles and Users in AWS with a Practical Example: Running an Application on EKS","text":"<p>AWS Identity and Access Management (IAM) is a cornerstone of security and access control in AWS environments. IAM allows you to manage users, groups, and roles, and to specify their permissions to access AWS resources. In this article, we will explore the differences between IAM roles and IAM users, provide a practical scenario of deploying an application on Amazon EKS (Elastic Kubernetes Service), and clarify how IAM roles relate to specific AWS console options.</p>"},{"location":"2024/08/11/iam-for-eks/#1-iam-role-vs-iam-user","title":"1. IAM Role vs. IAM User","text":"<p>IAM User: - An IAM user is an entity that represents a person or application that interacts with AWS resources. Each IAM user has a unique set of credentials (username and password or access keys). - IAM users are generally assigned permissions directly or through group memberships. - Example: A developer who needs access to specific AWS resources and can authenticate using their IAM user credentials.</p> <p>IAM Role: - An IAM role is an AWS identity with specific permissions that can be assumed by entities like AWS services, EC2 instances, or even IAM users. - Roles are temporary and are assumed by entities that need to perform specific actions. They do not have permanent credentials; instead, they provide temporary security credentials. - Example: An EC2 instance running a containerized application that needs to pull images from Amazon ECR.</p>"},{"location":"2024/08/11/iam-for-eks/#2-practical-scenario-deploying-an-application-on-eks","title":"2. Practical Scenario: Deploying an Application on EKS","text":"<p>Let\u2019s walk through an example of deploying an application, such as Gradio, on Amazon EKS. Gradio is a popular library for creating machine learning demos.</p> <p>Steps for Deployment:</p> <ol> <li> <p>Set Up IAM Roles for EKS:</p> <ul> <li>Cluster Role: Create an IAM role that the EKS cluster will use. This role allows EKS to manage the underlying EC2 instances and perform other operations.</li> <li>Node Instance Role: Create another IAM role for EC2 instances that will run the Kubernetes worker nodes. This role allows the instances to interact with other AWS services, like pulling images from ECR or writing logs to CloudWatch.</li> </ul> <p>Example IAM Policy for Node Instance Role:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ec2:DescribeInstances\",\n                \"ec2:DescribeTags\",\n                \"ecr:GetAuthorizationToken\",\n                \"ecr:BatchCheckLayerAvailability\",\n                \"ecr:GetDownloadUrlForLayer\",\n                \"ecr:BatchGetImage\",\n                \"logs:CreateLogStream\",\n                \"logs:PutLogEvents\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n</code></pre> </li> <li> <p>Create an EKS Cluster:</p> <ul> <li>Go to the EKS Console.</li> <li>Create a new EKS cluster and associate it with the IAM cluster role created earlier.</li> </ul> </li> <li> <p>Launch EC2 Instances for Kubernetes Nodes:</p> <ul> <li>Launch EC2 instances and associate them with the IAM node instance role. Ensure these instances are part of the EKS node group.</li> </ul> </li> <li> <p>Deploy Gradio Application:</p> <ul> <li>Package your Gradio application into a Docker container and push the image to Amazon ECR.</li> <li>Create a Kubernetes deployment YAML file specifying the container image from ECR and deploy it to your EKS cluster.</li> </ul> <p>Example Kubernetes Deployment YAML:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gradio-app\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: gradio\n  template:\n    metadata:\n      labels:\n        app: gradio\n    spec:\n      containers:\n        - name: gradio-container\n          image: &lt;your-ecr-repository-url&gt;/gradio-app:latest\n          ports:\n            - containerPort: 7860\n</code></pre> </li> <li> <p>Expose the Application:</p> <ul> <li>Create a Kubernetes Service to expose the Gradio application to the internet.</li> </ul> <p>Example Service YAML:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: gradio-service\nspec:\n  selector:\n    app: gradio\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 7860\n  type: LoadBalancer\n</code></pre> </li> </ol>"},{"location":"2024/08/11/iam-for-eks/#3-iam-roles-in-aws-console-options","title":"3. IAM Roles in AWS Console Options","text":"<p>You mentioned having different console options: <code>DATAengROLE</code>, <code>DATASCIENC</code>, and <code>DATAANALYST</code>. These are IAM roles configured for various teams or purposes. Here\u2019s how they might be used:</p> <ul> <li>DATAengROLE: This role could be configured to provide access to data engineering tools and resources, like Apache Airflow, for data engineers. If you have access to Airflow with this role, it means that <code>DATAengROLE</code> includes permissions to view and manage Airflow resources.</li> </ul> <ul> <li>DATASCIENC: This role might be tailored for data scientists, granting access to tools and resources pertinent to data analysis and modeling. The specific permissions and services available to this role would depend on the policies attached.</li> </ul> <ul> <li>DATAANALYST: This role could be for data analysts, providing access to reporting tools or datasets but not necessarily the same resources as the other roles.</li> </ul> <p>Role-Based Access: In your case, when using the <code>DATAengROLE</code>, you have access to Airflow because this role has the necessary permissions configured. Conversely, the <code>DATAANALYST</code> role might not have the same permissions, hence the lack of access to Airflow.</p>"},{"location":"2024/08/11/iam-for-eks/#conclusion","title":"Conclusion","text":"<p>Understanding the difference between IAM roles and IAM users is fundamental for managing access and permissions in AWS. IAM roles are particularly useful for granting temporary access and managing permissions for AWS services and resources, while IAM users are suited for individuals requiring direct access.</p> <p>In the context of deploying an application like Gradio on Amazon EKS, properly configuring IAM roles ensures that your EKS cluster and EC2 instances have the appropriate permissions to interact with other AWS services. Additionally, understanding IAM roles in relation to different AWS console options helps in managing access based on specific roles and responsibilities within your organization.</p>"},{"location":"2024/09/08/%C3%A9valuer-les-capacit%C3%A9s-des-llms--les-benchmarks/","title":"\u00c9valuer les Capacit\u00e9s des LLMs : les benchmarks","text":"<p>Les mod\u00e8les de langage (LLMs) ont r\u00e9alis\u00e9 des avanc\u00e9es spectaculaires dans des t\u00e2ches vari\u00e9es, telles que la r\u00e9daction, la conversation, et la programmation. Pour \u00e9valuer et comparer efficacement leur intelligence, divers benchmarks sont utilis\u00e9s, mesurant des capacit\u00e9s allant des connaissances acad\u00e9miques (comme MMLU) au raisonnement complexe (GPQA), en passant par des comp\u00e9tences sp\u00e9cifiques telles que les math\u00e9matiques de base (GSM8K) ou la g\u00e9n\u00e9ration de code (HumanEval). </p> <p>Ces \u00e9valuations permettent de mieux cerner la port\u00e9e des capacit\u00e9s des LLMs, bien que certains benchmarks se concentrent encore principalement sur des questions ferm\u00e9es avec des r\u00e9ponses courtes, limitant ainsi une \u00e9valuation compl\u00e8te de leurs aptitudes.</p> <p>Pour mieux comprendre comment les LLMs se comparent entre eux, il est essentiel d'examiner ces benchmarks en d\u00e9tail. Le tableau ci-dessous, illustre une comparaison des performances de leur mod\u00e8le Claude face \u00e0 d'autres LLMs leaders :  Source : Site Anthropic</p>"},{"location":"2024/09/08/%C3%A9valuer-les-capacit%C3%A9s-des-llms--les-benchmarks/#concepts-clees","title":"Concepts cl\u00e9es :","text":"<ul> <li>SOTA (State-of-the-Art) : Se r\u00e9f\u00e8re aux mod\u00e8les, algorithmes ou techniques les plus performants actuellement dans un domaine d'\u00e9tude sp\u00e9cifique.</li> <li>STEM : Acronyme pour Science, Technology, Engineering, and Mathematics, repr\u00e9sentant des disciplines cl\u00e9s souvent utilis\u00e9es pour tester les capacit\u00e9s des LLMs en mati\u00e8re de compr\u00e9hension et de raisonnement scientifique.</li> </ul>"},{"location":"2024/09/08/%C3%A9valuer-les-capacit%C3%A9s-des-llms--les-benchmarks/#les-principaux-benchmarks-pour-evaluer-les-llms","title":"Les Principaux Benchmarks pour \u00c9valuer les LLMs","text":""},{"location":"2024/09/08/%C3%A9valuer-les-capacit%C3%A9s-des-llms--les-benchmarks/#1-mmlu-massive-multitask-language-understanding","title":"1. MMLU (Massive Multitask Language Understanding)","text":"<ul> <li>Publication : 2021</li> <li>Liens : Code | Dataset | Papier</li> </ul> <p>Le MMLU \u00e9value les mod\u00e8les en se basant sur les connaissances acquises lors de la pr\u00e9-formation, en se concentrant sur les r\u00e9glages z\u00e9ro-shot et few-shot. Ce benchmark couvre 57 sujets, incluant les STEM, les sciences humaines et sociales, avec des niveaux de difficult\u00e9 allant de l'\u00e9l\u00e9mentaire \u00e0 l'avanc\u00e9. </p> <ul> <li>Type de donn\u00e9es : Questions \u00e0 choix multiples</li> <li>Crit\u00e8re de scoring : Proportion de r\u00e9ponses correctes exactes (par exemple, 'A', 'B', etc.).</li> <li>Environnement d'\u00e9valuation : Con\u00e7u pour des configurations z\u00e9ro-shot et few-shot pour tester les capacit\u00e9s g\u00e9n\u00e9rales des LLMs sans ajustement sp\u00e9cifique aux t\u00e2ches.</li> </ul> <p> Source : Papier original MMLU</p>"},{"location":"2024/09/08/%C3%A9valuer-les-capacit%C3%A9s-des-llms--les-benchmarks/#2-hellaswag","title":"2. HellaSwag","text":"<ul> <li>Publication : 2019</li> <li>Liens : Code | Dataset | Papier</li> </ul> <p>HellaSwag \u00e9value les capacit\u00e9s de raisonnement des LLMs \u00e0 travers des t\u00e2ches de compl\u00e9tion de phrases. Il teste si les mod\u00e8les peuvent s\u00e9lectionner la fin appropri\u00e9e parmi un ensemble de quatre choix pour 10 000 phrases. </p> <ul> <li>M\u00e9trique utilis\u00e9e : Proportion de r\u00e9ponses correctes exactes.</li> <li>Sp\u00e9cificit\u00e9 : Met l'accent sur le raisonnement de bon sens, un domaine o\u00f9 de nombreux mod\u00e8les \u00e9chouent encore.</li> <li>Structure des t\u00e2ches : Les t\u00e2ches sont des compl\u00e9tions de phrases o\u00f9 les choix sont construits de mani\u00e8re \u00e0 sembler plausibles pour tester les limites du mod\u00e8le.</li> </ul> <p></p> <p>Source : Papier original HellaSwag</p>"},{"location":"2024/09/08/%C3%A9valuer-les-capacit%C3%A9s-des-llms--les-benchmarks/#3-big-bench-hard-beyond-the-imitation-game-benchmark","title":"3. BIG-Bench Hard (Beyond the Imitation Game Benchmark)","text":"<ul> <li>Publication : 2022</li> <li>Liens : Code | Dataset | Papier</li> </ul> <p>BIG-Bench Hard s\u00e9lectionne 23 t\u00e2ches difficiles du BIG-Bench suite, un ensemble diversifi\u00e9 de 204 t\u00e2ches con\u00e7ues pour d\u00e9passer les capacit\u00e9s des mod\u00e8les de langage. </p> <ul> <li>Caract\u00e9ristiques uniques : Inclut des t\u00e2ches qui d\u00e9passent les capacit\u00e9s des mod\u00e8les de langage actuels, n\u00e9cessitant souvent un raisonnement avanc\u00e9 ou des r\u00e9ponses multi-pas.</li> <li>M\u00e9thodologie : Utilisation de Chain-of-Thought (CoT) prompting pour am\u00e9liorer les performances des LLMs sur des t\u00e2ches complexes.</li> </ul>"},{"location":"2024/09/08/%C3%A9valuer-les-capacit%C3%A9s-des-llms--les-benchmarks/#4-humaneval","title":"4. HumanEval","text":"<ul> <li>Publication : 2021</li> <li>Liens : Code | Dataset | Papier</li> </ul> <p>HumanEval consiste en 164 t\u00e2ches de programmation uniques pour \u00e9valuer les capacit\u00e9s de g\u00e9n\u00e9ration de code des mod\u00e8les. Ces t\u00e2ches couvrent un large spectre, des algorithmes \u00e0 la compr\u00e9hension des langages de programmation. </p> <ul> <li>Types de t\u00e2ches : Algorithmes, manipulation de donn\u00e9es, compr\u00e9hension syntaxique.</li> <li>M\u00e9trique d'\u00e9valuation : Capacit\u00e9 du mod\u00e8le \u00e0 g\u00e9n\u00e9rer du code correct sans intervention humaine. Les sorties doivent \u00eatre correctes au premier essai.</li> </ul>"},{"location":"2024/09/08/%C3%A9valuer-les-capacit%C3%A9s-des-llms--les-benchmarks/#5-mt-bench","title":"5. MT-Bench","text":"<ul> <li>Publication : 2021</li> <li>Liens : Code | Dataset | Papier</li> </ul> <p>MT-Bench \u00e9value la qualit\u00e9 des assistants de chat en les soumettant \u00e0 une s\u00e9rie de questions ouvertes et multi-turn, en utilisant des LLMs comme juges. </p> <ul> <li>Structure : 80 questions multi-turn pour \u00e9valuer la conversation et le suivi d'instructions.</li> <li>Crit\u00e8re de scoring : Utilise GPT-4 pour noter chaque interaction sur une \u00e9chelle de 1 \u00e0 10. Le score final est la moyenne de toutes les \u00e9valuations.</li> </ul> <p> Source : Papier original MT-Bench</p>"},{"location":"2024/09/08/%C3%A9valuer-les-capacit%C3%A9s-des-llms--les-benchmarks/#6-drop-discrete-reasoning-over-paragraphs","title":"6. DROP (Discrete Reasoning Over Paragraphs)","text":"<ul> <li>Publication : 2019</li> <li>Liens : Code | Dataset | Papier</li> </ul> <p>DROP teste les capacit\u00e9s des LLMs \u00e0 effectuer des raisonnements complexes et discrets en fonction des informations contenues dans un paragraphe. Les t\u00e2ches incluent des questions n\u00e9cessitant des calculs, des comparaisons et des extractions de texte.</p> <ul> <li>Type de donn\u00e9es : Questions \u00e0 r\u00e9ponse ouverte n\u00e9cessitant des calculs ou des comparaisons.</li> <li>Crit\u00e8re de scoring : Utilisation du F1 Score, qui combine la pr\u00e9cision et le rappel pour mesurer la capacit\u00e9 des mod\u00e8les \u00e0 g\u00e9n\u00e9rer des r\u00e9ponses exactes.</li> </ul> <p> Source : Papier original MT-Bench</p>"},{"location":"2024/09/08/%C3%A9valuer-les-capacit%C3%A9s-des-llms--les-benchmarks/#conclusion","title":"Conclusion","text":"<p>Les benchmarks comme MMLU, HellaSwag, BIG-Bench Hard, HumanEval, MT-Bench, DROP et l'utilisation du F1 Score offrent des \u00e9valuations pr\u00e9cieuses pour mesurer les capacit\u00e9s des LLMs dans divers domaines tels que la compr\u00e9hension du langage, le raisonnement, la programmation et la conversation. Ces benchmarks, combin\u00e9s \u00e0 des scores et m\u00e9triques sp\u00e9cifiques, aident \u00e0 identifier les forces et les faiblesses des mod\u00e8les, ouvrant ainsi la voie \u00e0 des am\u00e9liorations continues dans le domaine des LLMs.</p>"},{"location":"2024/09/08/%C3%A9valuer-les-capacit%C3%A9s-des-llms--les-benchmarks/#references","title":"R\u00e9f\u00e9rences:","text":"<p>https://github.com/leobeeson/llm_benchmarks</p>"},{"location":"2024/07/29/mermaid-for-data-scientist/","title":"Mermaid for data scientist","text":""},{"location":"2024/07/29/mermaid-for-data-scientist/#mermaid-pour-documenter-vos-projets","title":"Mermaid pour documenter vos projets","text":"<p>La documentation est une \u00e9tape cruciale dans tout projet de d\u00e9veloppement, mais soyons honn\u00eates, ce n'est pas toujours la plus amusante. Pourtant, quand il s'agit de clarifier des concepts complexes ou de repr\u00e9senter des processus, rien ne vaut un bon diagramme. Beaucoup de gens se tournent vers Word ou PowerPoint, voire des logiciels sp\u00e9cialis\u00e9s comme Lucidchart, pour cr\u00e9er ces illustrations. Mais pour nous, d\u00e9veloppeurs, qui aimons le code et l'automatisation, il existe un outil qui pourrait bien changer la donne : Mermaid.</p> <p>Mermaid permet de cr\u00e9er des diagrammes et des flowcharts directement \u00e0 partir de texte, ce qui vous permet de rester dans votre \u00e9diteur de code pr\u00e9f\u00e9r\u00e9. Si vous avez toujours voulu documenter vos projets avec des diagrammes sans quitter votre environnement de d\u00e9veloppement, ce guide est fait pour vous. D\u00e9couvrez comment Mermaid peut transformer votre approche de la documentation.</p>"},{"location":"2024/07/29/mermaid-for-data-scientist/#preparer-votre-environnement","title":"Pr\u00e9parer votre Environnement","text":"<p>Avant de plonger dans la cr\u00e9ation de diagrammes avec Mermaid, vous devez configurer votre environnement de travail.</p>"},{"location":"2024/07/29/mermaid-for-data-scientist/#extensions-recommandees-pour-vscode","title":"Extensions Recommand\u00e9es pour VSCode","text":"<p>Pour une exp\u00e9rience optimale avec Mermaid, nous vous recommandons d'installer les extensions suivantes dans Visual Studio Code :</p> <ul> <li>Markdown Preview Mermaid Support : Permet d'afficher un aper\u00e7u en temps r\u00e9el de vos diagrammes Mermaid directement dans VSCode avec la commande <code>Ctrl + K V</code>.</li> <li>Markdown PDF : Permet d'exporter vos diagrammes Mermaid en PDF avec la commande <code>Ctrl + Shift + P</code>.</li> </ul> <p>Une fois ces extensions en place, vous \u00eates pr\u00eat \u00e0 cr\u00e9er vos premiers diagrammes avec Mermaid !</p>"},{"location":"2024/07/29/mermaid-for-data-scientist/#etape-1-definir-les-nuds-et-les-relations","title":"\u00c9tape 1 : D\u00e9finir les N\u0153uds et les Relations","text":"<p>Cr\u00e9er un diagramme avec Mermaid commence par la d\u00e9finition des n\u0153uds (ou \u00e9tapes) et des relations entre eux. Voici comment proc\u00e9der :</p>"},{"location":"2024/07/29/mermaid-for-data-scientist/#nuds","title":"N\u0153uds","text":"<p>Les n\u0153uds repr\u00e9sentent diff\u00e9rentes \u00e9tapes ou entit\u00e9s dans votre processus. Chaque n\u0153ud est identifi\u00e9 par un identifiant unique et peut avoir une \u00e9tiquette optionnelle. Voici un exemple simple :</p> <pre><code>graph LR\n    data --&gt; clean;   \n    clean --&gt; explore;   \n    explore --&gt; preprocess; </code></pre> <p>Dans cet exemple, <code>data</code>, <code>clean</code>, <code>explore</code>, et <code>preprocess</code> sont des identifiants de n\u0153uds. Les fl\u00e8ches (<code>--&gt;</code>) indiquent les relations entre eux. Ce diagramme montre un flux de gauche \u00e0 droite gr\u00e2ce \u00e0 la directive <code>graph LR</code>.</p>"},{"location":"2024/07/29/mermaid-for-data-scientist/#relations","title":"Relations","text":"<p>Les relations d\u00e9finissent la s\u00e9quence ou la connexion entre les n\u0153uds. Voici quelques types de relations que vous pouvez utiliser :</p> <ul> <li><code>--&gt;</code> : Relation directionnelle de gauche \u00e0 droite.</li> <li><code>---</code> : Relation horizontale sans fl\u00e8che.</li> <li><code>==&gt;</code> : Relation bidirectionnelle.</li> <li><code>==&gt;|label|</code> : Relation directionnelle avec un label.</li> </ul> <p>Un exemple avec des relations plus complexes et des labels :</p> <pre><code>graph LR\n    data --&gt; clean;       \n    clean &lt;---&gt; explore;       \n    explore ==&gt; preprocess;       \n    preprocess ==&gt;|split| Model; </code></pre>"},{"location":"2024/07/29/mermaid-for-data-scientist/#etape-2-personnaliser-les-formes-et-les-couleurs","title":"\u00c9tape 2 : Personnaliser les Formes et les Couleurs","text":"<p>Mermaid offre une vari\u00e9t\u00e9 de formes pour vos n\u0153uds. Voici un aper\u00e7u des formes les plus couramment utilis\u00e9es :</p> Forme Code Description Rectangle <code>[(...)]</code> N\u0153ud rectangulaire standard Rectangle arrondi <code>[[...]]</code> N\u0153ud avec des bords arrondis Cylindre <code>[(...)]</code> N\u0153ud en forme de cylindre Cercle <code>((...))</code> N\u0153ud en forme de cercle <p>Utilisez ces formes pour personnaliser vos diagrammes en fonction de vos besoins. Par exemple, pour cr\u00e9er un flux de travail de data science :</p> <pre><code>graph LR\n    data[(Data Collection)] --&gt; clean([Data Cleaning]);\n    clean --&gt; explore([Exploratory Data Analysis]);\n    explore --&gt; preprocess([Data Preprocessing]);\n    preprocess --&gt; split([Train/Test Split]);\n    split --&gt; model([Model Training]);\n    model --&gt; evaluate([Model Evaluation]);\n    evaluate --&gt; tune([Model Tuning]);\n    tune --&gt; model; \n    evaluate --&gt; deploy([Model Deployment]);</code></pre>"},{"location":"2024/07/29/mermaid-for-data-scientist/#etape-3-ajuster-la-direction-et-le-style","title":"\u00c9tape 3 : Ajuster la Direction et le Style","text":"<p>Mermaid vous permet de contr\u00f4ler la direction et le style de vos diagrammes gr\u00e2ce \u00e0 quelques mots-cl\u00e9s simples.</p>"},{"location":"2024/07/29/mermaid-for-data-scientist/#direction-du-flux","title":"Direction du Flux","text":"<p>Vous pouvez ajuster la direction du flux avec les options suivantes :</p> <ul> <li><code>LR</code> : De gauche \u00e0 droite (Left to Right).</li> <li><code>TB</code> : De haut en bas (Top to Bottom).</li> <li><code>RL</code> : De droite \u00e0 gauche (Right to Left).</li> <li><code>BT</code> : De bas en haut (Bottom to Top).</li> </ul>"},{"location":"2024/07/29/mermaid-for-data-scientist/#style-des-nuds","title":"Style des N\u0153uds","text":"<p>Pour personnaliser l'apparence des n\u0153uds, vous pouvez utiliser :</p> <ul> <li><code>fill</code> : Couleur de fond du n\u0153ud.</li> <li><code>stroke</code> : Couleur de la bordure du n\u0153ud.</li> <li><code>color</code> : Couleur du texte \u00e0 l'int\u00e9rieur du n\u0153ud.</li> </ul> <p>Voici un exemple combinant direction et style :</p> <pre><code>graph TB\n\n    data((Data Sources))-. DHA .-&gt;DHA_data[Environmental Data];\n    data-. OpenData .-&gt;OpenData[Satellites Imagery];\n    data-. CSV files .-&gt;CSV_files[Operational Data];\n\n    DHA_data--&gt;|Preparation| Store[(Feature Store)];\n    OpenData--&gt;|Preparation| Store[(Feature Store)];\n    CSV_files--&gt;|Preparation| Store[(Feature Store)];\n\n    Store--&gt;|Modeling|Modeling_1[Airports Severity];\n\n    style data fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;\n    style Modeling_1 fill:#9f9,stroke:#000,stroke-width:0px;</code></pre> <p>Ce diagramme utilise un flux de haut en bas (TB) et applique des styles personnalis\u00e9s aux n\u0153uds pour les mettre en valeur visuellement.</p>"},{"location":"2024/07/29/mermaid-for-data-scientist/#ressources-utiles","title":"Ressources Utiles","text":"<p>Pour aller plus loin avec Mermaid, voici quelques liens utiles :</p> <ul> <li>Documentation officielle de Mermaid</li> <li>Guide de syntaxe Mermaid pour les flowcharts</li> <li>\u00c9diteur en ligne Mermaid</li> </ul>"},{"location":"2024/07/29/mermaid-for-data-scientist/#conclusion","title":"Conclusion","text":"<p>Avec Mermaid, documenter vos projets devient non seulement plus simple, mais aussi plus amusant. Fini les allers-retours entre plusieurs outils : vous pouvez cr\u00e9er, personnaliser et int\u00e9grer des diagrammes directement depuis votre code. Que vous soyez en train de planifier un projet, de documenter un processus ou de clarifier une architecture, Mermaid est un atout pr\u00e9cieux pour tout d\u00e9veloppeur. Essayez-le, et vous ne reviendrez plus en arri\u00e8re !</p>"},{"location":"2024/09/14/faire-tourner-son-chatbot-avec-une-interface-%C3%A9quivalente-%C3%A0-chatgpt-gr%C3%A2ce-%C3%A0-openwebui/","title":"Faire tourner son chatbot avec une interface \u00e9quivalente \u00e0 ChatGPT gr\u00e2ce \u00e0 OpenWebUI","text":"<p>L'\u00e8re des assistants conversationnels est en pleine expansion, et gr\u00e2ce \u00e0 des outils comme OpenWebUI, il est d\u00e9sormais possible de cr\u00e9er son propre UI conversationnel avec des fonctionnalit\u00e9s similaires \u00e0 celles de ChatGPT sans beaucoup d'effort. On suppose que vous avez d\u00e9j\u00e0 un chatbot qui fonctionne correctement et que vous souhaitez une meilleure interface utilisateur. Cet article vous guidera dans la mise en place de cette UI.</p>"},{"location":"2024/09/14/faire-tourner-son-chatbot-avec-une-interface-%C3%A9quivalente-%C3%A0-chatgpt-gr%C3%A2ce-%C3%A0-openwebui/#1-quest-ce-quopenwebui","title":"1. Qu\u2019est-ce qu\u2019OpenWebUI ?","text":"<p>OpenWebUI est con\u00e7u pour \u00eatre une solution flexible d\u2019interface utilisateur (UI) open-source qui facilite l\u2019interaction avec les LLM comme GPT-3.5 ou GPT-4. Il repose sur deux composants principaux :</p> <ul> <li>Le composant OpenWebUI proprement dit : C\u2019est l\u2019interface utilisateur qui permet de g\u00e9rer les interactions entre l'utilisateur et le mod\u00e8le.</li> </ul> <ul> <li>Le composant Pipelines : Cette composante s\u2019occupe de la logique LLM. Elle permet d'intercepter, traiter et modifier les prompts utilisateurs avant de les envoyer au mod\u00e8le final.</li> </ul> <p>Bon, comme vous le savez , une image vaut mieux milles vaux. Voici la magie que nous propose openwebui </p>"},{"location":"2024/09/14/faire-tourner-son-chatbot-avec-une-interface-%C3%A9quivalente-%C3%A0-chatgpt-gr%C3%A2ce-%C3%A0-openwebui/#2-concepts-de-pipelines","title":"2. Concepts de Pipelines","text":"<p>Pour comprendre l'outil pipelines, il faut s\u2019int\u00e9resser aux valves, filtres (filters) et pipes.</p>"},{"location":"2024/09/14/faire-tourner-son-chatbot-avec-une-interface-%C3%A9quivalente-%C3%A0-chatgpt-gr%C3%A2ce-%C3%A0-openwebui/#21-concepts-de-valves","title":"2.1. Concepts de Valves","text":"<p>Les valves jouent un r\u00f4le de r\u00e9gulation dans le pipeline, autorisant ou bloquant le passage de certaines donn\u00e9es. Pour qu'un pipeline soit fonctionnel, on doit avoir une classe valves. Le plus souvent, c\u2019est l'endroit o\u00f9 sont pass\u00e9s les credentials cl\u00e9s et param\u00e8tres des mod\u00e8les.</p>"},{"location":"2024/09/14/faire-tourner-son-chatbot-avec-une-interface-%C3%A9quivalente-%C3%A0-chatgpt-gr%C3%A2ce-%C3%A0-openwebui/#22-concept-de-filter","title":"2.2. Concept de Filter","text":"<p>Un Filter Pipeline est principalement utilis\u00e9 pour intercepter le message avant qu'il ne soit envoy\u00e9 au LLM, ou apr\u00e8s avoir re\u00e7u la r\u00e9ponse du LLM mais avant de l'envoyer \u00e0 l'utilisateur. L'id\u00e9e derri\u00e8re le Filter Pipeline est d\u2019ajouter des \u00e9tapes avant ou apr\u00e8s l'appel au mod\u00e8le. Il sert donc principalement \u00e0 :</p> <ul> <li>R\u00e9cup\u00e9rer des informations externes (RAG) pour enrichir le contexte du message avant l'envoi au LLM.</li> </ul> <ul> <li>Ex\u00e9cuter des outils qui ajoutent des donn\u00e9es suppl\u00e9mentaires n\u00e9cessaires au LLM.</li> </ul> <ul> <li>Appliquer des filtres de s\u00e9curit\u00e9 ou d'autres types de transformation avant que la r\u00e9ponse ne soit affich\u00e9e \u00e0 l'utilisateur.</li> </ul> <pre><code>graph LR;\n    A[Chat Request] --&gt; B[Inlet];\n    B --&gt; C[LLM Model];\n    C --&gt; D[Outlet];\n    D --&gt; E[Chat Response];\n\n    subgraph Filter Pipeline\n        B--&gt;C;\n        C--&gt;D;\n    end</code></pre> <p>Exemple :</p> <p>Si l'utilisateur demande \"Quelle est la m\u00e9t\u00e9o \u00e0 Paris ?\", le Filter Pipeline peut intercepter la requ\u00eate avant de l\u2019envoyer au LLM, appeler une API m\u00e9t\u00e9o pour obtenir la temp\u00e9rature, et ensuite ajouter cette information dans le message contextuel envoy\u00e9 au mod\u00e8le.</p> <p>Voici un diagramme pour illustrer le flux d'un Filter Pipeline :</p>"},{"location":"2024/09/14/faire-tourner-son-chatbot-avec-une-interface-%C3%A9quivalente-%C3%A0-chatgpt-gr%C3%A2ce-%C3%A0-openwebui/#23-concept-de-pipe","title":"2.3 Concept de Pipe","text":"<p>Un Pipe Pipeline prend enti\u00e8rement en charge le traitement des messages. Il remplace ou enrichit la mani\u00e8re dont le message est g\u00e9r\u00e9 par le LLM. Au lieu de simplement ajouter des informations autour du message, comme dans un Filter Pipeline, le Pipe Pipeline contr\u00f4le tout le processus. Cela inclut :</p> <ul> <li>Appeler diff\u00e9rents mod\u00e8les LLM (comme GPT-4, GPT-3.5, Mistral, etc.) pour r\u00e9pondre directement au message.</li> </ul> <ul> <li>Construire des workflows complexes qui peuvent int\u00e9grer de nouvelles fonctionnalit\u00e9s, comme ex\u00e9cuter du code, consulter des bases de donn\u00e9es, ou r\u00e9cup\u00e9rer des informations.</li> </ul> <ul> <li>RAG (Retrieve and Generate) : Cr\u00e9er un syst\u00e8me complet o\u00f9 les informations sont non seulement r\u00e9cup\u00e9r\u00e9es mais aussi g\u00e9n\u00e9r\u00e9es par un mod\u00e8le choisi.</li> </ul> <p>Exemple :</p> <p>Dans un Pipe Pipeline, si l'utilisateur demande \"Raconte-moi une histoire\", ce pipeline pourrait d\u00e9cider quel mod\u00e8le LLM utiliser (GPT-4, Claude, etc.) et cr\u00e9er une r\u00e9ponse en fonction du workflow configur\u00e9. </p> <p>Voici un diagramme pour illustrer le flux d'un Pipe Pipeline :</p> <pre><code>graph LR;\n    A[Chat Request] --&gt; B[Pipe];\n    B --&gt; C[Chat Response];\n\n    subgraph Pipe Pipeline\n        B;\n    end</code></pre> <p>On parle de pipelines manifold lorsque l\u2019on a un pipe qui sait g\u00e9rer plusieurs mod\u00e8les. En gros, c\u2019est la m\u00eame logique d'impl\u00e9mentation, mais le LLM utilis\u00e9 pour le chat va diff\u00e9rer. Un peu plus bas, j'ai impl\u00e9ment\u00e9 un pipe qui sert de ChatGPT, o\u00f9 je peux choisir quel mod\u00e8le utiliser entre GPT-3.5, GPT-4, ou GPT-mini.</p>"},{"location":"2024/09/14/faire-tourner-son-chatbot-avec-une-interface-%C3%A9quivalente-%C3%A0-chatgpt-gr%C3%A2ce-%C3%A0-openwebui/#24-differences","title":"2.4. Diff\u00e9rences","text":"<p>La diff\u00e9rence principale entre un Filter Pipeline et un Pipe (ou Manifold) Pipeline repose sur le moment et la mani\u00e8re dont les donn\u00e9es sont trait\u00e9es avant ou apr\u00e8s l'appel \u00e0 un mod\u00e8le de langage (LLM).</p>"},{"location":"2024/09/14/faire-tourner-son-chatbot-avec-une-interface-%C3%A9quivalente-%C3%A0-chatgpt-gr%C3%A2ce-%C3%A0-openwebui/#3-implementation-de-pipelines","title":"3. Impl\u00e9mentation de pipelines","text":""},{"location":"2024/09/14/faire-tourner-son-chatbot-avec-une-interface-%C3%A9quivalente-%C3%A0-chatgpt-gr%C3%A2ce-%C3%A0-openwebui/#31-pipeline-simple","title":"3.1. Pipeline simple","text":"<p>Voici un exemple d'impl\u00e9mentation d'un pipeline basique, qui utilise l'API OpenAI pour r\u00e9pondre aux messages utilisateur.</p> <pre><code>from typing import List, Union, Generator, Iterator\nfrom pydantic import BaseModel\nimport os\nimport requests\n\nclass Pipeline:\n    class Valves(BaseModel):\n        OPENAI_API_KEY: str = os.getenv(\"OPENAI_API_KEY\", \"my-keys\")\n\n    def __init__(self):\n        self.name = \"OpenAI Pipeline GPT3.5\"\n        self.valves = self.Valves()\n\n    def pipe(self, user_message: str, model_id: str, messages: List[dict], body: dict) -&gt; Union[str, Generator, Iterator]:\n        headers = {\"Authorization\": f\"Bearer {self.valves.OPENAI_API_KEY}\", \"Content-Type\": \"application/json\"}\n        payload = {**body, \"model\": model_id}\n        self._clean_payload(payload)\n\n        try:\n            response = requests.post(url=\"https://api.openai.com/v1/chat/completions\", json=payload, headers=headers)\n            response.raise_for_status()\n            return response.json()\n        except requests.RequestException as e:\n            return f\"Error: {e}\"\n\n    @staticmethod\n    def _clean_payload(payload: dict):\n        keys_to_remove = {\"user\", \"chat_id\", \"title\"}\n        for key in keys_to_remove:\n            payload.pop(key, None)\n</code></pre>"},{"location":"2024/09/14/faire-tourner-son-chatbot-avec-une-interface-%C3%A9quivalente-%C3%A0-chatgpt-gr%C3%A2ce-%C3%A0-openwebui/#32-pipeline-manifold","title":"3.2 Pipeline manifold","text":"<p>Un pipeline manifold (multi-mod\u00e8le) permet de g\u00e9rer plusieurs mod\u00e8les d'IA en parall\u00e8le. Voici un exemple qui inclut plusieurs mod\u00e8les d'OpenAI. Desormais, je peux utiliser mon pipeline et avoir acc\u00e8s \u00e0 n'importe quel mod\u00e8le d'openai.</p> <pre><code>from typing import List, Union, Generator, Iterator\nfrom pydantic import BaseModel\nimport os\nimport requests\n\nclass Pipeline:\n    class Valves(BaseModel):\n        OPENAI_API_BASE_URL: str = \"https://api.openai.com/v1\"\n        OPENAI_API_KEY: str = os.getenv(\"OPENAI_API_KEY\", \"your-openai-api-key\")\n\n    def __init__(self, name: str = \"manifold: \"):\n        self.type = \"manifold\"\n        self.name = name\n        self.valves = self.Valves()\n        self.pipelines = self.get_openai_models()\n\n    def get_openai_models(self):\n        predefined_model_ids = ['gpt-4', 'gpt-3.5-turbo', 'gpt-4o-2024-08-06', 'gpt-4o-mini']\n        return [{'id': model_id, 'name': model_id} for model_id in predefined_model_ids]\n\n    def pipe(self, user_message: str, model_id: str, messages: List[dict], body: dict) -&gt; Union[str, Generator, Iterator]:\n        headers = {\"Authorization\": f\"Bearer {self.valves.OPENAI_API_KEY}\", \"Content-Type\": \"application/json\"}\n        payload = {**body, \"model\": model_id}\n        self._clean_payload(payload)\n\n        try:\n            response = requests.post(url=f\"{self.valves.OPENAI_API_BASE_URL}/chat/completions\", json=payload, headers=headers)\n            response.raise_for_status()\n            return response.json()\n        except requests.RequestException as e:\n            return f\"Error: {e}\"\n\n    @staticmethod\n    def _clean_payload(payload: dict):\n        keys_to_remove = {\"user\", \"chat_id\", \"title\"}\n        for key in keys_to_remove:\n            payload.pop(key, None)\n</code></pre>"},{"location":"2024/09/14/faire-tourner-son-chatbot-avec-une-interface-%C3%A9quivalente-%C3%A0-chatgpt-gr%C3%A2ce-%C3%A0-openwebui/#33-pour-aller-plus-loin","title":"3.3 Pour aller plus loin :","text":"<p>Il n'y a rien de mieux que la documentation officielle. Vous y trouverez une pl\u00e9thore d'exemples d'impl\u00e9mentation de pipelines que vous pourriez personnaliser. Vous trouverez plus d'une cinquantaine d'exemples ici : exemples de pipelines</p>"},{"location":"2024/09/14/faire-tourner-son-chatbot-avec-une-interface-%C3%A9quivalente-%C3%A0-chatgpt-gr%C3%A2ce-%C3%A0-openwebui/#4-construire-votre-stack-avec-docker-compose","title":"4. Construire votre stack avec Docker Compose","text":""},{"location":"2024/09/14/faire-tourner-son-chatbot-avec-une-interface-%C3%A9quivalente-%C3%A0-chatgpt-gr%C3%A2ce-%C3%A0-openwebui/#41-motivation","title":"4.1. Motivation","text":"<p>L'une des mani\u00e8res les plus efficaces de mettre en place cette architecture est d\u2019utiliser Docker Compose. Voici un exemple de configuration pour orchestrer les services n\u00e9cessaires au fonctionnement de votre chatbot.</p> <p>Selon la documentation officielle pour installer open-webui/open-webui, une commande telle que :</p> <pre><code>docker run -d -p 3000:8080 -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama\n</code></pre> <p>est recommand\u00e9e. D'autres m\u00e9thodes d'installation sont aussi d\u00e9taill\u00e9es sur le site officiel ici.</p> <p>En utilisant cette commande, l'interface est d\u00e9j\u00e0 pr\u00eate \u00e0 l'emploi. Cependant, pour la communication avec l'UI, https://github.com/open-webui/pipelines propose une configuration simplifi\u00e9e via Docker :</p> <p>Ex\u00e9cutez le conteneur Pipelines avec la commande suivante :</p> <pre><code>docker run -d -p 9099:9099 --add-host=host.docker.internal:host-gateway -v pipelines:/app/pipelines --name pipelines --restart always ghcr.io/open-webui/pipelines:main\n</code></pre> <p>Ensuite, connectez Open WebUI :</p> <ul> <li>Allez dans Settings &gt; Connections &gt; OpenAI API dans Open WebUI.</li> <li>Configurez l'URL de l'API \u00e0 <code>http://localhost:9099</code> et la cl\u00e9 API \u00e0 <code>0p3n-w3bu!</code>. Vos pipelines devraient maintenant \u00eatre actifs.</li> </ul> <p>Cependant, la connexion n\u2019est pas toujours aussi simple \ud83d\ude05. La documentation n\u2019est pas encore optimale. Je vous conseille de bien explorer Pipelines sur le site et de comprendre comment l\u2019interaction entre les deux services devrait se faire pour am\u00e9liorer la communication.</p> <p>Parfois, des pipelines sont d\u00e9j\u00e0 disponibles et vous pouvez vous inspirer des exemples ici : Pipelines Exemples. Cependant, l\u2019int\u00e9gration est une autre affaire. Apr\u00e8s plusieurs essais, j\u2019ai r\u00e9ussi \u00e0 connecter les deux services en ajustant des variables cl\u00e9s comme REQUIREMENTS_PATH, PYTHONPATH, et d'autres, gr\u00e2ce \u00e0 des volumes de copie pour les pipelines.</p>"},{"location":"2024/09/14/faire-tourner-son-chatbot-avec-une-interface-%C3%A9quivalente-%C3%A0-chatgpt-gr%C3%A2ce-%C3%A0-openwebui/#42-exemple-de-solution-avec-docker-compose","title":"4.2. Exemple de solution avec docker-compose","text":"<pre><code>services:\n  open-webui:\n    image: ghcr.io/open-webui/open-webui:main\n    container_name: open-webui\n    volumes:\n      - open-webui:/app/backend/data\n    ports:\n      - ${OPEN_WEBUI_PORT-3000}:8080\n    environment:\n      - WEBUI_SECRET_KEY=\n      - OPENAI_API_BASE_URL=http://pipelines:9099\n      - OPENAI_API_KEY=0p3n-w3bu!\n      - ENABLE_OLLAMA_API=false\n    extra_hosts:\n      - host.docker.internal:host-gateway\n    restart: unless-stopped\n\n  pipelines:\n    image: ghcr.io/open-webui/pipelines:main\n    container_name: pipelines\n    volumes:\n      - ./chat_pipelines/pipelines:/app/pipelines\n      - ./chat_pipelines/openwebui_utils:/app/openwebui_utils\n      - ./src/onepiece_bot:/app/onepiece_bot\n      - ./requirements.txt:/app/requirements_custom.txt\n    extra_hosts:\n      - host.docker.internal:host-gateway\n    environment:\n      - OPENAI_API_KEY=${OPENAI_API_KEY}\n      - PIPELINES_DIR=${PIPELINES_DIR}\n      - RESET_PIPELINES_DIR=${RESET_PIPELINES_DIR}\n      - PIPELINES_REQUIREMENTS_PATH=${PIPELINES_REQUIREMENTS_PATH}\n      - PYTHONPATH=/app\n    restart: unless-stopped\n    ports:\n      - 9099:9099\n\nvolumes:\n  open-webui:\n</code></pre> <p>Comme vous travaillez avec Docker, vous pouvez facilement inspecter ce qui se passe et v\u00e9rifier si tout fonctionne correctement ou non. Le service UI (OpenWebUI) fonctionne g\u00e9n\u00e9ralement tr\u00e8s bien ; cependant, il faut porter une attention particuli\u00e8re \u00e0 Pipelines. </p> <p>Pour d\u00e9boguer, vous pouvez ex\u00e9cuter une commande comme <code>docker ps</code> ou <code>docker logs pipelines</code>. Si vous utilisez Docker Desktop, vous devriez voir quelque chose de similaire \u00e0 ceci, montrant que vos deux conteneurs sont en cours d'ex\u00e9cution :</p> <p></p> <p>Les logs sont disponibles en cliquant sur les noms de chaque service :</p> <p></p> <p>Ici, on voit que tout fonctionne bien ! \ud83d\ude0a</p> <p>Pour un exemple complet d'impl\u00e9mentation de bout en bout et comment l'ex\u00e9cuter, je vous invite \u00e0 consulter mon d\u00e9p\u00f4t GitHub o\u00f9 vous trouverez des exemples et des pipelines pr\u00eats \u00e0 l'emploi : Mon d\u00e9p\u00f4t GitHub. Vous y trouverez \u00e9galement des informations sur les variables d'environnement et d'autres configurations utiles.</p>"},{"location":"2024/09/14/faire-tourner-son-chatbot-avec-une-interface-%C3%A9quivalente-%C3%A0-chatgpt-gr%C3%A2ce-%C3%A0-openwebui/#conclusion","title":"Conclusion","text":"<p>En combinant OpenWebUI et une architecture pipeline, il est possible de cr\u00e9er un UI de conversion flexible commme celui de chatgpt, tout en ayant un contr\u00f4le total sur l'interface utilisateur et la logique de traitement des requ\u00eates. Que vous soyez un passionn\u00e9 de LLM ou simplement curieux de tester, OpenWebUI avec Pipelines offre une base solide pour innover. Comme le projet est actuellment nouveau, n'hesitez pas \u00e0 suivre de pr\u00e8s leurs evolutions.</p>"},{"location":"2024/08/04/setup-emr/","title":"Setup emr","text":""},{"location":"2024/08/04/setup-emr/#comment-creer-et-configurer-un-cluster-emr-on-ec2","title":"Comment Cr\u00e9er et Configurer un Cluster  EMR on EC2","text":"<p>Cr\u00e9er un cluster EMR (Elastic MapReduce) sur EC2 peut sembler complexe, surtout si vous vous lancez pour la premi\u00e8re fois. Dans cet article, je vais vous guider \u00e0 travers chaque \u00e9tape, depuis la cr\u00e9ation des r\u00f4les jusqu\u2019\u00e0 la connexion \u00e0 JupyterHub via un tunnel SSH. Ce guide est con\u00e7u pour vous fournir des explications claires et d\u00e9taill\u00e9es afin que vous puissiez configurer votre cluster sans difficult\u00e9. Nous nous concentrerons sp\u00e9cifiquement sur EMR sur EC2. La petite histoire est que j'ai pass\u00e9 des heures \u00e0 expliquer cela \u00e0 plusieurs \u00e9tudiants int\u00e9ress\u00e9s par EMR. J'ai donc d\u00e9cid\u00e9 de r\u00e9diger cet article pour aider un plus grand nombre de personnes \u00e0 comprendre et \u00e0 configurer un cluster EMR de mani\u00e8re efficace. </p> <p>EMR ne faisant pas partie des services gratuit du compte tier, il faut prevoir entre 1 \u00e0 5 euros de facture sur aws.</p>"},{"location":"2024/08/04/setup-emr/#1-creer-un-role-emr-avec-les-permissions-necessaires","title":"1. Cr\u00e9er un R\u00f4le EMR avec les Permissions N\u00e9cessaires","text":""},{"location":"2024/08/04/setup-emr/#pourquoi-est-ce-important","title":"Pourquoi est-ce important?","text":"<p>Avant de pouvoir cr\u00e9er un cluster EMR, vous devez configurer les r\u00f4les et permissions n\u00e9cessaires pour permettre \u00e0 Amazon Web Services (AWS) de g\u00e9rer votre cluster en toute s\u00e9curit\u00e9. Ces r\u00f4les permettent \u00e0 votre cluster d\u2019interagir avec d'autres services AWS comme S3, EC2, et EMR lui-m\u00eame.</p>"},{"location":"2024/08/04/setup-emr/#etape-1-creation-dun-role-emr-pour-ec2","title":"\u00c9tape 1: Cr\u00e9ation d'un R\u00f4le EMR pour EC2","text":"<ol> <li> <p>Acc\u00e9der \u00e0 la console IAM:    Connectez-vous \u00e0 votre compte AWS et rendez-vous sur la console IAM. IAM (Identity and Access Management) est l'endroit o\u00f9 vous g\u00e9rez les permissions et les r\u00f4les pour vos services AWS.</p> </li> <li> <p>Cr\u00e9er un nouveau r\u00f4le:</p> <ul> <li>Cliquez sur R\u00f4les dans le menu de gauche, puis sur Cr\u00e9er un r\u00f4le.</li> <li>S\u00e9lectionnez EMR comme type de r\u00f4le, puis choisissez EC2 comme service. Cela signifie que vous cr\u00e9ez un r\u00f4le qui sera utilis\u00e9 par les instances EC2 dans votre cluster EMR.</li> </ul> </li> <li> <p>Ajouter les politiques de permissions:</p> <ul> <li>AmazonS3FullAccess : Permet \u00e0 votre cluster de lire et d\u2019\u00e9crire dans des buckets S3, essentiel pour stocker les donn\u00e9es et les journaux de votre cluster.</li> <li>AmazonEC2FullAccess : Permet \u00e0 EMR de g\u00e9rer les instances EC2 (d\u00e9marrage, arr\u00eat, configuration).</li> <li>AmazonElasticMapReduceFullAccess : Donne \u00e0 EMR un acc\u00e8s complet pour g\u00e9rer toutes les op\u00e9rations li\u00e9es \u00e0 votre cluster.</li> </ul> </li> </ol> <p>Lorsque vous configurerez votre cluster, s\u00e9lectionnez ce r\u00f4le sous EC2 instance profile pour permettre \u00e0 votre cluster d\u2019utiliser ces permissions.</p> <p></p> <p>Astuce: Les permissions sont cruciales pour la s\u00e9curit\u00e9. Accordez les permissions minimales n\u00e9cessaires pour accomplir vos t\u00e2ches.</p>"},{"location":"2024/08/04/setup-emr/#2-creer-une-cle-ssh-pour-ec2","title":"2. Cr\u00e9er une Cl\u00e9 SSH pour EC2","text":""},{"location":"2024/08/04/setup-emr/#pourquoi-en-avez-vous-besoin","title":"Pourquoi en avez-vous besoin?","text":"<p>Une cl\u00e9 SSH est n\u00e9cessaire pour se connecter \u00e0 distance aux instances EC2 de votre cluster. Cette connexion vous permet d'administrer les n\u0153uds du cluster, d\u2019installer des logiciels suppl\u00e9mentaires ou de d\u00e9boguer directement sur le serveur.</p>"},{"location":"2024/08/04/setup-emr/#etape-2-creation-dune-paire-de-cles-ssh","title":"\u00c9tape 2: Cr\u00e9ation d'une Paire de Cl\u00e9s SSH","text":"<ol> <li> <p>Acc\u00e9der \u00e0 la console EC2:    Dans la console AWS, acc\u00e9dez \u00e0 la section EC2 pour g\u00e9rer vos instances et autres ressources li\u00e9es \u00e0 EC2.</p> </li> <li> <p>Cr\u00e9er une paire de cl\u00e9s:</p> <ul> <li>Dans le menu de gauche, s\u00e9lectionnez Key Pairs sous la section Network &amp; Security.</li> <li>Cliquez sur Create Key Pair.</li> <li>Donnez un nom \u00e0 votre cl\u00e9 et choisissez le format souhait\u00e9 (PEM pour Linux/Mac, PPK pour Windows avec PuTTY).</li> <li>T\u00e9l\u00e9chargez la cl\u00e9. Cette cl\u00e9 vous permettra de vous connecter en SSH \u00e0 vos instances EC2.</li> </ul> </li> </ol> <p>Note: Gardez cette cl\u00e9 en s\u00e9curit\u00e9. Si vous la perdez, vous ne pourrez pas vous reconnecter \u00e0 votre instance.</p>"},{"location":"2024/08/04/setup-emr/#3-creation-et-configuration-du-cluster-emr","title":"3. Cr\u00e9ation et Configuration du Cluster EMR","text":""},{"location":"2024/08/04/setup-emr/#pourquoi-cette-etape-est-elle-cruciale","title":"Pourquoi cette \u00e9tape est-elle cruciale?","text":"<p>La configuration du cluster est le c\u0153ur de l\u2019op\u00e9ration. C\u2019est ici que vous choisissez les logiciels que vous voulez installer, les types d'instances \u00e0 utiliser, et les options de mise en r\u00e9seau.</p>"},{"location":"2024/08/04/setup-emr/#etape-3-creer-un-cluster-emr","title":"\u00c9tape 3: Cr\u00e9er un Cluster EMR","text":"<ol> <li> <p>Cr\u00e9er le cluster:</p> <ul> <li>Allez sur la console EMR et cliquez sur Create cluster.</li> <li>Donnez un nom \u00e0 votre cluster. Par exemple, \"Cluster-DataScience\".</li> </ul> </li> <li> <p>Choisir les composants logiciels:</p> <ul> <li>EMR propose plusieurs logiciels que vous pouvez installer directement lors de la cr\u00e9ation du cluster. Pour un environnement de science des donn\u00e9es, incluez TensorFlow, JupyterHub (pour g\u00e9rer vos notebooks), et Zeppelin.  </li> </ul> </li> <li> <p>Choisir les types d'instances:</p> <ul> <li>Master node: Le n\u0153ud ma\u00eetre coordonne toutes les t\u00e2ches du cluster. Une instance <code>m5.xlarge</code> est un bon choix pour \u00e9quilibrer co\u00fbt et performance.</li> <li>Core nodes: Ces n\u0153uds ex\u00e9cutent les t\u00e2ches. Vous pouvez opter pour des instances plus \u00e9conomiques si vous avez un budget serr\u00e9.</li> </ul> </li> <li> <p>Configurer le volume EBS:</p> <ul> <li>Par d\u00e9faut, EMR vous propose un volume racine EBS pour chaque instance. Vous pouvez g\u00e9n\u00e9ralement accepter cette valeur par d\u00e9faut.</li> </ul> </li> <li> <p>Configurer le r\u00e9seau (VPC et sous-r\u00e9seau):</p> <ul> <li>Assurez-vous de s\u00e9lectionner un sous-r\u00e9seau public pour acc\u00e9der \u00e0 votre cluster via SSH et Web. Si vous n'avez pas de VPC disponible, cr\u00e9ez-le ainsi:  </li> </ul> </li> <li> <p>Ajouter des actions Bootstrap:</p> <ul> <li>Les actions Bootstrap sont des scripts ex\u00e9cut\u00e9s sur chaque n\u0153ud lors du d\u00e9marrage. Cr\u00e9ez un fichier avec l'extension <code>.sh</code> que vous garderez dans votre S3 et fournissez-le \u00e0 EMR via son URI :    <pre><code>#!/bin/bash -xe\nsudo pip install -U \\\n  awscli            \\\n  boto3             \\\n  wheel             \\\n  s3fs              \\\n  fsspec            \\\n  pyarrow\nsudo pip install -U pandas pillow scikit-learn tensorflow\n</code></pre></li> <li>Ce script installe des biblioth\u00e8ques Python couramment utilis\u00e9es dans le traitement de donn\u00e9es. Il est essentiel de r\u00e9aliser ces actions \u00e0 cette \u00e9tape pour que les packages soient install\u00e9s sur l'ensemble des machines du cluster, et non uniquement sur le driver (comme ce serait le cas si vous ex\u00e9cutiez ces commandes directement dans le notebook JupyterHub ou dans la console EMR).</li> </ul> </li> <li> <p>Configurer la persistance des notebooks Jupyter:</p> <ul> <li>Configurez JupyterHub pour qu\u2019il sauvegarde automatiquement vos notebooks sur un bucket S3 en ajoutant ce param\u00e8tre dans Software settings:    <pre><code>[\n    {\n        \"Classification\": \"jupyter-s3-conf\",\n        \"Properties\": {\n            \"s3.persistence.enabled\": \"true\",\n            \"s3.persistence.bucket\": \"MyJupyterBackups\"\n        }\n    }\n]\n</code></pre></li> </ul> </li> <li> <p>Configurer les journaux du cluster:</p> <ul> <li>Assurez-vous que les journaux du cluster sont publi\u00e9s sur S3. Cela vous aidera \u00e0 diagnostiquer tout probl\u00e8me apr\u00e8s coup.</li> </ul> </li> </ol> <p>Rappel: La cr\u00e9ation du cluster peut prendre entre 5 et 10 minutes, et vous commencerez \u00e0 \u00eatre factur\u00e9 d\u00e8s que le cluster d\u00e9marre.</p> <p>Une fois pr\u00eat, vous verrez vos applications list\u00e9es, mais elles ne seront pas encore accessibles. </p>"},{"location":"2024/08/04/setup-emr/#4-configuration-du-tunnel-ssh-vers-le-nud-maitre","title":"4. Configuration du Tunnel SSH vers le N\u0153ud Ma\u00eetre","text":""},{"location":"2024/08/04/setup-emr/#pourquoi-configurer-un-tunnel-ssh","title":"Pourquoi configurer un tunnel SSH?","text":"<p>Le n\u0153ud ma\u00eetre de votre cluster est prot\u00e9g\u00e9 derri\u00e8re un pare-feu, et les applications comme JupyterHub ne sont accessibles que via le r\u00e9seau local du cluster. Le tunnel SSH vous permet de contourner cette restriction en cr\u00e9ant un pont s\u00e9curis\u00e9 entre votre machine locale et le n\u0153ud ma\u00eetre.</p>"},{"location":"2024/08/04/setup-emr/#etape-4-creation-du-tunnel-ssh","title":"\u00c9tape 4: Cr\u00e9ation du Tunnel SSH","text":""},{"location":"2024/08/04/setup-emr/#41-ouverture-du-port-22","title":"4.1 Ouverture du Port 22","text":"<ol> <li> <p>Configurer les autorisations:</p> <ul> <li>Sur la console EC2, allez dans Security Groups.</li> <li>Modifiez le groupe de s\u00e9curit\u00e9 attach\u00e9 au n\u0153ud ma\u00eetre de votre cluster.</li> <li>Ajoutez une r\u00e8gle pour autoriser les connexions SSH (port 22) depuis n'importe quelle IP, ou restreignez-la \u00e0 votre IP pour plus de s\u00e9curit\u00e9. Modifiez le groupe de s\u00e9curit\u00e9 associ\u00e9 aux <code>master</code>. Il ressemblera \u00e0 ceci :  </li> </ul> <p>\u00c0 la fin, vous devriez avoir des r\u00e8gles qui ressemblent \u00e0 ceci :  </p> </li> </ol>"},{"location":"2024/08/04/setup-emr/#42-etablissement-du-tunnel-ssh","title":"4.2 \u00c9tablissement du Tunnel SSH","text":"<ol> <li> <p>R\u00e9cup\u00e9rer les informations de connexion:</p> <ul> <li>Sur la console EMR, acc\u00e9dez \u00e0 l\u2019onglet Summary de votre cluster.</li> <li>Cliquez sur Enable Web Connection pour g\u00e9n\u00e9rer la commande SSH n\u00e9cessaire pour \u00e9tablir le tunnel.</li> </ul> </li> <li> <p>Utilisation de PuTTY pour Windows:</p> <ul> <li>Si vous \u00eates sur Windows, utilisez PuTTY pour \u00e9tablir le tunnel SSH en configurant les param\u00e8tres avec votre cl\u00e9 <code>.ppk</code>.</li> <li>Suivez les \u00e9tapes indiqu\u00e9es sur la console AWS.  </li> </ul> </li> </ol> <p>Astuce: Si vous utilisez Linux ou macOS, ex\u00e9cutez simplement la commande SSH dans votre terminal.</p> <p>Si votre connexion SSH a r\u00e9ussi, vous verrez cette fen\u00eatre : </p>"},{"location":"2024/08/04/setup-emr/#43-configuration-de-switchomega-ou-foxyproxy","title":"4.3 Configuration de SwitchOmega ou FoxyProxy","text":"<ol> <li> <p>Installation de SwitchyOmega:</p> <ul> <li>Pour acc\u00e9der aux applications via le tunnel SSH, configurez votre navigateur pour utiliser ce tunnel. Installez l\u2019extension SwitchyOmega pour Chrome et configurez-la en suivant les instructions d\u00e9taill\u00e9es dans la documentation AWS.</li> </ul> <p>Si cela vous semble compliqu\u00e9, r\u00e9f\u00e9rez-vous aux vid\u00e9os YouTube suivantes :</p> <ul> <li>Vid\u00e9o 1 (\u00c0 partir de 5:10)</li> <li>Vid\u00e9o 2 (\u00c0 partir de 8:00)</li> </ul> </li> </ol> <p>Note: Parfois, il peut \u00eatre n\u00e9cessaire de red\u00e9marrer le navigateur ou attendre quelques minutes pour que l'extension fonctionne correctement.</p>"},{"location":"2024/08/04/setup-emr/#5-connexion-a-jupyterhub","title":"5. Connexion \u00e0 JupyterHub","text":""},{"location":"2024/08/04/setup-emr/#etape-5-acceder-a-jupyterhub","title":"\u00c9tape 5: Acc\u00e9der \u00e0 JupyterHub","text":"<ol> <li> <p>Connexion \u00e0 JupyterHub:</p> <ul> <li>Une fois le tunnel SSH configur\u00e9 et votre navigateur param\u00e9tr\u00e9, vous devriez voir JupyterHub list\u00e9 parmi les applications disponibles sur votre cluster EMR.</li> <li>Cliquez sur JupyterHub pour ouvrir l\u2019interface de connexion.  </li> </ul> <ul> <li>Utilisez les identifiants par d\u00e9faut (login: <code>jovyan</code>, password: <code>jupyter</code>) pour vous connecter.</li> </ul> </li> </ol>"},{"location":"2024/08/04/setup-emr/#conclusion","title":"Conclusion","text":"<p>F\u00e9licitations! Vous avez maintenant un cluster EMR pleinement op\u00e9rationnel, avec JupyterHub configur\u00e9 et accessible via un tunnel SSH s\u00e9curis\u00e9. Vous \u00eates pr\u00eat \u00e0 tirer parti de la puissance de calcul d'Amazon EMR pour vos projets de science des donn\u00e9es, d\u2019analyse, ou de machine learning.</p> <p>Si vous avez des questions ou des difficult\u00e9s, n'h\u00e9sitez pas \u00e0 me contacter. Bonne chance avec votre cluster EMR et n'oubliez pas de l'\u00e9teindre une fois termin\u00e9!</p>"},{"location":"2024/09/01/tests-for-data-scientists/","title":"Tests for data scientists","text":""},{"location":"2024/09/01/tests-for-data-scientists/#parlons-de-tests-pour-data-scientists-mlops","title":"Parlons de tests pour data scientists - MLOPS","text":"<p>Dans cet article, je vais partager avec vous une pratique qui a transform\u00e9 ma carri\u00e8re de data scientist end-to-end : tester son code. C'est une \u00e9tape cruciale dans l'adoption des bonnes pratiques MLOps, mais qui est souvent n\u00e9glig\u00e9e par beaucoup de data scientists. Je vais explorer les raisons pour lesquelles les tests sont souvent peu utilis\u00e9s dans ce domaine et comment vous pouvez int\u00e9grer des tests efficaces dans le workflow de vos projets de data science. Vous trouverez \u00e9galement un tutoriel pratique pour vous aider \u00e0 d\u00e9marrer, en utilisant des outils comme pytest, beartype, pandera, unittest, ou pydantic.</p>"},{"location":"2024/09/01/tests-for-data-scientists/#1-les-types-de-tests","title":"1. Les types de tests","text":"<p>Les tests en d\u00e9veloppement proviennent du g\u00e9nie logiciel. On imagine souvent une pyramide avec diff\u00e9rents niveaux de tests, chacun ayant son r\u00f4le pour garantir que le logiciel fonctionne correctement. Cependant, la mani\u00e8re dont les tests sont abord\u00e9s d\u00e9pend beaucoup des t\u00e2ches sp\u00e9cifiques que vous r\u00e9alisez en tant que data scientist. Voici un aper\u00e7u des principaux types de tests, illustr\u00e9s dans le contexte d'un projet de data science.</p>"},{"location":"2024/09/01/tests-for-data-scientists/#tests-unitaires","title":"Tests unitaires","text":"<p>Les tests unitaires sont \u00e0 la base de la pyramide des tests. Ils sont utilis\u00e9s pour v\u00e9rifier chaque fonction ou composant de mani\u00e8re isol\u00e9e. En data science, cela peut inclure :</p> <ul> <li>V\u00e9rifier le format et le type des donn\u00e9es (par exemple, s'assurer qu'une colonne est toujours un entier).</li> <li>Tester les param\u00e8tres d'un mod\u00e8le pour valider leur conformit\u00e9.</li> <li>Contr\u00f4ler les variables d'entr\u00e9e pour d\u00e9tecter les anomalies ou les valeurs hors des plages attendues.</li> <li>Tester les performances d\u2019un mod\u00e8le sur des jeux de donn\u00e9es sp\u00e9cifiques.</li> </ul> <p>Ces tests sont particuli\u00e8rement utiles lorsque vous d\u00e9veloppez des fonctions de transformation de donn\u00e9es ou des algorithmes personnalis\u00e9s. Ils permettent de v\u00e9rifier la stabilit\u00e9 du code \u00e0 chaque changement et d'\u00e9viter des erreurs fr\u00e9quentes, comme un format de donn\u00e9es incorrect.</p>"},{"location":"2024/09/01/tests-for-data-scientists/#tests-dintegration","title":"Tests d\u2019int\u00e9gration","text":"<p>Les tests d'int\u00e9gration s'assurent que diff\u00e9rents composants fonctionnent correctement ensemble, ce qui est essentiel dans les projets o\u00f9 plusieurs parties du syst\u00e8me doivent collaborer. Par exemple, dans un pipeline de donn\u00e9es complet (de la collecte au pr\u00e9traitement jusqu\u2019au mod\u00e8le et \u00e0 la production des r\u00e9sultats), ces tests v\u00e9rifient que toutes les \u00e9tapes s'encha\u00eenent sans erreur. Souvent, ils sont int\u00e9gr\u00e9s dans un cadre de CI/CD (Int\u00e9gration et D\u00e9ploiement Continus) pour automatiser et garantir la coh\u00e9rence des tests.</p>"},{"location":"2024/09/01/tests-for-data-scientists/#tests-systemes","title":"Tests syst\u00e8mes","text":"<p>Les tests syst\u00e8mes, ou tests \"bo\u00eete noire\", \u00e9valuent le logiciel dans son ensemble dans des sc\u00e9narios d'utilisation r\u00e9els. Ils sont particuli\u00e8rement utiles pour les projets qui aboutissent \u00e0 des applications pr\u00eates \u00e0 \u00eatre utilis\u00e9es par des utilisateurs finaux. </p> <ul> <li>Exemple : V\u00e9rifier que votre mod\u00e8le d\u00e9ploy\u00e9 sur une application web fait des pr\u00e9dictions et renvoie les r\u00e9sultats correctement.</li> </ul> <p>Ces tests sont souvent effectu\u00e9s par des \u00e9quipes ind\u00e9pendantes pour garantir une \u00e9valuation impartiale de l'ensemble du syst\u00e8me.</p>"},{"location":"2024/09/01/tests-for-data-scientists/#tests-dacceptation","title":"Tests d\u2019acceptation","text":"<p>Au sommet de la pyramide se trouvent les tests d\u2019acceptation, qui v\u00e9rifient que le produit final r\u00e9pond aux attentes du client ou de l\u2019utilisateur. Ces tests sont g\u00e9n\u00e9ralement r\u00e9alis\u00e9s par le Product Owner, le client, ou le sponsor du projet, plut\u00f4t que par les d\u00e9veloppeurs. </p> <ul> <li>Exemple : Pour une application de recommandation, s'assurer que les suggestions fournies aux utilisateurs sont pertinentes et align\u00e9es avec les crit\u00e8res d\u00e9finis par le client.</li> </ul> <p>Ces tests sont essentiels dans les projets avec des parties prenantes d\u00e9finies (client, Product Owner, sponsor, expert m\u00e9tier) qui doivent valider que le produit r\u00e9pond bien aux besoins sp\u00e9cifi\u00e9s.</p>"},{"location":"2024/09/01/tests-for-data-scientists/#2-pourquoi-les-tests-sont-essentiels-en-data-science","title":"2. Pourquoi les tests sont essentiels en data science","text":"<p>Faire des tests ou non d\u00e9pend vraiment de tes t\u00e2ches quotidiennes. Si tu es en exploration de donn\u00e9es ou en phase de recherche, tu pourrais moins sentir le besoin d'automatiser des tests rigoureux. Mais d\u00e8s que tu commences \u00e0 mettre des choses en production, les tests deviennent indispensables. Voil\u00e0 pourquoi :</p> <ul> <li>Gagner du temps \u00e0 long terme : Attraper les erreurs t\u00f4t \u00e9vite de devoir corriger des bugs co\u00fbteux apr\u00e8s coup.</li> <li>Assurer la fiabilit\u00e9 : Les tests automatis\u00e9s garantissent que les mod\u00e8les continuent de fonctionner correctement, m\u00eame apr\u00e8s des mises \u00e0 jour du code.</li> <li>Documentation vivante : Des tests bien r\u00e9dig\u00e9s peuvent te servir de documentation. Ils montrent exactement comment chaque partie du syst\u00e8me est cens\u00e9e fonctionner.</li> </ul> <p>La documentation est souvent n\u00e9glig\u00e9e, mais c'est crucial car les data scientists ou les d\u00e9veloppeurs bougent beaucoup entre les entreprises. Si le code est bien document\u00e9 et test\u00e9, la personne qui le reprend pourra plus facilement s'approprier le projet.</p> <p>Bref, l'id\u00e9e, c'est de s'adapter \u00e0 ton contexte. Si tu codes r\u00e9guli\u00e8rement des fonctions critiques, mets en place des tests unitaires et d'int\u00e9gration. Si tu d\u00e9veloppes un produit pour des utilisateurs finaux, pense aux tests syst\u00e8mes et d'acceptation. Si tu es un data scientist qui fait des \u00e9tudes ad hoc ou travaille \u00e9troitement avec des m\u00e9tiers, alors les tests d'acceptation suffiront probablement. Mais si tu es un ML engineer ou un AI engineer, les tests deviennent une necessit\u00e9.</p>"},{"location":"2024/09/01/tests-for-data-scientists/#3-pourquoi-les-data-scientists-negligent-ils-les-tests","title":"3. Pourquoi les data scientists n\u00e9gligent-ils les tests ?","text":"<p>En data science, les tests sont souvent n\u00e9glig\u00e9s pour plusieurs raisons :</p> <ul> <li>Manque d'exp\u00e9rience en ing\u00e9nierie logicielle : Beaucoup de data scientists viennent d\u2019un background acad\u00e9mique (stats, maths, physique, data) sans formation solide en d\u00e9veloppement logiciel.</li> <li>Pression pour produire des r\u00e9sultats rapides : Les entreprises poussent souvent pour des prototypes rapides, ce qui laisse peu de temps pour les tests rigoureux.</li> <li>Mauvaise compr\u00e9hension de la valeur des tests : Les data scientists ne voient pas toujours comment les tests peuvent faciliter leur travail en production.</li> <li>Data scientist \u2260 d\u00e9veloppeur : M\u00eame si c'est un d\u00e9bat, je fais partie de ceux qui pensent qu'un data scientist n'a pas n\u00e9cessairement besoin d'\u00eatre un bon d\u00e9veloppeur. N\u00e9anmoins, quand tu veux faire du ML engineering ou du end-to-end data science, coder en respectant les bonnes pratiques devient une n\u00e9cessit\u00e9. N'oublie pas que l'une des premi\u00e8res d\u00e9finitions d'un data scientist a \u00e9t\u00e9 : \"I think of data scientists as knowing more about statistics than computer scientists and more about computer science than statisticians.\" \u2014 Michael O'Connell.</li> </ul>"},{"location":"2024/09/01/tests-for-data-scientists/#4-lexperience-et-les-bonnes-pratiques","title":"4. L'exp\u00e9rience et les bonnes pratiques","text":"<p>En production, les erreurs co\u00fbtent cher. Les tests te permettent d'\u00e9viter des bugs r\u00e9p\u00e9titifs et d'\u00e9conomiser du temps. En lisant des tests bien \u00e9crits, tu comprends le fonctionnement du syst\u00e8me sans documentation suppl\u00e9mentaire.</p> <p>Les d\u00e9veloppeurs se bonifient avec les projets qu'ils font et gr\u00e2ce \u00e0 leurs collaborations avec d'autres d\u00e9veloppeurs. Si je fais des tests aujourd'hui, c'est gr\u00e2ce \u00e0 ces seniors ou d\u00e9veloppeurs qui ne juraient que par les tests et m'ont oblig\u00e9 \u00e0 le faire avant de valider mes <code>merge merquest</code>. </p> <p>Bonnes pratiques :</p> <ul> <li>Commencez par les tests unitaires simples.</li> <li>N\u2019essayez pas de tout couvrir \u00e0 100 % d\u00e8s le d\u00e9but. Concentrez-vous sur les parties critiques.</li> <li>Ne testez pas les biblioth\u00e8ques tierces ; concentrez-vous sur votre code.</li> <li>Automatisez les tests dans votre pipeline CI/CD ou avec des hooks Git pour d\u00e9tecter les erreurs avant de d\u00e9ployer.</li> </ul>"},{"location":"2024/09/01/tests-for-data-scientists/#5-les-tests-en-data-science-et-les-outils-utiles","title":"5. Les tests en data Science et les outils utiles","text":"<p>Pour les data scientists, il existe plusieurs outils pour \u00e9crire des tests :</p> <ul> <li>Pandera : Pour valider les DataFrames Pandas (eh oui, les data scientists adorent Pandas !).</li> <li>Pytest ou Unittest : Pour \u00e9crire des tests unitaires et d'int\u00e9gration.</li> <li>Beartype ou Pydantic : Pour la validation de types dans vos fonctions.</li> </ul>"},{"location":"2024/09/01/tests-for-data-scientists/#1-exemple-de-tests-implicites-avec-pandera","title":"1. Exemple de tests implicites avec pandera","text":"<p>Lorsque l'on travaille avec des DataFrames, il est bien connu que la gestion des types dans pandas peut \u00eatre probl\u00e9matique, en particulier avec le type <code>object</code>. Pandera est une biblioth\u00e8que qui facilite la validation des DataFrames en permettant de d\u00e9finir des contraintes sur les colonnes et les sch\u00e9mas attendus. Pour utiliser Pandera pour valider la structure d'un DataFrame, voici un exemple simple. Vous pouvez consulter la documentation officielle pour plus de d\u00e9tails : Documentation de Pandera. L'argument <code>strict</code> permet de valider les colonnes sp\u00e9cifi\u00e9es dans le DataFrame.</p> <pre><code>import pandas as pd\nimport pandera as pa\nfrom pandera import Column, DataFrameSchema, Check\n\n# D\u00e9finition de schema\nschema = DataFrameSchema({\n    \"country\": Column(str, Check(lambda x: len(x) &gt; 0)),  # Nom de pays non vide\n    \"population\": Column(int, Check(lambda x: x &gt;= 0)),   # Population doit \u00eatre un entier non n\u00e9gatif\n    \"superficies\": Column(float, Check(lambda x: x &gt; 0)) , \n\n},strict='filter')\n\n# Exemple de DataFrame qui respecte le sch\u00e9ma\nvalid_df = pd.DataFrame({\n    \"country\": [\"France\", \"Burkina Faso\"],\n    \"population\": [67000000, 20000000],\n    \"superficies\": [551695.0, 274200.0],\n    \"Category\" :[\"A\", \"C\"]\n})\n\n# Exemple de DataFrame qui ne respecte  pas le sch\u00e9ma\ninvalid_df = pd.DataFrame({\n    \"country\": [\"France\", \"Burkina Faso\"],\n    \"population\": [\"67 millions\", 20000000],  # Erreur : population devrait \u00eatre un entier\n    \"superficies\": [551695.0, 274200.0],\n     \"Category\" :[\"A\", \"C\"]\n\n})\n\n@pa.check_input(schema)\ndef process_data(df):\n    return df.assign(density=df[\"population\"]/df[\"superficies\"])\n</code></pre> <p>En appliquant ce code, vous obtenez : </p> <p> On peut facilement voir que la fonction a <code>raise</code> une erreur de schema d\u00e8s l'appel.  Combiner Pandera avec Pytest permet de valider les donn\u00e9es. En data science, il est fr\u00e9quent de faire passer des DataFrames en entr\u00e9e de fonction, donc il est pr\u00e9f\u00e9rable de valider les types en entr\u00e9e et d'\u00e9viter tout probl\u00e8me.</p>"},{"location":"2024/09/01/tests-for-data-scientists/#2-exemple-de-validation-des-types-avec-beartype","title":"2. Exemple de validation des types avec beartype","text":"<p>Beartype peut \u00eatre utilis\u00e9 pour v\u00e9rifier les types de donn\u00e9es \u00e0 l'ex\u00e9cution.  Beartype est un validateur de types Python qui v\u00e9rifie que les valeurs pass\u00e9es \u00e0 une fonction correspondent bien aux annotations de type d\u00e9finies. C'est une excellente mani\u00e8re de renforcer la robustesse de ton code sans avoir \u00e0 \u00e9crire de tests manuels pour chaque param\u00e8tre de fonction.</p> <p>La documentation est ici : Beartype. Vous pourriez utiliser Pydantic \u00e0 la place :</p> <pre><code>from beartype import beartype\n\n@beartype\ndef add_numbers(a: int, b: int) -&gt; int:\n    return a + b\n</code></pre> <p></p> <p>Comme on le voit, lorsque le type est viol\u00e9, la fonction l\u00e8ve une erreur.</p>"},{"location":"2024/09/01/tests-for-data-scientists/#6-implementation-de-tests-plus-completes","title":"6. Impl\u00e9mentation de tests plus compl\u00e8tes","text":""},{"location":"2024/09/01/tests-for-data-scientists/#1-organisation-du-projet","title":"1. Organisation du projet","text":"<p>Pour un projet de data science orient\u00e9 MLOps, une organisation claire et structur\u00e9e est essentielle pour faciliter le d\u00e9veloppement, les tests, et la maintenance du code. Un bon sch\u00e9ma de projet permet non seulement de rendre le code plus lisible, mais aussi de simplifier l'int\u00e9gration des tests \u00e0 chaque \u00e9tape du pipeline, depuis le pr\u00e9traitement des donn\u00e9es jusqu'au d\u00e9ploiement des mod\u00e8les.</p> <p>Imaginons le projet ci-dessous o\u00f9 PYTHONPATH pointe vers <code>mypackages</code> et  voici une structure typique qui peut servir de base solide :</p> <pre><code>project/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 main.py\n\u2502   \u2514\u2500\u2500 mypackages/\n\u2502       \u251c\u2500\u2500 algo.py \n\u2502       \u2514\u2500\u2500 __init__.py\n\u2502\n\u2514\u2500\u2500 test/\n    \u251c\u2500\u2500 __init__.py  # Optionnel mais utile pour les tests\n    \u251c\u2500\u2500 test_algo.py\n\u2514\u2500\u2500 test-data/\n    \u251c\u2500\u2500 profil_1.json\n    \u251c\u2500\u2500 profil_2.json\n    \u251c\u2500\u2500 profil_3.json\n    \u251c\u2500\u2500 expected_profil_1.json\n    \u251c\u2500\u2500 expected_profil_2.json\n    \u2514\u2500\u2500 expected_profil_3.json\n</code></pre>"},{"location":"2024/09/01/tests-for-data-scientists/#2-simple-test-unitaire","title":"2. Simple test unitaire","text":"<p>Le fichier <code>test_algo1.py</code> contient les lignes de test unitaire suivantes :</p> <pre><code>from mypackages.algo import Algorithm_1\n\ndef test_algorithm_1():\n    input_data = [1, 2, 3]\n    algorithm_1 = Algorithm_1()\n    result = algorithm_1.predict(input_data)\n    expected = [1, 4, 9]\n    assert result == expected\n</code></pre> <p>Avec la commande <code>pytest</code>, vous pouvez ex\u00e9cuter ce test unitaire. L'organisation du projet peut varier, chacun s'organise comme il le souhaite.</p>"},{"location":"2024/09/01/tests-for-data-scientists/#3-plusieurs-tests-unitaires-en-un-avec-pytestmarkparametrize","title":"3. Plusieurs tests unitaires en Un avec <code>pytest.mark.parametrize</code>","text":"<p><code>pyest.mark.parametrize</code> : C'est une fonctionnalit\u00e9 de Pytest qui permet de tester une m\u00eame fonction avec diff\u00e9rents ensembles de donn\u00e9es. Cela t'aide \u00e0 v\u00e9rifier que ton code fonctionne correctement avec une large vari\u00e9t\u00e9 d'entr\u00e9es, et c'est particuli\u00e8rement utile en data science o\u00f9 les variations de donn\u00e9es sont fr\u00e9quentes. Dans ce paragraphe, je mets en place un test en utilisant le decorateur pyest.mark.parametrize.</p> <p>Voici le d\u00e9tail de <code>test_algo2.py</code> : <pre><code>import pytest\nimport os\nimport json\nimport pandas as pd\nfrom mypackages.algo import Algorithm_2  \nfrom loguru import logger\n\ndef get_data_path(filename: str) -&gt; str:\n    return os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\", \"test-data\", filename))\n\ndef load_data(filename: str) -&gt; pd.DataFrame:\n    with open(get_data_path(filename), \"rb\") as f:\n        json_file = json.load(f)\n    return pd.json_normalize(json_file)\n\n@pytest.mark.parametrize(\n    (\"input_file\", \"expected_file\"),\n    [\n        (\"profil_2.json\", \"expected_profil_2.json\"),\n        (\"profil_1.json\", \"expected_profil_1.json\"),\n        (\"profil_3.json\", \"expected_profil_3.json\"),\n    ],\n)\ndef test_algorithm_2(input_file, expected_file):\n    logger.info(f\"testing {input_file}\")\n\n    df_input = load_data(input_file)\n    df_expected = load_data(expected_file) \n\n    algorithm_2 = Algorithm_2()  \n    result = algorithm_2.predict(df_input)  \n\n    pd.testing.assert_frame_equal(result, df_expected)\n</code></pre></p> <p>Comme par magie, en ex\u00e9cutant la commande <code>pytest</code>, tout passe comme vous pouvez le voir dans l'image ci-apr\u00e8s : </p>"},{"location":"2024/09/01/tests-for-data-scientists/#conclusion","title":"Conclusion","text":"<p>Les tests sont une comp\u00e9tence essentielle pour les data scientists, surtout ceux qui visent \u00e0 mettre leurs mod\u00e8les en production ou livrent leur projects \u00e0 d'autres dev. Commencez petit, concentrez-vous sur les parties critiques, et faites des tests une habituddans vos projets. Les outils comme Pandera, Pytest, Beartype, et Pydantic sont l\u00e0 pour vous aider \u00e0 am\u00e9liorer la qualit\u00e9 et la fiabilit\u00e9 de votre code. C'est un investissement qui paie \u00e0 long terme, vous \u00e9vitant de nombreux soucis et vous permettant de livrer des mod\u00e8les plus robustes et maintenables.</p> <p>Pr\u00eat \u00e0 ajouter des tests \u00e0 vos projets de data science ? Commen\u00e7ons d\u00e8s maintenant !</p>"},{"location":"2024/09/01/mlflow--best-practices/","title":"MLflow &amp; best practices","text":"<p>Dans un projet de machine learning, g\u00e9rer les exp\u00e9rimentations et les mod\u00e8les peut rapidement devenir un casse-t\u00eate. Imaginez : l'\u00e9quipe s'agrandit, les exigences fusent de toutes parts\u2026 et l\u00e0, votre chef de projet d\u00e9barque avec une demande sp\u00e9ciale : \"Dis, tu te souviens de ce mod\u00e8le super performant qu'on a test\u00e9 en avril ? On aimerait le comparer avec nos r\u00e9sultats actuels.\"\u00c0 ce moment pr\u00e9cis, \u00e0 moins d'avoir une m\u00e9moire digne d'un \u00e9l\u00e9phant, vous vous retrouvez \u00e0 naviguer fr\u00e9n\u00e9tiquement dans des fichiers Excel. Entre nous, c'est le genre de situation o\u00f9 l'on se dit : \"Pourquoi je n\u2019ai pas tout not\u00e9 quelque part de fa\u00e7on plus propre ?!\"C\u2019est justement l\u00e0 qu\u2019MLflow entre en sc\u00e8ne.</p>"},{"location":"2024/09/01/mlflow--best-practices/#les-composantes-de-mlflow","title":"Les Composantes de MLflow","text":"<p>MLflow est une plateforme open source qui g\u00e8re tout le cycle de vie de vos mod\u00e8les de machine learning. Voici ses principales fonctionnalit\u00e9s pour rendre votre vie (et celle de votre chef de projet) beaucoup plus simple :</p> <ol> <li> <p>MLflow Tracking : Un journal de bord d\u00e9taill\u00e9 qui consigne chaque exp\u00e9rimentation, avec les param\u00e8tres, m\u00e9triques, artefacts, et m\u00eame les versions de code. Plus besoin de deviner quels param\u00e8tres ont \u00e9t\u00e9 utilis\u00e9s pour ce mod\u00e8le de mai dernier !</p> </li> <li> <p>MLflow Projects : Pour structurer vos projets de mani\u00e8re reproductible, plut\u00f4t que d'avoir des scripts \u00e9parpill\u00e9s un peu partout. MLflow Projects aide \u00e0 organiser votre code et vos donn\u00e9es, pour que tout soit toujours en ordre.</p> </li> <li> <p>MLflow Models : D\u00e9ployez facilement vos mod\u00e8les, peu importe le framework utilis\u00e9 (Scikit-learn, TensorFlow, PyTorch, etc.).</p> </li> <li> <p>Model Registry : Gardez une trace de tous vos mod\u00e8les, de leur version, et sachez lesquels sont en production ou en test. C\u2019est comme une biblioth\u00e8que, mais pour vos mod\u00e8les ML.</p> </li> </ol>"},{"location":"2024/09/01/mlflow--best-practices/#pourquoi-utiliser-mlflow","title":"Pourquoi Utiliser MLflow ?","text":"<p>\u00c0 mesure que votre projet de machine learning \u00e9volue, vous faites face \u00e0 plusieurs d\u00e9fis :</p> <ul> <li>Semaine 1 : Les donn\u00e9es changent constamment   Vous commencez avec un jeu de donn\u00e9es initial, mais rapidement, de nouvelles donn\u00e9es arrivent, n\u00e9cessitant des ajustements constants des pipelines et des tests d'int\u00e9gration pour maintenir la coh\u00e9rence.</li> </ul> <ul> <li>Semaine 3 : Collaboration avec l'\u00e9quipe   Un nouveau data scientist rejoint l'\u00e9quipe, apportant de nouvelles id\u00e9es et m\u00e9thodes. Vous devez suivre les contributions de chacun et coordonner les efforts pour \u00e9viter duplications et incoh\u00e9rences.</li> </ul> <ul> <li>Semaine 5 : Multiplication des exp\u00e9rimentations   Vous essayez divers mod\u00e8les et configurations, et chaque exp\u00e9rimentation produit des r\u00e9sultats diff\u00e9rents. Cela devient difficile de suivre quel mod\u00e8le a \u00e9t\u00e9 form\u00e9 avec quels param\u00e8tres et quelles donn\u00e9es.</li> </ul> <ul> <li>Semaine 7 : Demandes croissantes des parties prenantes   Les responsables de projet et les \u00e9quipes m\u00e9tiers demandent des rapports sur les performances des mod\u00e8les et souhaitent voir les meilleurs mod\u00e8les d\u00e9ploy\u00e9s rapidement en production ou en test.</li> </ul> <p>MLflow vous aide \u00e0 r\u00e9pondre efficacement \u00e0 ces d\u00e9fis en fournissant :</p> <ul> <li>Une tra\u00e7abilit\u00e9 compl\u00e8te des exp\u00e9rimentations : Vous pouvez toujours savoir quels param\u00e8tres et donn\u00e9es ont \u00e9t\u00e9 utilis\u00e9s pour chaque mod\u00e8le.</li> <li>Une collaboration facilit\u00e9e : Les contributions de chaque membre de l'\u00e9quipe sont document\u00e9es et accessibles.</li> <li>Une gestion simplifi\u00e9e des d\u00e9ploiements : MLflow permet de d\u00e9ployer facilement les mod\u00e8les les plus performants pour les besoins des \u00e9quipes m\u00e9tiers.</li> </ul>"},{"location":"2024/09/01/mlflow--best-practices/#configuration-de-mlflow","title":"Configuration de MLflow","text":"<p>En fonction de l'utilisation, la configuration de MLflow peut varier. Vous pouvez installer MLflow avec la commande suivante :</p> <pre><code>pip install mlflow\n</code></pre> <p>Pour des pratiques plus MLOps, MLflow est g\u00e9n\u00e9ralement h\u00e9berg\u00e9 sur un serveur sp\u00e9cifique, soit sur une machine virtuelle (via conteneur) ou sur le cloud. Vous pouvez consulter ce projet GitHub pour un exemple de configuration de MLflow.</p> <p></p> Configuration D\u00e9veloppement solo avec Localhost D\u00e9veloppement solo avec Base de Donn\u00e9es Locale D\u00e9veloppement en \u00c9quipe avec Serveur MLflow Sc\u00e9nario Localhost (par d\u00e9faut) Suivi local avec base de donn\u00e9es Suivi distant avec serveur MLflow Cas d'utilisation D\u00e9veloppement solo D\u00e9veloppement solo D\u00e9veloppement en \u00e9quipe Description Par d\u00e9faut, MLflow enregistre les m\u00e9tadonn\u00e9es et artefacts de chaque run dans un r\u00e9pertoire local, <code>mlruns</code>. C'est la m\u00e9thode la plus simple pour d\u00e9marrer sans configuration suppl\u00e9mentaire. Le client MLflow peut se connecter \u00e0 une base de donn\u00e9es compatible SQLAlchemy (par ex. SQLite, PostgreSQL, MySQL). Cela permet une meilleure gestion des donn\u00e9es d'exp\u00e9rimentation sans avoir \u00e0 configurer un serveur. Le serveur de suivi MLflow peut \u00eatre configur\u00e9 avec un proxy HTTP pour les artefacts, permettant de g\u00e9rer les requ\u00eates d'artefacts via le serveur de suivi sans interagir directement avec les services de stockage (S3 par exemple) et une base de donn\u00e9es pour le suivi. Id\u00e9al pour les d\u00e9veloppements en \u00e9quipe avec stockage centralis\u00e9."},{"location":"2024/09/01/mlflow--best-practices/#organisation-du-projet","title":"Organisation du Projet","text":"<p>Pour int\u00e9grer MLflow proprement et efficacement, il est recommand\u00e9 d\u2019isoler le code fonctionnel du code de suivi. Voici une structure de projet exemple :</p> <pre><code>project/\n\u2502\n\u251c\u2500\u2500 main.py                # Script principal d'ex\u00e9cution\n\u2514\u2500\u2500 src/\n    \u251c\u2500\u2500 __init__.py        # Init du package src\n    \u251c\u2500\u2500 modeling.py        # Code pour l'entra\u00eenement et l'\u00e9valuation des mod\u00e8les\n    \u251c\u2500\u2500 plot_utils.py      # Fonctions utilitaires pour les graphiques\n    \u2514\u2500\u2500 mlflow_utils.py    # Fonctions sp\u00e9cifiques \u00e0 MLflow pour enregistrer les r\u00e9sultats\n</code></pre>"},{"location":"2024/09/01/mlflow--best-practices/#exemple-de-code","title":"Exemple de Code","text":""},{"location":"2024/09/01/mlflow--best-practices/#srcmlflow_utilspy","title":"<code>src/mlflow_utils.py</code>","text":"<p>Voici un exemple de fonction pour enregistrer les artefacts avec MLflow :</p> <pre><code>from loguru import logger\nimport matplotlib.pyplot as plt\nimport mlflow\n\ndef log_artifacts(metrics, params, fig, model):\n    \"\"\"\n    Enregistre les param\u00e8tres, m\u00e9triques, graphiques, et mod\u00e8le dans MLflow.\n\n    Args:\n        metrics (dict): Dictionnaire des m\u00e9triques de performance du mod\u00e8le.\n        params (dict): Dictionnaire des param\u00e8tres du mod\u00e8le.\n        fig (plotly figure): Graphique des r\u00e9sultats.\n        model (object): Le mod\u00e8le entra\u00een\u00e9 (par exemple, un mod\u00e8le scikit-learn).\n    \"\"\"\n    try:\n        logger.info(\"Logging parameters to MLflow\")\n        mlflow.log_params(params) \n\n        logger.info(\"Logging metrics to MLflow\")\n        mlflow.log_metrics(metrics) \n\n        logger.info(\"Logging figure to MLflow\")\n        fig_path = \"results_plot.png\"\n        fig.savefig(fig_path)\n        mlflow.log_artifact(fig_path, \"figures\")  \n\n        logger.info(\"Logging model to MLflow\")\n        mlflow.sklearn.log_model(model, \"model\")  \n\n        logger.info(\"All artifacts logged successfully\")\n    except Exception as e:\n        logger.error(f\"Failed to log artifacts: {e}\")\n</code></pre>"},{"location":"2024/09/01/mlflow--best-practices/#srcplot_utilspy","title":"<code>src/plot_utils.py</code>","text":"<p>Voici une fonction pour cr\u00e9er un graphique comparant les valeurs r\u00e9elles et pr\u00e9dites :</p> <pre><code>import plotly.graph_objects as go\n\ndef create_plot(X_test, y_test, y_pred):\n    \"\"\"Cr\u00e9e un graphique comparant les valeurs r\u00e9elles aux valeurs pr\u00e9dites avec des points pour les vraies valeurs et une ligne pour les pr\u00e9dites.\"\"\"\n    fig = go.Figure()\n\n    fig.add_trace(go.Scatter(\n        x=X_test.squeeze(), \n        y=y_test, \n        mode='markers', \n        name='Actual', \n        marker=dict(size=8, color='rgba(152, 0, 0, .8)', line=dict(width=1, color='DarkSlateGrey'))\n    ))\n\n    fig.add_trace(go.Scatter(\n        x=X_test.squeeze(), \n        y=y_pred, \n        mode='lines', \n        name='Predicted',\n        line=dict(color='rgba(0, 152, 0, .8)', width=2)\n    ))\n\n    fig.update_layout(\n\n\n        title='Actual vs Predicted Values',\n        xaxis_title='Feature Value',\n        yaxis_title='Target Value',\n        template='plotly_white'\n    )\n\n    return fig\n</code></pre>"},{"location":"2024/09/01/mlflow--best-practices/#srcmodelingpy","title":"<code>src/modeling.py</code>","text":"<p>Voici un exemple de module modeling: <pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.metrics import mean_squared_error, r2_score\n\ndef simulate_data(seed=42, size=100):\n    \"\"\"Simulates data for training.\"\"\"\n    np.random.seed(seed)\n    X = np.random.rand(size, 1) * 10\n    y = 3 * X.squeeze() + 4 + np.random.randn(size) * 2\n    return pd.DataFrame(data={'X': X.squeeze(), 'y': y})\n\ndef train_model(X_train, y_train, alpha, l1_ratio):\n    \"\"\"Trains an ElasticNet model.\"\"\"\n    model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n    model.fit(X_train, y_train)\n    return model\n\ndef evaluate_model(model, X_test, y_test):\n    \"\"\"Evaluates the model on the test set.\"\"\"\n    y_pred = model.predict(X_test)\n    mse = round(mean_squared_error(y_test, y_pred), 2)\n    r2 = round(r2_score(y_test, y_pred), 2)\n    return y_pred, {\"mse\": mse, \"r2\": r2}\n</code></pre></p>"},{"location":"2024/09/01/mlflow--best-practices/#script-principal-mainpy","title":"Script Principal <code>main.py</code> :","text":"<p>Le script principal utilise log_artifacts pour int\u00e9grer MLflow et suivre l'entra\u00eenement et l'\u00e9valuation du mod\u00e8le.</p> <pre><code>from loguru import logger \nimport argparse\nimport mlflow\nfrom sklearn.model_selection import train_test_split\nfrom modeling import simulate_data, train_model, evaluate_model\nfrom mlflow_utils import log_artifacts\nfrom plot_utils import create_plot\n\n# Constants\nEXPERIMENT_NAME = 'blog-backbone'\nRUN_NAME = \"simple linear regression\"\nREMOTE_SERVER_URL = \"http://mlflow_server:5000\" \n\ndef main(alpha, l1_ratio):\n    \"\"\"Fonction principale pour ex\u00e9cuter l'entra\u00eenement et l'\u00e9valuation du mod\u00e8le.\"\"\"\n    logger.info(\"Starting script execution\")\n\n    logger.info(\"Simulating data\")\n    data = simulate_data()\n    X_train, X_test, y_train, y_test = train_test_split(data[['X']], data['y'], test_size=0.2, random_state=42)\n\n    logger.info(\"Training model with alpha: {} and l1_ratio: {}\".format(alpha, l1_ratio))\n    model = train_model(X_train, y_train, alpha, l1_ratio)\n\n    logger.info(\"Model evaluation\")\n    y_pred, metrics = evaluate_model(model, X_test, y_test)\n\n    logger.info(\"Preparing results\")\n    params = {\n        \"alpha\": alpha,\n        \"l1_ratio\": l1_ratio,\n        \"test_size\": 0.2\n    }\n    fig = create_plot(X_test, y_test, y_pred)\n\n    logger.info(\"Main execution completed\")\n    return metrics, params, fig, model\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"ElasticNet model training script\")\n    parser.add_argument(\"--alpha\", type=float, required=True, help=\"Alpha parameter for ElasticNet\")\n    parser.add_argument(\"--l1_ratio\", type=float, required=True, help=\"L1 ratio parameter for ElasticNet\")\n    args = parser.parse_args()\n    mlflow.set_tracking_uri(REMOTE_SERVER_URL)\n    mlflow.set_experiment(EXPERIMENT_NAME)    \n    with mlflow.start_run(run_name=RUN_NAME):\n        metrics, params, fig, model = main(args.alpha, args.l1_ratio)\n        log_artifacts(metrics, params, fig, model)\n</code></pre>"},{"location":"2024/09/01/mlflow--best-practices/#etape-dexecution","title":"\u00c9tape d'Ex\u00e9cution","text":"<p>Pour ex\u00e9cuter le projet et observer les r\u00e9sultats de l'enregistrement des exp\u00e9rimentations avec MLflow, on pourrait utiliser la proc\u00e9dure ci-apr\u00e8s:</p> <ol> <li> <p>Configuration du PYTHONPATH :    Avant d'ex\u00e9cuter le script principal, configurez le <code>PYTHONPATH</code> pour que Python reconnaisse correctement les modules de votre projet. Sans cette configuration, vous pourriez \u00eatre amen\u00e9 \u00e0 utiliser des chemins relatifs dans le code, ce qui peut compliquer les choses. \u2019utilisation de PYTHONPATH permet de sp\u00e9cifier le r\u00e9pertoire racine de vos modules Python, \u00e9vitant ainsi les chemins relatifs compliqu\u00e9s dans votre code. Cela simplifie l'importation des modules et rend le code plus propre et plus maintenable. Configurez <code>PYTHONPATH</code> avec la commande suivante dans un terminal :</p> <pre><code>export PYTHONPATH=\"src/\"\n</code></pre> </li> <li> <p>Ex\u00e9cution du Script Principal :    Utilisez <code>argparse</code> pour rendre votre script param\u00e9trable depuis la ligne de commande. Voici comment lancer le script avec diff\u00e9rents param\u00e8tres :</p> <pre><code>python -m main --alpha 0.1 --l1_ratio 0  #L2  ridge regression\npython -m main --alpha 0.1 --l1_ratio 1  #L1  lasso regression\n</code></pre> <ul> <li><code>argparse</code> : Rend le script interactif et param\u00e9trable depuis la ligne de commande, facilitant les tests avec diff\u00e9rents param\u00e8tres sans modifier le code source. </li> </ul> </li> </ol> <p>Bien que vous puissiez utiliser des notebooks, il est souvent recommand\u00e9 de travailler avec des fichiers <code>.py</code> pour une meilleure organisation et gestion du projet, notamment pour des projets de prototypage ou d'industrialisation.</p>"},{"location":"2024/09/01/mlflow--best-practices/#interface-utilisateur-mlflow","title":"Interface Utilisateur MLflow","text":"<p>Apr\u00e8s avoir ex\u00e9cut\u00e9 les commandes ci-dessus, vous pouvez v\u00e9rifier que les exp\u00e9rimentations sont correctement enregistr\u00e9es en acc\u00e9dant \u00e0 l'interface utilisateur de MLflow. Vous y trouverez les param\u00e8tres, les m\u00e9triques, les graphiques, et les mod\u00e8les associ\u00e9s \u00e0 chaque exp\u00e9rimentation. Pour acc\u00e9der \u00e0 cette interface, vous devez utiliser l'URL de votre serveur MLflow. Personnellement, j'utilise un reverse proxy pour acc\u00e9der \u00e0 mon service. En mode local, vous pouvez d\u00e9marrer l'interface utilisateur avec la commande <code>mlflow ui</code>.</p> <p></p> <p>Comme le montre l'image ci-dessus, les deux exp\u00e9rimentations que j'ai lanc\u00e9es sont enregistr\u00e9es. Chacune affiche les param\u00e8tres, les m\u00e9triques et les temps d'ex\u00e9cution. Vous pouvez obtenir plus de d\u00e9tails en cliquant sur une exp\u00e9rimentation sp\u00e9cifique.</p> <p></p> <p>Dans la section des artefacts de la premi\u00e8re exp\u00e9rimentation, vous pouvez r\u00e9cup\u00e9rer votre graphique (n'est-ce pas g\u00e9nial ?), ainsi que les m\u00e9tadonn\u00e9es o\u00f9 sont sauvegard\u00e9s les coefficients. Vous trouverez \u00e9galement le mod\u00e8le enregistr\u00e9, que vous pouvez utiliser directement pour faire des pr\u00e9dictions.</p>"},{"location":"2024/09/01/mlflow--best-practices/#conclusion","title":"Conclusion","text":"<p>Alors, la prochaine fois que quelqu\u2019un vous demande de retrouver des resulats datant de trois mois, vous pourrez vous permettre de sourire et dire : \"Pas de probl\u00e8me, je vais chercher \u00e7a dans MLflow.\" Parce qu'avec MLflow, fini le stress des donn\u00e9es \u00e9gar\u00e9es et des param\u00e8tres oubli\u00e9s ; tout est \u00e0 port\u00e9e de clic, pr\u00eat \u00e0 \u00eatre revisit\u00e9, analys\u00e9 et, bien s\u00fbr, utilis\u00e9 \u00e0 bon escient. Quand on utilise du Mlflow c'est que aimerait faire du MLOps et c'est d\u00e9j\u00e0 un bon debut. Faire du code propre vous permettra d'integrer mlfow dans vos projets</p>"},{"location":"2024/09/01/mlflow--best-practices/#references","title":"R\u00e9f\u00e9rences:","text":"<ul> <li>https://mlflow.org/docs/latest/index.html</li> <li>https://mlflow.org/docs/latest/tracking.html#quickstart</li> </ul>"},{"location":"archive/2024/","title":"2024","text":""},{"location":"category/processing/","title":"Processing","text":""},{"location":"category/mlops/","title":"MlOps","text":""},{"location":"category/iagen/","title":"IAgen","text":""},{"location":"category/cicd/","title":"CI/CD","text":""},{"location":"category/documentation/","title":"documentation","text":""},{"location":"category/cloud/","title":"Cloud","text":""}]}