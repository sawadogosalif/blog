{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Personal blog","text":""},{"location":"2024/08/24/all-things-start-with-git/","title":"All things start with git","text":""},{"location":"2024/08/24/all-things-start-with-git/#git-gitlab-et-github-pour-le-cicd","title":"Git, GitLab et GitHub pour le CI/CD","text":"<p>Dans cet article, nous allons explorer comment Git, GitLab, et GitHub sont utilis\u00e9s pour impl\u00e9menter des pipelines CI/CD (Int\u00e9gration Continue et D\u00e9ploiement Continu). Ces outils sont devenus essentiels pour automatiser et g\u00e9rer les processus de d\u00e9veloppement logiciel. Comment vous pouvez les utiliser pour am\u00e9liorer votre flux de travail.</p>"},{"location":"2024/08/24/all-things-start-with-git/#quest-ce-que-git","title":"Qu'est-ce que Git ?","text":"<p>Git est un syst\u00e8me de gestion de version distribu\u00e9. Cela signifie qu'il permet \u00e0 plusieurs d\u00e9veloppeurs de travailler sur un projet de mani\u00e8re simultan\u00e9e sans se marcher sur les pieds. Voici quelques concepts de base de Git :</p> <ul> <li>Repository (D\u00e9p\u00f4t) : C'est un espace de stockage o\u00f9 l'historique de votre projet est enregistr\u00e9. Il contient tous les fichiers et l'historique des modifications.</li> <li>Branch (Branche) : Une branche est une version parall\u00e8le du code sur laquelle vous pouvez travailler ind\u00e9pendamment. Une branche peut \u00eatre fusionn\u00e9e avec la branche principale (<code>main</code> ou <code>master</code>) apr\u00e8s approbation.</li> <li>Commit : Un commit est un enregistrement de changements dans le d\u00e9p\u00f4t. Chaque commit a un identifiant unique qui permet de revenir en arri\u00e8re ou de fusionner des modifications.</li> <li>Merge : C'est l'action de fusionner les changements d'une branche dans une autre.</li> </ul>"},{"location":"2024/08/24/all-things-start-with-git/#comment-fonctionne-git","title":"Comment fonctionne Git ?","text":"<p>Voici une illustration simple du fonctionnement de Git avec un exemple de flux de travail :</p> <pre><code>graph TD;\n    A[Clone du d\u00e9p\u00f4t] --&gt; B[Cr\u00e9ation d'une branche];\n    B --&gt; C[Modification du code];\n    C --&gt; D[Commit des changements];\n    D --&gt; E[Fusion de la branche];\n    E --&gt; F[Push vers le d\u00e9p\u00f4t distant];</code></pre>"},{"location":"2024/08/24/all-things-start-with-git/#explication-du-processus-git","title":"Explication du Processus Git","text":"<ol> <li>Clone du D\u00e9p\u00f4t : Vous commencez par cloner un d\u00e9p\u00f4t existant depuis GitHub ou GitLab vers votre machine locale.</li> <li>Cr\u00e9ation d'une Branche : Vous cr\u00e9ez une nouvelle branche pour travailler sur une fonctionnalit\u00e9 ou une correction de bug.</li> <li>Modification du Code : Vous faites les changements n\u00e9cessaires dans votre code.</li> <li>Commit des Changements : Vous enregistrez vos modifications dans le d\u00e9p\u00f4t local avec un message de commit.</li> <li>Fusion de la Branche : Une fois les modifications pr\u00eates, vous fusionnez votre branche dans la branche principale.</li> <li>Push vers le D\u00e9p\u00f4t Distant : Enfin, vous poussez vos changements vers le d\u00e9p\u00f4t distant pour les partager avec les autres d\u00e9veloppeurs.</li> </ol>"},{"location":"2024/08/24/all-things-start-with-git/#quest-ce-que-gitlab-et-github","title":"Qu'est-ce que GitLab et GitHub ?","text":""},{"location":"2024/08/24/all-things-start-with-git/#github","title":"GitHub","text":"<p>GitHub est une plateforme de d\u00e9veloppement collaboratif qui repose sur Git. Elle est principalement utilis\u00e9e pour h\u00e9berger des d\u00e9p\u00f4ts Git et permet de collaborer sur des projets de mani\u00e8re transparente. GitHub offre \u00e9galement des fonctionnalit\u00e9s de CI/CD via GitHub Actions, qui permettent d'automatiser les tests, les builds, et les d\u00e9ploiements.</p>"},{"location":"2024/08/24/all-things-start-with-git/#gitlab","title":"GitLab","text":"<p>GitLab est une plateforme similaire \u00e0 GitHub, mais avec un ensemble d'outils encore plus complet pour le DevOps. GitLab CI/CD est une fonctionnalit\u00e9 int\u00e9gr\u00e9e qui permet de cr\u00e9er des pipelines pour automatiser les tests, les builds, et les d\u00e9ploiements directement depuis le d\u00e9p\u00f4t GitLab.</p>"},{"location":"2024/08/24/all-things-start-with-git/#fonctionnement-de-la-cicd-avec-gitlab-et-github","title":"Fonctionnement de la CI/CD avec GitLab et GitHub","text":"<ol> <li>GitHub Actions (CI/CD)</li> </ol> <p>GitHub Actions vous permet de cr\u00e9er des workflows pour automatiser les processus de d\u00e9veloppement. Ces workflows sont d\u00e9finis dans un fichier YAML au sein du d\u00e9p\u00f4t.</p> <p>Exemple de workflow pour GitHub Actions :</p> <pre><code>name: CI/CD Pipeline\n\non:\n  push:\n    branches:\n      - main\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v2\n\n      - name: Install dependencies\n        run: pip install -r requirements.txt\n\n      - name: Run tests\n        run: pytest\n</code></pre> <ol> <li>GitLab CI/CD</li> </ol> <p>GitLab CI/CD utilise un fichier <code>.gitlab-ci.yml</code> pour d\u00e9finir les pipelines. Ce fichier d\u00e9crit les \u00e9tapes que GitLab doit suivre pour tester, construire, et d\u00e9ployer le code.</p> <p>Exemple de pipeline pour GitLab CI/CD :</p> <pre><code>stages:\n  - test\n  - build\n  - deploy\n\ntest:\n  stage: test\n  script:\n    - pytest\n\nbuild:\n  stage: build\n  script:\n    - python setup.py sdist\n\ndeploy:\n  stage: deploy\n  script:\n    - scp dist/* user@server:/path/to/deploy/\n</code></pre> <ol> <li>Visualisation du Processus CI/CD</li> </ol> <p>Pour mieux comprendre le processus CI/CD avec Git, GitHub, et GitLab, voici une visualisation avec Mermaid :</p> <pre><code>graph LR\n    A[Push du Code] --&gt; B[Pipeline CI/CD D\u00e9marr\u00e9]\n    B --&gt; C[Tests]\n    C --&gt; D{Tests R\u00e9ussis?}\n    D --&gt;|Oui| E[Build de l'Application]\n    E --&gt; F[D\u00e9ploiement sur Environnement de ProD]\n    D --&gt;|Non| G[Retourne les Erreurs au D\u00e9veloppeur]</code></pre> <p>\u00c9tapes du Processus CI/CD</p> <ol> <li>Push du Code : Le d\u00e9veloppeur pousse son code vers GitLab ou GitHub.</li> <li>Pipeline CI/CD D\u00e9marr\u00e9 : Le push d\u00e9clenche automatiquement un pipeline CI/CD.</li> <li>Tests : Le code est test\u00e9 pour d\u00e9tecter les erreurs ou les bugs.</li> <li>Build : Si les tests r\u00e9ussissent, l'application est construite (compilation, packaging, etc.).</li> <li>D\u00e9ploiement : Enfin, l'application est d\u00e9ploy\u00e9e sur l'environnement de production. Si les tests \u00e9chouent, les erreurs sont retourn\u00e9es au d\u00e9veloppeur pour correction.</li> </ol>"},{"location":"2024/08/24/all-things-start-with-git/#conclusion","title":"Conclusion","text":"<p>Git, GitLab, et GitHub sont des outils puissants pour g\u00e9rer et automatiser le d\u00e9veloppement logiciel. En utilisant Git pour le contr\u00f4le de version et GitLab/GitHub pour le CI/CD, vous pouvez cr\u00e9er un flux de travail robuste qui assure que votre code est test\u00e9, valid\u00e9, et d\u00e9ploy\u00e9 automatiquement \u00e0 chaque changement. Ces pratiques vous permettent de livrer du code de qualit\u00e9 plus rapidement et plus efficacement.</p>"},{"location":"2024/08/25/cicd-pour-les-data-scientists--quand-le-code-se-met-%C3%A0-danser/","title":"CI/CD pour les Data Scientists : Quand le Code Se Met \u00e0 Danser","text":"<p>La collaboration entre data scientists et d\u00e9veloppeurs peut parfois ressembler \u00e0 une partie de ping-pong chaotique : chacun fait rebondir des id\u00e9es et des bouts de code, mais rien ne semble vraiment s'assembler correctement. Heureusement, il existe une solution pour rendre cette danse collaborative plus harmonieuse : la CI/CD (Continuous Integration/Continuous Deployment). Oui, m\u00eame pour les data scientists\u202f! Alors, prenez vos notebooks, ajustez vos lunettes, et d\u00e9couvrons comment transformer cette pagaille en une symphonie bien orchestr\u00e9e :)</p>"},{"location":"2024/08/25/cicd-pour-les-data-scientists--quand-le-code-se-met-%C3%A0-danser/#quest-ce-que-la-cicd","title":"Qu'est-ce que la CI/CD ?","text":"<p>Avant de plonger dans les d\u00e9tails, clarifions ce que signifie CI/CD, surtout pour ceux qui, parmi nous, passent plus de temps \u00e0 jongler des notebooks qu\u2019\u00e0 jongler avec des pipelines. Ou travaillent : generalement en nombre reduits au pr\u00e8s des metiers:</p> <ul> <li>Continuous Integration (CI): une pratique qui consiste \u00e0 int\u00e9grer r\u00e9guli\u00e8rement les modifications du code dans un d\u00e9p\u00f4t central, o\u00f9 elles sont automatiquement test\u00e9es. Imaginez un petit robot qui v\u00e9rifie si chaque ligne de code que vous ajoutez fonctionne bien avec le reste du code, comme un danseur de tango qui s'assure que chaque pas est en harmonie avec le rythme. L'id\u00e9e est de d\u00e9tecter rapidement les erreurs afin qu'elles ne s'accumulent pas comme une pile de vaisselle sale (vous savez, celle qu\u2019on promet de faire plus tard, mais qui finit par devenir un Everest in\u00e9branlable).</li> </ul> <ul> <li>Continuous Deployment (CD): c' est comme la cerise sur le g\u00e2teau. Ici, chaque modification valid\u00e9e (apr\u00e8s les tests de CI) est automatiquement d\u00e9ploy\u00e9e en production. Oui, vous avez bien entendu : plus besoin d'appuyer sur un bouton pour d\u00e9ployer, c'est comme si votre code se d\u00e9ployait tout seul, un peu comme une machine \u00e0 caf\u00e9 qui se pr\u00e9pare elle-m\u00eame une nouvelle tasse de caf\u00e9 d\u00e8s que vous avez termin\u00e9 la pr\u00e9c\u00e9dente. Le r\u00eave, non ?</li> </ul>"},{"location":"2024/08/25/cicd-pour-les-data-scientists--quand-le-code-se-met-%C3%A0-danser/#cicd-pour-les-data-scientists-pourquoi","title":"CI/CD pour les Data Scientists : Pourquoi ?","text":"<p>Pendant longtemps, la cicd etait propre aux developpeurs : Bonne pratique de developpement (devops). Avec le developpement de la data science et la volont\u00e9 de maturer les projets data, on a donc commencer \u00e0 entendre parler de MLOPS. Disons que la CICD est une composante pour faire du MlOps.</p> <p>En tant que data scientist, vous pourriez vous demander : \"Pourquoi devrais-je me pr\u00e9occuper de tout ce bazar ? Mes notebooks fonctionnent tr\u00e8s bien tels quels !\" Certes, mais imaginez la sc\u00e8ne : vous travaillez sur un mod\u00e8le hyper complexe, vous l\u2019entra\u00eenez pendant des heures (ou des jours), et puis\u2026 Oups, un autre membre de l'\u00e9quipe modifie le code d'importation des donn\u00e9es, et votre magnifique mod\u00e8le ne fonctionne plus. Catastrophe. Ou plus simplement, vous voudriez suivre l'historique d'un code. Le code marchait -il avant? Difficilement de repondre \u00e0 ces questions \u00e0 priori sans CICD</p>"},{"location":"2024/08/25/cicd-pour-les-data-scientists--quand-le-code-se-met-%C3%A0-danser/#les-enjeux-de-la-collaboration","title":"Les Enjeux de la Collaboration","text":"<p>La collaboration entre plusieurs data scientists (et d\u00e9veloppeurs) sur un m\u00eame projet peut vite devenir compliqu\u00e9e. Chacun a son style, ses m\u00e9thodes, et son code. Comme une recette de cuisine o\u00f9 chaque cuisinier ajoute ses propres ingr\u00e9dients sans se concerter avec les autres, le r\u00e9sultat peut \u00eatre\u2026 surprenant, pour ne pas dire immangeable. </p>"},{"location":"2024/08/25/cicd-pour-les-data-scientists--quand-le-code-se-met-%C3%A0-danser/#la-solution-cicd-pour-data-scientists","title":"La Solution : CI/CD pour Data Scientists","text":"<p>Impl\u00e9menter une cha\u00eene CI/CD dans vos projets de data science permet d'assurer que :</p> <ol> <li> <p>Tous les changements sont test\u00e9s : Vous \u00e9vitez le fameux \"\u00e7a marche sur ma machine !\" en vous assurant que chaque modification est test\u00e9e dans un environnement standardis\u00e9.</p> </li> <li> <p>Le code est toujours pr\u00eat pour la production : Vous pouvez d\u00e9ployer vos mod\u00e8les en production rapidement et en toute confiance, sans avoir \u00e0 passer des jours \u00e0 les v\u00e9rifier manuellement.</p> </li> <li> <p>La documentation et le versionning sont automatiques : Chaque modification est document\u00e9e, et vous pouvez facilement revenir en arri\u00e8re en cas de probl\u00e8me (comme une machine \u00e0 remonter le temps pour votre code).</p> </li> <li> <p>Tout developpeur ou data scientist pourrait reprendre vos travaux sans perdre les cheveux.</p> </li> </ol> <p>C'est quoi git , gitlab ou github dans tout \u00e7a? Je vous invite \u00e0 faire un tour sur cet article au cas o\u00f9 vous n'etes pas tres familier avec des trois mots: github vs gitlab vs git</p>"},{"location":"2024/08/25/cicd-pour-les-data-scientists--quand-le-code-se-met-%C3%A0-danser/#exemple-pour-un-projet-data-science","title":"Exemple pour un Projet Data Science","text":"<p>Dans ce guide, nous allons cr\u00e9er un pipeline CI/CD pour un projet de data science sur GitLab. Nous aborderons la structure du projet, la configuration du pipeline avec GitLab CI/CD, les tests, le d\u00e9ploiement d'une application Dash, et un bonus sur l'utilisation des \u201cgit hooks\u201d pour tester localement avant de pousser les changements. Avant tout parlons de </p>"},{"location":"2024/08/25/cicd-pour-les-data-scientists--quand-le-code-se-met-%C3%A0-danser/#structure-du-projet","title":"Structure du Projet","text":"<p>Voici une structure typique pour un projet de data science utilisant GitLab CI/CD :</p> <pre><code>mon_projet_data_science/\n\u2502\n\u251c\u2500\u2500 .gitlab-ci.yml        # Fichier de configuration pour le pipeline CI/CD\n\u251c\u2500\u2500 requirements.txt      # Fichier listant les d\u00e9pendances Python\n\u251c\u2500\u2500 README.md             # Documentation du projet\n\u251c\u2500\u2500 setup.py              # Script d'installation pour le projet\n\u2502\n\u251c\u2500\u2500 data/                 # R\u00e9pertoire contenant les donn\u00e9es\n\u2502   \u251c\u2500\u2500 raw/              # Donn\u00e9es brutes non trait\u00e9es\n\u2502   \u2514\u2500\u2500 processed/        # Donn\u00e9es pr\u00e9-trait\u00e9es\n\u2502\n\u251c\u2500\u2500 src/                  # R\u00e9pertoire du code source principal\n\u2502   \u251c\u2500\u2500 __init__.py       # Fichier d'initialisation du package Python\n\u2502   \u251c\u2500\u2500 data_loader.py    # Script pour charger et traiter les donn\u00e9es\n\u2502   \u251c\u2500\u2500 model.py          # D\u00e9finition du mod\u00e8le de machine learning\n\u2502   \u2514\u2500\u2500 train_model.py    # Script pour l'entra\u00eenement du mod\u00e8le\n\u2502\n\u251c\u2500\u2500 tests/                # R\u00e9pertoire contenant les tests\n\u2502   \u251c\u2500\u2500 __init__.py       # Fichier d'initialisation du package de tests\n\u2502   \u251c\u2500\u2500 test_data_loader.py  # Tests pour le chargement des donn\u00e9es\n\u2502   \u2514\u2500\u2500 test_model.py     # Tests pour le mod\u00e8le de machine learning\n\u2502\n\u2514\u2500\u2500 notebooks/            # R\u00e9pertoire pour les notebooks Jupyter\n    \u251c\u2500\u2500 exploration.ipynb # Notebook pour l'exploration des donn\u00e9es\n    \u2514\u2500\u2500 analysis.ipynb    # Notebook pour l'analyse des r\u00e9sultats\n</code></pre>"},{"location":"2024/08/25/cicd-pour-les-data-scientists--quand-le-code-se-met-%C3%A0-danser/#exemple-avec-gitlab","title":"Exemple avec gitlab","text":"<p>Cela revient \u00e0 configurer le fichier <code>.gitlab-ci.yml</code> Voici un exemple de configuration pour le pipeline CI/CD :</p> <pre><code>stages:\n  - install\n  - test\n  - lint\n  - train\n  - deploy\n\nvariables:\n  VENV_PATH: .venv\n\nbefore_script:\n  - python3 -m venv $VENV_PATH\n  - source $VENV_PATH/bin/activate\n  - pip install --upgrade pip\n  - pip install -r requirements.txt\n\ninstall:\n  stage: install\n  script:\n    - pip install -r requirements.txt\n  cache:\n    paths:\n      - $VENV_PATH\n\ntest:\n  stage: test\n  script:\n    - pytest tests/\n\nlint:\n  stage: lint\n  script:\n    - flake8 .\n\ntrain:\n  stage: train\n  script:\n    - python src/train_model.py\n\ndeploy:\n  stage: deploy\n  script:\n    - echo \"D\u00e9ploiement de l'application Dash sur le serveur de production...\"\n    - scp -r * user@server:/path/to/deployment\n  only:\n    - main\n</code></pre> <p>Explications des \u00c9tapes du Pipeline :</p> <ol> <li>Install Stage : Installe les d\u00e9pendances Python d\u00e9finies dans <code>requirements.txt</code>.</li> <li>Test Stage : Ex\u00e9cute les tests unitaires avec <code>pytest</code> pour s'assurer que chaque composant fonctionne correctement.</li> <li>Lint Stage : Utilise <code>flake8</code> pour v\u00e9rifier la qualit\u00e9 du code et s'assurer qu'il respecte les bonnes pratiques de codage.</li> <li>Train Stage : Lance l'entra\u00eenement du mod\u00e8le de machine learning en ex\u00e9cutant le script <code>train_model.py</code>.</li> <li>Deploy Stage : D\u00e9ploie l'application Dash sur un serveur distant. Cette \u00e9tape est d\u00e9clench\u00e9e uniquement pour la branche <code>main</code>.</li> </ol>"},{"location":"2024/08/25/cicd-pour-les-data-scientists--quand-le-code-se-met-%C3%A0-danser/#utiliser-des-git-hooks","title":"Utiliser des Git Hooks","text":"<p>Dans le cas o\u00f9 vous ne disposer pas de serveur distant pour lancer vos codes, vous pourriez utiliser  git hooks pour ex\u00e9cuter les tests locaux. Par exemple, un hook <code>pre-push</code> peut \u00eatre utilis\u00e9 pour ex\u00e9cuter les tests avant chaque <code>git push</code>.</p> <ol> <li> <p>Cr\u00e9er un Hook <code>pre-push</code> :</p> <p>Dans le r\u00e9pertoire <code>.git/hooks</code>, cr\u00e9ez un fichier nomm\u00e9 <code>pre-push</code> :</p> <p><pre><code>touch .git/hooks/pre-push\n</code></pre> 2. Rendre le Hook Ex\u00e9cutable : 3. Ajouter le Script pour Ex\u00e9cuter les Tests :</p> <p>Ouvrez le fichier <code>pre-push</code> et ajoutez le script suivant :</p> </li> </ol> <p><pre><code>#!/bin/bash\n\necho \"Ex\u00e9cution des tests locaux avant le push...\"\nsource .venv/bin/activate\npytest tests/\n\nif [ $? -ne 0 ]; then\n    echo \"\u00c9chec des tests. Annulation du push.\"\n    exit 1\nfi\n\necho \"Tests r\u00e9ussis. Push en cours...\"\n</code></pre> Avec ce hook en place, chaque tentative de git push ex\u00e9cutera les tests locaux. Si les tests \u00e9chouent, le push sera annul\u00e9, garantissant ainsi que seul le code valide est pouss\u00e9 vers le d\u00e9p\u00f4t distant.</p>"},{"location":"2024/08/25/cicd-pour-les-data-scientists--quand-le-code-se-met-%C3%A0-danser/#conclusion","title":"Conclusion","text":"<p>En somme, int\u00e9grer la CI/CD dans vos projets de data science est comme apprendre \u00e0 danser le tango avec vos coll\u00e8gues : c'est au d\u00e9but un peu maladroit, mais une fois que vous avez le rythme, vous ne pouvez plus vous en passer. Cela transforme votre fa\u00e7on de travailler, rend vos collaborations plus fluides, et garantit que vos mod\u00e8les sont toujours au top de leur forme.</p> <p>Alors, chers data scientists, pr\u00eats \u00e0 chausser vos chaussures de danse et \u00e0 adopter la CI/CD ? Parce qu\u2019une fois que vous y aurez go\u00fbt\u00e9, vous ne reviendrez jamais en arri\u00e8re. Promis, jur\u00e9.</p> <p>References : - https://martinfowler.com/articles/continuousIntegration.html</p> <ul> <li>https://martinfowler.com/books/duvall.html</li> </ul>"},{"location":"2024/08/18/build-your-blog/","title":"Build your blog","text":""},{"location":"2024/08/18/build-your-blog/#building-and-deploying-a-blog-with-mkdocs-with-ci","title":"Building and Deploying a Blog with MkDocs with CI","text":"<p>Welcome! In this guide, you\u2019ll learn how to build and deploy a blog using MkDocs, a tool that makes it easy to create beautiful documentation. We\u2019ll cover each step to help you get your blog online.</p>"},{"location":"2024/08/18/build-your-blog/#what-you-need","title":"What You Need","text":"<ol> <li>GitHub Account: You need a GitHub account to store and deploy your blog.</li> <li>Basic Knowledge: Familiarity with GitHub, Docker, and some command line basics will be helpful.</li> </ol>"},{"location":"2024/08/18/build-your-blog/#step-1-setting-up-your-blog","title":"Step 1: Setting Up Your Blog","text":"<ol> <li> <p>Create a GitHub Repository:</p> <ul> <li>Visit GitHub and log in.</li> <li>Click on New to create a repository.</li> <li>Name your repository, for example, <code>my-blog</code>.</li> <li>Choose Public or Private based on your preference.</li> <li>Click Create repository.</li> </ul> </li> <li> <p>Install MkDocs Locally:</p> <ul> <li>Open your terminal or command line.</li> <li>Install MkDocs with pip:    <pre><code>pip install mkdocs\n</code></pre></li> </ul> </li> <li> <p>Set Up Your Blog:</p> <ul> <li>Navigate to the folder where you want to create your blog.</li> <li>Run:    <pre><code>mkdocs new my-blog\n</code></pre></li> <li>This creates a folder named <code>my-blog</code> with the basic files for your blog.</li> </ul> </li> <li> <p>Customize Your Blog:</p> <ul> <li>Open the <code>mkdocs.yml</code> file in your blog's folder. This is where you set your blog\u2019s name and theme.</li> <li> <p>Modify the <code>mkdocs.yml</code> file to look like this:</p> <pre><code>site_name: My Blog\ntheme:\n  name: material\nnav:\n  - Home: index.md\n</code></pre> </li> </ul> <ul> <li>Add content by editing <code>index.md</code> or creating new Markdown files in the <code>docs</code> folder.</li> </ul> </li> </ol>"},{"location":"2024/08/18/build-your-blog/#step-2-build-and-deploy-using-docker","title":"Step 2: Build and Deploy Using Docker","text":"<ol> <li> <p>Create a Docker Image:</p> <ul> <li>Write a Dockerfile to include MkDocs and necessary tools.</li> <li> <p>Here\u2019s a basic Dockerfile:</p> <pre><code># Use Python 3.12 image\nFROM python:3.12-slim\n\n# Set the working directory\nWORKDIR /app\n\n# Install MkDocs and plugins\nRUN pip install mkdocs mkdocs-material ghp-import\n\n# Copy blog files into Docker image\nCOPY . /app\n\n# Set command to build MkDocs\nCMD [\"mkdocs\", \"build\", \"--verbose\", \"--site-dir\", \"site\"]\n</code></pre> </li> </ul> <ul> <li>Build the Docker image with:    <pre><code>docker build -t my-blog-image .\n</code></pre></li> </ul> </li> <li> <p>Deploy Your Blog:</p> <ul> <li> <p>Create a GitHub Actions workflow file to automate deployment. Save the following as <code>.github/workflows/deploy.yml</code> in your repository:</p> <pre><code>name: Build and Deploy MkDocs Site\n\non:\n  push:\n    branches:\n      - main\n  pull_request:\n    branches:\n      - main\n  workflow_dispatch:\n\nenv:\n  IMAGE_NAME: my-blog-image\n  IMAGE_TAG: latest\n\njobs:\n  build-and-deploy:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v3\n\n      - name: Build MkDocs site\n        run: |\n          docker run --rm -v ${{ github.workspace }}:/app -w /app ${{ env.IMAGE_NAME }}:${{ env.IMAGE_TAG }} mkdocs build --verbose --site-dir site\n\n      - name: Deploy to GitHub Pages\n        if: github.ref == 'refs/heads/main'\n        run: |\n          docker run --rm -v ${{ github.workspace }}:/app -w /app ${{ env.IMAGE_NAME }}:${{ env.IMAGE_TAG }} /bin/bash -c \"\n            ghp-import -n -p -f site -r https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git -b gh-pages\"\n\n      - name: Clean up Docker resources\n        run: docker system prune -f\n</code></pre> </li> </ul> </li> </ol>"},{"location":"2024/08/18/build-your-blog/#final-steps","title":"Final Steps","text":"<ol> <li> <p>Push Your Changes:</p> <ul> <li>Commit and push your changes to GitHub:    <pre><code>git add .\ngit commit -m \"Set up MkDocs site\"\ngit push origin main\n</code></pre></li> </ul> </li> <li> <p>Ensure Correct Permissions for Deployment:</p> <ul> <li>To avoid permission issues with GitHub Actions, ensure that the <code>GITHUB_TOKEN</code> has the necessary permissions.</li> <li>Check Repository Settings:<ul> <li>Go to your repository on GitHub.</li> <li>Navigate to Settings &gt; Actions &gt; General.</li> <li>Under Workflow permissions, ensure Read and write permissions are selected.</li> </ul> </li> </ul> </li> <li> <p>Verify Deployment:</p> <ul> <li>Go to your GitHub repository.</li> <li>Navigate to Settings &gt; Pages.</li> <li>Ensure the source is set to the <code>gh-pages</code> branch.</li> </ul> <p>Your blog should now be live! Visit the URL provided in the GitHub Pages settings to see your site.</p> </li> </ol>"},{"location":"2024/08/18/build-your-blog/#conclusion","title":"Conclusion","text":"<p>Congratulations! You\u2019ve built and deployed a blog using MkDocs and Docker. This guide aimed to simplify the process, so you can easily share your content online. Happy blogging!</p> <p>For a complete example, check out my project: GitHub Repository.</p> <p>This version of the guide simplifies the instructions and organizes the steps clearly to help you get started with building and deploying your blog.</p>"},{"location":"2024/08/11/iam-for-eks/","title":"Iam for eks","text":""},{"location":"2024/08/11/iam-for-eks/#understanding-iam-roles-and-users-in-aws-with-a-practical-example-running-an-application-on-eks","title":"Understanding IAM Roles and Users in AWS with a Practical Example: Running an Application on EKS","text":"<p>AWS Identity and Access Management (IAM) is a cornerstone of security and access control in AWS environments. IAM allows you to manage users, groups, and roles, and to specify their permissions to access AWS resources. In this article, we will explore the differences between IAM roles and IAM users, provide a practical scenario of deploying an application on Amazon EKS (Elastic Kubernetes Service), and clarify how IAM roles relate to specific AWS console options.</p>"},{"location":"2024/08/11/iam-for-eks/#1-iam-role-vs-iam-user","title":"1. IAM Role vs. IAM User","text":"<p>IAM User: - An IAM user is an entity that represents a person or application that interacts with AWS resources. Each IAM user has a unique set of credentials (username and password or access keys). - IAM users are generally assigned permissions directly or through group memberships. - Example: A developer who needs access to specific AWS resources and can authenticate using their IAM user credentials.</p> <p>IAM Role: - An IAM role is an AWS identity with specific permissions that can be assumed by entities like AWS services, EC2 instances, or even IAM users. - Roles are temporary and are assumed by entities that need to perform specific actions. They do not have permanent credentials; instead, they provide temporary security credentials. - Example: An EC2 instance running a containerized application that needs to pull images from Amazon ECR.</p>"},{"location":"2024/08/11/iam-for-eks/#2-practical-scenario-deploying-an-application-on-eks","title":"2. Practical Scenario: Deploying an Application on EKS","text":"<p>Let\u2019s walk through an example of deploying an application, such as Gradio, on Amazon EKS. Gradio is a popular library for creating machine learning demos.</p> <p>Steps for Deployment:</p> <ol> <li> <p>Set Up IAM Roles for EKS:</p> <ul> <li>Cluster Role: Create an IAM role that the EKS cluster will use. This role allows EKS to manage the underlying EC2 instances and perform other operations.</li> <li>Node Instance Role: Create another IAM role for EC2 instances that will run the Kubernetes worker nodes. This role allows the instances to interact with other AWS services, like pulling images from ECR or writing logs to CloudWatch.</li> </ul> <p>Example IAM Policy for Node Instance Role:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ec2:DescribeInstances\",\n                \"ec2:DescribeTags\",\n                \"ecr:GetAuthorizationToken\",\n                \"ecr:BatchCheckLayerAvailability\",\n                \"ecr:GetDownloadUrlForLayer\",\n                \"ecr:BatchGetImage\",\n                \"logs:CreateLogStream\",\n                \"logs:PutLogEvents\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n</code></pre> </li> <li> <p>Create an EKS Cluster:</p> <ul> <li>Go to the EKS Console.</li> <li>Create a new EKS cluster and associate it with the IAM cluster role created earlier.</li> </ul> </li> <li> <p>Launch EC2 Instances for Kubernetes Nodes:</p> <ul> <li>Launch EC2 instances and associate them with the IAM node instance role. Ensure these instances are part of the EKS node group.</li> </ul> </li> <li> <p>Deploy Gradio Application:</p> <ul> <li>Package your Gradio application into a Docker container and push the image to Amazon ECR.</li> <li>Create a Kubernetes deployment YAML file specifying the container image from ECR and deploy it to your EKS cluster.</li> </ul> <p>Example Kubernetes Deployment YAML:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gradio-app\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: gradio\n  template:\n    metadata:\n      labels:\n        app: gradio\n    spec:\n      containers:\n        - name: gradio-container\n          image: &lt;your-ecr-repository-url&gt;/gradio-app:latest\n          ports:\n            - containerPort: 7860\n</code></pre> </li> <li> <p>Expose the Application:</p> <ul> <li>Create a Kubernetes Service to expose the Gradio application to the internet.</li> </ul> <p>Example Service YAML:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: gradio-service\nspec:\n  selector:\n    app: gradio\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 7860\n  type: LoadBalancer\n</code></pre> </li> </ol>"},{"location":"2024/08/11/iam-for-eks/#3-iam-roles-in-aws-console-options","title":"3. IAM Roles in AWS Console Options","text":"<p>You mentioned having different console options: <code>DATAengROLE</code>, <code>DATASCIENC</code>, and <code>DATAANALYST</code>. These are IAM roles configured for various teams or purposes. Here\u2019s how they might be used:</p> <ul> <li>DATAengROLE: This role could be configured to provide access to data engineering tools and resources, like Apache Airflow, for data engineers. If you have access to Airflow with this role, it means that <code>DATAengROLE</code> includes permissions to view and manage Airflow resources.</li> </ul> <ul> <li>DATASCIENC: This role might be tailored for data scientists, granting access to tools and resources pertinent to data analysis and modeling. The specific permissions and services available to this role would depend on the policies attached.</li> </ul> <ul> <li>DATAANALYST: This role could be for data analysts, providing access to reporting tools or datasets but not necessarily the same resources as the other roles.</li> </ul> <p>Role-Based Access: In your case, when using the <code>DATAengROLE</code>, you have access to Airflow because this role has the necessary permissions configured. Conversely, the <code>DATAANALYST</code> role might not have the same permissions, hence the lack of access to Airflow.</p>"},{"location":"2024/08/11/iam-for-eks/#conclusion","title":"Conclusion","text":"<p>Understanding the difference between IAM roles and IAM users is fundamental for managing access and permissions in AWS. IAM roles are particularly useful for granting temporary access and managing permissions for AWS services and resources, while IAM users are suited for individuals requiring direct access.</p> <p>In the context of deploying an application like Gradio on Amazon EKS, properly configuring IAM roles ensures that your EKS cluster and EC2 instances have the appropriate permissions to interact with other AWS services. Additionally, understanding IAM roles in relation to different AWS console options helps in managing access based on specific roles and responsibilities within your organization.</p>"},{"location":"2024/07/29/mermaid-for-data-scientist/","title":"Mermaid for data scientist","text":""},{"location":"2024/07/29/mermaid-for-data-scientist/#mermaid-pour-documenter-vos-projets","title":"Mermaid pour documenter vos projets","text":"<p>La documentation est une \u00e9tape cruciale dans tout projet de d\u00e9veloppement, mais soyons honn\u00eates, ce n'est pas toujours la plus amusante. Pourtant, quand il s'agit de clarifier des concepts complexes ou de repr\u00e9senter des processus, rien ne vaut un bon diagramme. Beaucoup de gens se tournent vers Word ou PowerPoint, voire des logiciels sp\u00e9cialis\u00e9s comme Lucidchart, pour cr\u00e9er ces illustrations. Mais pour nous, d\u00e9veloppeurs, qui aimons le code et l'automatisation, il existe un outil qui pourrait bien changer la donne : Mermaid.</p> <p>Mermaid permet de cr\u00e9er des diagrammes et des flowcharts directement \u00e0 partir de texte, ce qui vous permet de rester dans votre \u00e9diteur de code pr\u00e9f\u00e9r\u00e9. Si vous avez toujours voulu documenter vos projets avec des diagrammes sans quitter votre environnement de d\u00e9veloppement, ce guide est fait pour vous. D\u00e9couvrez comment Mermaid peut transformer votre approche de la documentation.</p>"},{"location":"2024/07/29/mermaid-for-data-scientist/#preparer-votre-environnement","title":"Pr\u00e9parer votre Environnement","text":"<p>Avant de plonger dans la cr\u00e9ation de diagrammes avec Mermaid, vous devez configurer votre environnement de travail.</p>"},{"location":"2024/07/29/mermaid-for-data-scientist/#extensions-recommandees-pour-vscode","title":"Extensions Recommand\u00e9es pour VSCode","text":"<p>Pour une exp\u00e9rience optimale avec Mermaid, nous vous recommandons d'installer les extensions suivantes dans Visual Studio Code :</p> <ul> <li>Markdown Preview Mermaid Support : Permet d'afficher un aper\u00e7u en temps r\u00e9el de vos diagrammes Mermaid directement dans VSCode avec la commande <code>Ctrl + K V</code>.</li> <li>Markdown PDF : Permet d'exporter vos diagrammes Mermaid en PDF avec la commande <code>Ctrl + Shift + P</code>.</li> </ul> <p>Une fois ces extensions en place, vous \u00eates pr\u00eat \u00e0 cr\u00e9er vos premiers diagrammes avec Mermaid !</p>"},{"location":"2024/07/29/mermaid-for-data-scientist/#etape-1-definir-les-nuds-et-les-relations","title":"\u00c9tape 1 : D\u00e9finir les N\u0153uds et les Relations","text":"<p>Cr\u00e9er un diagramme avec Mermaid commence par la d\u00e9finition des n\u0153uds (ou \u00e9tapes) et des relations entre eux. Voici comment proc\u00e9der :</p>"},{"location":"2024/07/29/mermaid-for-data-scientist/#nuds","title":"N\u0153uds","text":"<p>Les n\u0153uds repr\u00e9sentent diff\u00e9rentes \u00e9tapes ou entit\u00e9s dans votre processus. Chaque n\u0153ud est identifi\u00e9 par un identifiant unique et peut avoir une \u00e9tiquette optionnelle. Voici un exemple simple :</p> <pre><code>graph LR\n    data --&gt; clean;   \n    clean --&gt; explore;   \n    explore --&gt; preprocess; </code></pre> <p>Dans cet exemple, <code>data</code>, <code>clean</code>, <code>explore</code>, et <code>preprocess</code> sont des identifiants de n\u0153uds. Les fl\u00e8ches (<code>--&gt;</code>) indiquent les relations entre eux. Ce diagramme montre un flux de gauche \u00e0 droite gr\u00e2ce \u00e0 la directive <code>graph LR</code>.</p>"},{"location":"2024/07/29/mermaid-for-data-scientist/#relations","title":"Relations","text":"<p>Les relations d\u00e9finissent la s\u00e9quence ou la connexion entre les n\u0153uds. Voici quelques types de relations que vous pouvez utiliser :</p> <ul> <li><code>--&gt;</code> : Relation directionnelle de gauche \u00e0 droite.</li> <li><code>---</code> : Relation horizontale sans fl\u00e8che.</li> <li><code>==&gt;</code> : Relation bidirectionnelle.</li> <li><code>==&gt;|label|</code> : Relation directionnelle avec un label.</li> </ul> <p>Un exemple avec des relations plus complexes et des labels :</p> <pre><code>graph LR\n    data --&gt; clean;       \n    clean &lt;---&gt; explore;       \n    explore ==&gt; preprocess;       \n    preprocess ==&gt;|split| Model; </code></pre>"},{"location":"2024/07/29/mermaid-for-data-scientist/#etape-2-personnaliser-les-formes-et-les-couleurs","title":"\u00c9tape 2 : Personnaliser les Formes et les Couleurs","text":"<p>Mermaid offre une vari\u00e9t\u00e9 de formes pour vos n\u0153uds. Voici un aper\u00e7u des formes les plus couramment utilis\u00e9es :</p> Forme Code Description Rectangle <code>[(...)]</code> N\u0153ud rectangulaire standard Rectangle arrondi <code>[[...]]</code> N\u0153ud avec des bords arrondis Cylindre <code>[(...)]</code> N\u0153ud en forme de cylindre Cercle <code>((...))</code> N\u0153ud en forme de cercle <p>Utilisez ces formes pour personnaliser vos diagrammes en fonction de vos besoins. Par exemple, pour cr\u00e9er un flux de travail de data science :</p> <pre><code>graph LR\n    data[(Data Collection)] --&gt; clean([Data Cleaning]);\n    clean --&gt; explore([Exploratory Data Analysis]);\n    explore --&gt; preprocess([Data Preprocessing]);\n    preprocess --&gt; split([Train/Test Split]);\n    split --&gt; model([Model Training]);\n    model --&gt; evaluate([Model Evaluation]);\n    evaluate --&gt; tune([Model Tuning]);\n    tune --&gt; model; \n    evaluate --&gt; deploy([Model Deployment]);</code></pre>"},{"location":"2024/07/29/mermaid-for-data-scientist/#etape-3-ajuster-la-direction-et-le-style","title":"\u00c9tape 3 : Ajuster la Direction et le Style","text":"<p>Mermaid vous permet de contr\u00f4ler la direction et le style de vos diagrammes gr\u00e2ce \u00e0 quelques mots-cl\u00e9s simples.</p>"},{"location":"2024/07/29/mermaid-for-data-scientist/#direction-du-flux","title":"Direction du Flux","text":"<p>Vous pouvez ajuster la direction du flux avec les options suivantes :</p> <ul> <li><code>LR</code> : De gauche \u00e0 droite (Left to Right).</li> <li><code>TB</code> : De haut en bas (Top to Bottom).</li> <li><code>RL</code> : De droite \u00e0 gauche (Right to Left).</li> <li><code>BT</code> : De bas en haut (Bottom to Top).</li> </ul>"},{"location":"2024/07/29/mermaid-for-data-scientist/#style-des-nuds","title":"Style des N\u0153uds","text":"<p>Pour personnaliser l'apparence des n\u0153uds, vous pouvez utiliser :</p> <ul> <li><code>fill</code> : Couleur de fond du n\u0153ud.</li> <li><code>stroke</code> : Couleur de la bordure du n\u0153ud.</li> <li><code>color</code> : Couleur du texte \u00e0 l'int\u00e9rieur du n\u0153ud.</li> </ul> <p>Voici un exemple combinant direction et style :</p> <pre><code>graph TB\n\n    data((Data Sources))-. DHA .-&gt;DHA_data[Environmental Data];\n    data-. OpenData .-&gt;OpenData[Satellites Imagery];\n    data-. CSV files .-&gt;CSV_files[Operational Data];\n\n    DHA_data--&gt;|Preparation| Store[(Feature Store)];\n    OpenData--&gt;|Preparation| Store[(Feature Store)];\n    CSV_files--&gt;|Preparation| Store[(Feature Store)];\n\n    Store--&gt;|Modeling|Modeling_1[Airports Severity];\n\n    style data fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;\n    style Modeling_1 fill:#9f9,stroke:#000,stroke-width:0px;</code></pre> <p>Ce diagramme utilise un flux de haut en bas (TB) et applique des styles personnalis\u00e9s aux n\u0153uds pour les mettre en valeur visuellement.</p>"},{"location":"2024/07/29/mermaid-for-data-scientist/#ressources-utiles","title":"Ressources Utiles","text":"<p>Pour aller plus loin avec Mermaid, voici quelques liens utiles :</p> <ul> <li>Documentation officielle de Mermaid</li> <li>Guide de syntaxe Mermaid pour les flowcharts</li> <li>\u00c9diteur en ligne Mermaid</li> </ul>"},{"location":"2024/07/29/mermaid-for-data-scientist/#conclusion","title":"Conclusion","text":"<p>Avec Mermaid, documenter vos projets devient non seulement plus simple, mais aussi plus amusant. Fini les allers-retours entre plusieurs outils : vous pouvez cr\u00e9er, personnaliser et int\u00e9grer des diagrammes directement depuis votre code. Que vous soyez en train de planifier un projet, de documenter un processus ou de clarifier une architecture, Mermaid est un atout pr\u00e9cieux pour tout d\u00e9veloppeur. Essayez-le, et vous ne reviendrez plus en arri\u00e8re !</p>"},{"location":"2024/08/04/setup-emr/","title":"Setup emr","text":""},{"location":"2024/08/04/setup-emr/#comment-creer-et-configurer-un-cluster-emr-on-ec2","title":"Comment Cr\u00e9er et Configurer un Cluster  EMR on EC2","text":"<p>Cr\u00e9er un cluster EMR (Elastic MapReduce) sur EC2 peut sembler complexe, surtout si vous vous lancez pour la premi\u00e8re fois. Dans cet article, je vais vous guider \u00e0 travers chaque \u00e9tape, depuis la cr\u00e9ation des r\u00f4les jusqu\u2019\u00e0 la connexion \u00e0 JupyterHub via un tunnel SSH. Ce guide est con\u00e7u pour vous fournir des explications claires et d\u00e9taill\u00e9es afin que vous puissiez configurer votre cluster sans difficult\u00e9. Nous nous concentrerons sp\u00e9cifiquement sur EMR sur EC2. La petite histoire est que j'ai pass\u00e9 des heures \u00e0 expliquer cela \u00e0 plusieurs \u00e9tudiants int\u00e9ress\u00e9s par EMR. J'ai donc d\u00e9cid\u00e9 de r\u00e9diger cet article pour aider un plus grand nombre de personnes \u00e0 comprendre et \u00e0 configurer un cluster EMR de mani\u00e8re efficace. </p> <p>EMR ne faisant pas partie des services gratuit du compte tier, il faut prevoir entre 1 \u00e0 5 euros de facture sur aws.</p>"},{"location":"2024/08/04/setup-emr/#1-creer-un-role-emr-avec-les-permissions-necessaires","title":"1. Cr\u00e9er un R\u00f4le EMR avec les Permissions N\u00e9cessaires","text":""},{"location":"2024/08/04/setup-emr/#pourquoi-est-ce-important","title":"Pourquoi est-ce important?","text":"<p>Avant de pouvoir cr\u00e9er un cluster EMR, vous devez configurer les r\u00f4les et permissions n\u00e9cessaires pour permettre \u00e0 Amazon Web Services (AWS) de g\u00e9rer votre cluster en toute s\u00e9curit\u00e9. Ces r\u00f4les permettent \u00e0 votre cluster d\u2019interagir avec d'autres services AWS comme S3, EC2, et EMR lui-m\u00eame.</p>"},{"location":"2024/08/04/setup-emr/#etape-1-creation-dun-role-emr-pour-ec2","title":"\u00c9tape 1: Cr\u00e9ation d'un R\u00f4le EMR pour EC2","text":"<ol> <li> <p>Acc\u00e9der \u00e0 la console IAM:    Connectez-vous \u00e0 votre compte AWS et rendez-vous sur la console IAM. IAM (Identity and Access Management) est l'endroit o\u00f9 vous g\u00e9rez les permissions et les r\u00f4les pour vos services AWS.</p> </li> <li> <p>Cr\u00e9er un nouveau r\u00f4le:</p> <ul> <li>Cliquez sur R\u00f4les dans le menu de gauche, puis sur Cr\u00e9er un r\u00f4le.</li> <li>S\u00e9lectionnez EMR comme type de r\u00f4le, puis choisissez EC2 comme service. Cela signifie que vous cr\u00e9ez un r\u00f4le qui sera utilis\u00e9 par les instances EC2 dans votre cluster EMR.</li> </ul> </li> <li> <p>Ajouter les politiques de permissions:</p> <ul> <li>AmazonS3FullAccess : Permet \u00e0 votre cluster de lire et d\u2019\u00e9crire dans des buckets S3, essentiel pour stocker les donn\u00e9es et les journaux de votre cluster.</li> <li>AmazonEC2FullAccess : Permet \u00e0 EMR de g\u00e9rer les instances EC2 (d\u00e9marrage, arr\u00eat, configuration).</li> <li>AmazonElasticMapReduceFullAccess : Donne \u00e0 EMR un acc\u00e8s complet pour g\u00e9rer toutes les op\u00e9rations li\u00e9es \u00e0 votre cluster.</li> </ul> </li> </ol> <p>Lorsque vous configurerez votre cluster, s\u00e9lectionnez ce r\u00f4le sous EC2 instance profile pour permettre \u00e0 votre cluster d\u2019utiliser ces permissions.</p> <p></p> <p>Astuce: Les permissions sont cruciales pour la s\u00e9curit\u00e9. Accordez les permissions minimales n\u00e9cessaires pour accomplir vos t\u00e2ches.</p>"},{"location":"2024/08/04/setup-emr/#2-creer-une-cle-ssh-pour-ec2","title":"2. Cr\u00e9er une Cl\u00e9 SSH pour EC2","text":""},{"location":"2024/08/04/setup-emr/#pourquoi-en-avez-vous-besoin","title":"Pourquoi en avez-vous besoin?","text":"<p>Une cl\u00e9 SSH est n\u00e9cessaire pour se connecter \u00e0 distance aux instances EC2 de votre cluster. Cette connexion vous permet d'administrer les n\u0153uds du cluster, d\u2019installer des logiciels suppl\u00e9mentaires ou de d\u00e9boguer directement sur le serveur.</p>"},{"location":"2024/08/04/setup-emr/#etape-2-creation-dune-paire-de-cles-ssh","title":"\u00c9tape 2: Cr\u00e9ation d'une Paire de Cl\u00e9s SSH","text":"<ol> <li> <p>Acc\u00e9der \u00e0 la console EC2:    Dans la console AWS, acc\u00e9dez \u00e0 la section EC2 pour g\u00e9rer vos instances et autres ressources li\u00e9es \u00e0 EC2.</p> </li> <li> <p>Cr\u00e9er une paire de cl\u00e9s:</p> <ul> <li>Dans le menu de gauche, s\u00e9lectionnez Key Pairs sous la section Network &amp; Security.</li> <li>Cliquez sur Create Key Pair.</li> <li>Donnez un nom \u00e0 votre cl\u00e9 et choisissez le format souhait\u00e9 (PEM pour Linux/Mac, PPK pour Windows avec PuTTY).</li> <li>T\u00e9l\u00e9chargez la cl\u00e9. Cette cl\u00e9 vous permettra de vous connecter en SSH \u00e0 vos instances EC2.</li> </ul> </li> </ol> <p>Note: Gardez cette cl\u00e9 en s\u00e9curit\u00e9. Si vous la perdez, vous ne pourrez pas vous reconnecter \u00e0 votre instance.</p>"},{"location":"2024/08/04/setup-emr/#3-creation-et-configuration-du-cluster-emr","title":"3. Cr\u00e9ation et Configuration du Cluster EMR","text":""},{"location":"2024/08/04/setup-emr/#pourquoi-cette-etape-est-elle-cruciale","title":"Pourquoi cette \u00e9tape est-elle cruciale?","text":"<p>La configuration du cluster est le c\u0153ur de l\u2019op\u00e9ration. C\u2019est ici que vous choisissez les logiciels que vous voulez installer, les types d'instances \u00e0 utiliser, et les options de mise en r\u00e9seau.</p>"},{"location":"2024/08/04/setup-emr/#etape-3-creer-un-cluster-emr","title":"\u00c9tape 3: Cr\u00e9er un Cluster EMR","text":"<ol> <li> <p>Cr\u00e9er le cluster:</p> <ul> <li>Allez sur la console EMR et cliquez sur Create cluster.</li> <li>Donnez un nom \u00e0 votre cluster. Par exemple, \"Cluster-DataScience\".</li> </ul> </li> <li> <p>Choisir les composants logiciels:</p> <ul> <li>EMR propose plusieurs logiciels que vous pouvez installer directement lors de la cr\u00e9ation du cluster. Pour un environnement de science des donn\u00e9es, incluez TensorFlow, JupyterHub (pour g\u00e9rer vos notebooks), et Zeppelin.  </li> </ul> </li> <li> <p>Choisir les types d'instances:</p> <ul> <li>Master node: Le n\u0153ud ma\u00eetre coordonne toutes les t\u00e2ches du cluster. Une instance <code>m5.xlarge</code> est un bon choix pour \u00e9quilibrer co\u00fbt et performance.</li> <li>Core nodes: Ces n\u0153uds ex\u00e9cutent les t\u00e2ches. Vous pouvez opter pour des instances plus \u00e9conomiques si vous avez un budget serr\u00e9.</li> </ul> </li> <li> <p>Configurer le volume EBS:</p> <ul> <li>Par d\u00e9faut, EMR vous propose un volume racine EBS pour chaque instance. Vous pouvez g\u00e9n\u00e9ralement accepter cette valeur par d\u00e9faut.</li> </ul> </li> <li> <p>Configurer le r\u00e9seau (VPC et sous-r\u00e9seau):</p> <ul> <li>Assurez-vous de s\u00e9lectionner un sous-r\u00e9seau public pour acc\u00e9der \u00e0 votre cluster via SSH et Web. Si vous n'avez pas de VPC disponible, cr\u00e9ez-le ainsi:  </li> </ul> </li> <li> <p>Ajouter des actions Bootstrap:</p> <ul> <li>Les actions Bootstrap sont des scripts ex\u00e9cut\u00e9s sur chaque n\u0153ud lors du d\u00e9marrage. Cr\u00e9ez un fichier avec l'extension <code>.sh</code> que vous garderez dans votre S3 et fournissez-le \u00e0 EMR via son URI :    <pre><code>#!/bin/bash -xe\nsudo pip install -U \\\n  awscli            \\\n  boto3             \\\n  wheel             \\\n  s3fs              \\\n  fsspec            \\\n  pyarrow\nsudo pip install -U pandas pillow scikit-learn tensorflow\n</code></pre></li> <li>Ce script installe des biblioth\u00e8ques Python couramment utilis\u00e9es dans le traitement de donn\u00e9es. Il est essentiel de r\u00e9aliser ces actions \u00e0 cette \u00e9tape pour que les packages soient install\u00e9s sur l'ensemble des machines du cluster, et non uniquement sur le driver (comme ce serait le cas si vous ex\u00e9cutiez ces commandes directement dans le notebook JupyterHub ou dans la console EMR).</li> </ul> </li> <li> <p>Configurer la persistance des notebooks Jupyter:</p> <ul> <li>Configurez JupyterHub pour qu\u2019il sauvegarde automatiquement vos notebooks sur un bucket S3 en ajoutant ce param\u00e8tre dans Software settings:    <pre><code>[\n    {\n        \"Classification\": \"jupyter-s3-conf\",\n        \"Properties\": {\n            \"s3.persistence.enabled\": \"true\",\n            \"s3.persistence.bucket\": \"MyJupyterBackups\"\n        }\n    }\n]\n</code></pre></li> </ul> </li> <li> <p>Configurer les journaux du cluster:</p> <ul> <li>Assurez-vous que les journaux du cluster sont publi\u00e9s sur S3. Cela vous aidera \u00e0 diagnostiquer tout probl\u00e8me apr\u00e8s coup.</li> </ul> </li> </ol> <p>Rappel: La cr\u00e9ation du cluster peut prendre entre 5 et 10 minutes, et vous commencerez \u00e0 \u00eatre factur\u00e9 d\u00e8s que le cluster d\u00e9marre.</p> <p>Une fois pr\u00eat, vous verrez vos applications list\u00e9es, mais elles ne seront pas encore accessibles. </p>"},{"location":"2024/08/04/setup-emr/#4-configuration-du-tunnel-ssh-vers-le-nud-maitre","title":"4. Configuration du Tunnel SSH vers le N\u0153ud Ma\u00eetre","text":""},{"location":"2024/08/04/setup-emr/#pourquoi-configurer-un-tunnel-ssh","title":"Pourquoi configurer un tunnel SSH?","text":"<p>Le n\u0153ud ma\u00eetre de votre cluster est prot\u00e9g\u00e9 derri\u00e8re un pare-feu, et les applications comme JupyterHub ne sont accessibles que via le r\u00e9seau local du cluster. Le tunnel SSH vous permet de contourner cette restriction en cr\u00e9ant un pont s\u00e9curis\u00e9 entre votre machine locale et le n\u0153ud ma\u00eetre.</p>"},{"location":"2024/08/04/setup-emr/#etape-4-creation-du-tunnel-ssh","title":"\u00c9tape 4: Cr\u00e9ation du Tunnel SSH","text":""},{"location":"2024/08/04/setup-emr/#41-ouverture-du-port-22","title":"4.1 Ouverture du Port 22","text":"<ol> <li> <p>Configurer les autorisations:</p> <ul> <li>Sur la console EC2, allez dans Security Groups.</li> <li>Modifiez le groupe de s\u00e9curit\u00e9 attach\u00e9 au n\u0153ud ma\u00eetre de votre cluster.</li> <li>Ajoutez une r\u00e8gle pour autoriser les connexions SSH (port 22) depuis n'importe quelle IP, ou restreignez-la \u00e0 votre IP pour plus de s\u00e9curit\u00e9. Modifiez le groupe de s\u00e9curit\u00e9 associ\u00e9 aux <code>master</code>. Il ressemblera \u00e0 ceci :  </li> </ul> <p>\u00c0 la fin, vous devriez avoir des r\u00e8gles qui ressemblent \u00e0 ceci :  </p> </li> </ol>"},{"location":"2024/08/04/setup-emr/#42-etablissement-du-tunnel-ssh","title":"4.2 \u00c9tablissement du Tunnel SSH","text":"<ol> <li> <p>R\u00e9cup\u00e9rer les informations de connexion:</p> <ul> <li>Sur la console EMR, acc\u00e9dez \u00e0 l\u2019onglet Summary de votre cluster.</li> <li>Cliquez sur Enable Web Connection pour g\u00e9n\u00e9rer la commande SSH n\u00e9cessaire pour \u00e9tablir le tunnel.</li> </ul> </li> <li> <p>Utilisation de PuTTY pour Windows:</p> <ul> <li>Si vous \u00eates sur Windows, utilisez PuTTY pour \u00e9tablir le tunnel SSH en configurant les param\u00e8tres avec votre cl\u00e9 <code>.ppk</code>.</li> <li>Suivez les \u00e9tapes indiqu\u00e9es sur la console AWS.  </li> </ul> </li> </ol> <p>Astuce: Si vous utilisez Linux ou macOS, ex\u00e9cutez simplement la commande SSH dans votre terminal.</p> <p>Si votre connexion SSH a r\u00e9ussi, vous verrez cette fen\u00eatre : </p>"},{"location":"2024/08/04/setup-emr/#43-configuration-de-switchomega-ou-foxyproxy","title":"4.3 Configuration de SwitchOmega ou FoxyProxy","text":"<ol> <li> <p>Installation de SwitchyOmega:</p> <ul> <li>Pour acc\u00e9der aux applications via le tunnel SSH, configurez votre navigateur pour utiliser ce tunnel. Installez l\u2019extension SwitchyOmega pour Chrome et configurez-la en suivant les instructions d\u00e9taill\u00e9es dans la documentation AWS.</li> </ul> <p>Si cela vous semble compliqu\u00e9, r\u00e9f\u00e9rez-vous aux vid\u00e9os YouTube suivantes :</p> <ul> <li>Vid\u00e9o 1 (\u00c0 partir de 5:10)</li> <li>Vid\u00e9o 2 (\u00c0 partir de 8:00)</li> </ul> </li> </ol> <p>Note: Parfois, il peut \u00eatre n\u00e9cessaire de red\u00e9marrer le navigateur ou attendre quelques minutes pour que l'extension fonctionne correctement.</p>"},{"location":"2024/08/04/setup-emr/#5-connexion-a-jupyterhub","title":"5. Connexion \u00e0 JupyterHub","text":""},{"location":"2024/08/04/setup-emr/#etape-5-acceder-a-jupyterhub","title":"\u00c9tape 5: Acc\u00e9der \u00e0 JupyterHub","text":"<ol> <li> <p>Connexion \u00e0 JupyterHub:</p> <ul> <li>Une fois le tunnel SSH configur\u00e9 et votre navigateur param\u00e9tr\u00e9, vous devriez voir JupyterHub list\u00e9 parmi les applications disponibles sur votre cluster EMR.</li> <li>Cliquez sur JupyterHub pour ouvrir l\u2019interface de connexion.  </li> </ul> <ul> <li>Utilisez les identifiants par d\u00e9faut (login: <code>jovyan</code>, password: <code>jupyter</code>) pour vous connecter.</li> </ul> </li> </ol>"},{"location":"2024/08/04/setup-emr/#conclusion","title":"Conclusion","text":"<p>F\u00e9licitations! Vous avez maintenant un cluster EMR pleinement op\u00e9rationnel, avec JupyterHub configur\u00e9 et accessible via un tunnel SSH s\u00e9curis\u00e9. Vous \u00eates pr\u00eat \u00e0 tirer parti de la puissance de calcul d'Amazon EMR pour vos projets de science des donn\u00e9es, d\u2019analyse, ou de machine learning.</p> <p>Si vous avez des questions ou des difficult\u00e9s, n'h\u00e9sitez pas \u00e0 me contacter. Bonne chance avec votre cluster EMR et n'oubliez pas de l'\u00e9teindre une fois termin\u00e9!</p>"},{"location":"2024/09/01/mlflow--best-practices/","title":"MLflow &amp; best practices","text":""},{"location":"2024/09/01/mlflow--best-practices/#introduction","title":"Introduction","text":"<p>Dans un projet de machine learning, g\u00e9rer les exp\u00e9rimentations et les mod\u00e8les peut rapidement devenir un casse-t\u00eate. Imaginez : l'\u00e9quipe s'agrandit, les exigences fusent de toutes parts\u2026 et l\u00e0, votre chef de projet d\u00e9barque avec une demande sp\u00e9ciale : \"Dis, tu te souviens de ce mod\u00e8le super performant qu'on a test\u00e9 en avril ? On aimerait le comparer avec nos r\u00e9sultats actuels.\"\u00c0 ce moment pr\u00e9cis, \u00e0 moins d'avoir une m\u00e9moire digne d'un \u00e9l\u00e9phant, vous vous retrouvez \u00e0 naviguer fr\u00e9n\u00e9tiquement dans des fichiers Excel. Entre nous, c'est le genre de situation o\u00f9 l'on se dit : \"Pourquoi je n\u2019ai pas tout not\u00e9 quelque part de fa\u00e7on plus propre ?!\"C\u2019est justement l\u00e0 qu\u2019MLflow entre en sc\u00e8ne.</p>"},{"location":"2024/09/01/mlflow--best-practices/#les-composantes-de-mlflow","title":"Les Composantes de MLflow","text":"<p>MLflow est une plateforme open source qui g\u00e8re tout le cycle de vie de vos mod\u00e8les de machine learning. Voici ses principales fonctionnalit\u00e9s pour rendre votre vie (et celle de votre chef de projet) beaucoup plus simple :</p> <ol> <li> <p>MLflow Tracking : Un journal de bord d\u00e9taill\u00e9 qui consigne chaque exp\u00e9rimentation, avec les param\u00e8tres, m\u00e9triques, artefacts, et m\u00eame les versions de code. Plus besoin de deviner quels param\u00e8tres ont \u00e9t\u00e9 utilis\u00e9s pour ce mod\u00e8le de mai dernier !</p> </li> <li> <p>MLflow Projects : Pour structurer vos projets de mani\u00e8re reproductible, plut\u00f4t que d'avoir des scripts \u00e9parpill\u00e9s un peu partout. MLflow Projects aide \u00e0 organiser votre code et vos donn\u00e9es, pour que tout soit toujours en ordre.</p> </li> <li> <p>MLflow Models : D\u00e9ployez facilement vos mod\u00e8les, peu importe le framework utilis\u00e9 (Scikit-learn, TensorFlow, PyTorch, etc.).</p> </li> <li> <p>Model Registry : Gardez une trace de tous vos mod\u00e8les, de leur version, et sachez lesquels sont en production ou en test. C\u2019est comme une biblioth\u00e8que, mais pour vos mod\u00e8les ML.</p> </li> </ol>"},{"location":"2024/09/01/mlflow--best-practices/#pourquoi-utiliser-mlflow","title":"Pourquoi Utiliser MLflow ?","text":"<p>\u00c0 mesure que votre projet de machine learning \u00e9volue, vous faites face \u00e0 plusieurs d\u00e9fis :</p> <ul> <li>Semaine 1 : Les donn\u00e9es changent constamment   Vous commencez avec un jeu de donn\u00e9es initial, mais rapidement, de nouvelles donn\u00e9es arrivent, n\u00e9cessitant des ajustements constants des pipelines et des tests d'int\u00e9gration pour maintenir la coh\u00e9rence.</li> </ul> <ul> <li>Semaine 3 : Collaboration avec l'\u00e9quipe   Un nouveau data scientist rejoint l'\u00e9quipe, apportant de nouvelles id\u00e9es et m\u00e9thodes. Vous devez suivre les contributions de chacun et coordonner les efforts pour \u00e9viter duplications et incoh\u00e9rences.</li> </ul> <ul> <li>Semaine 5 : Multiplication des exp\u00e9rimentations   Vous essayez divers mod\u00e8les et configurations, et chaque exp\u00e9rimentation produit des r\u00e9sultats diff\u00e9rents. Cela devient difficile de suivre quel mod\u00e8le a \u00e9t\u00e9 form\u00e9 avec quels param\u00e8tres et quelles donn\u00e9es.</li> </ul> <ul> <li>Semaine 7 : Demandes croissantes des parties prenantes   Les responsables de projet et les \u00e9quipes m\u00e9tiers demandent des rapports sur les performances des mod\u00e8les et souhaitent voir les meilleurs mod\u00e8les d\u00e9ploy\u00e9s rapidement en production ou en test.</li> </ul> <p>MLflow vous aide \u00e0 r\u00e9pondre efficacement \u00e0 ces d\u00e9fis en fournissant :</p> <ul> <li>Une tra\u00e7abilit\u00e9 compl\u00e8te des exp\u00e9rimentations : Vous pouvez toujours savoir quels param\u00e8tres et donn\u00e9es ont \u00e9t\u00e9 utilis\u00e9s pour chaque mod\u00e8le.</li> <li>Une collaboration facilit\u00e9e : Les contributions de chaque membre de l'\u00e9quipe sont document\u00e9es et accessibles.</li> <li>Une gestion simplifi\u00e9e des d\u00e9ploiements : MLflow permet de d\u00e9ployer facilement les mod\u00e8les les plus performants pour les besoins des \u00e9quipes m\u00e9tiers.</li> </ul>"},{"location":"2024/09/01/mlflow--best-practices/#configuration-de-mlflow","title":"Configuration de MLflow","text":"<p>En fonction de l'utilisation, la configuration de MLflow peut varier. Vous pouvez installer MLflow avec la commande suivante :</p> <pre><code>pip install mlflow\n</code></pre> <p>Pour des pratiques plus MLOps, MLflow est g\u00e9n\u00e9ralement h\u00e9berg\u00e9 sur un serveur sp\u00e9cifique, soit sur une machine virtuelle (via conteneur) ou sur le cloud. Vous pouvez consulter ce projet GitHub pour un exemple de configuration de MLflow.</p> <p></p> Configuration D\u00e9veloppement solo avec Localhost D\u00e9veloppement solo avec Base de Donn\u00e9es Locale D\u00e9veloppement en \u00c9quipe avec Serveur MLflow Sc\u00e9nario Localhost (par d\u00e9faut) Suivi local avec base de donn\u00e9es Suivi distant avec serveur MLflow Cas d'utilisation D\u00e9veloppement solo D\u00e9veloppement solo D\u00e9veloppement en \u00e9quipe Description Par d\u00e9faut, MLflow enregistre les m\u00e9tadonn\u00e9es et artefacts de chaque run dans un r\u00e9pertoire local, <code>mlruns</code>. C'est la m\u00e9thode la plus simple pour d\u00e9marrer sans configuration suppl\u00e9mentaire. Le client MLflow peut se connecter \u00e0 une base de donn\u00e9es compatible SQLAlchemy (par ex. SQLite, PostgreSQL, MySQL). Cela permet une meilleure gestion des donn\u00e9es d'exp\u00e9rimentation sans avoir \u00e0 configurer un serveur. Le serveur de suivi MLflow peut \u00eatre configur\u00e9 avec un proxy HTTP pour les artefacts, permettant de g\u00e9rer les requ\u00eates d'artefacts via le serveur de suivi sans interagir directement avec les services de stockage (S3 par exemple) et une base de donn\u00e9es pour le suivi. Id\u00e9al pour les d\u00e9veloppements en \u00e9quipe avec stockage centralis\u00e9."},{"location":"2024/09/01/mlflow--best-practices/#organisation-du-projet","title":"Organisation du Projet","text":"<p>Pour int\u00e9grer MLflow proprement et efficacement, il est recommand\u00e9 d\u2019isoler le code fonctionnel du code de suivi. Voici une structure de projet exemple :</p> <pre><code>project/\n\u2502\n\u251c\u2500\u2500 main.py                # Script principal d'ex\u00e9cution\n\u2514\u2500\u2500 src/\n    \u251c\u2500\u2500 __init__.py        # Init du package src\n    \u251c\u2500\u2500 modeling.py        # Code pour l'entra\u00eenement et l'\u00e9valuation des mod\u00e8les\n    \u251c\u2500\u2500 plot_utils.py      # Fonctions utilitaires pour les graphiques\n    \u2514\u2500\u2500 mlflow_utils.py    # Fonctions sp\u00e9cifiques \u00e0 MLflow pour enregistrer les r\u00e9sultats\n</code></pre>"},{"location":"2024/09/01/mlflow--best-practices/#exemple-de-code","title":"Exemple de Code","text":""},{"location":"2024/09/01/mlflow--best-practices/#srcmlflow_utilspy","title":"<code>src/mlflow_utils.py</code>","text":"<p>Voici un exemple de fonction pour enregistrer les artefacts avec MLflow :</p> <pre><code>from loguru import logger\nimport matplotlib.pyplot as plt\nimport mlflow\n\ndef log_artifacts(metrics, params, fig, model):\n    \"\"\"\n    Enregistre les param\u00e8tres, m\u00e9triques, graphiques, et mod\u00e8le dans MLflow.\n\n    Args:\n        metrics (dict): Dictionnaire des m\u00e9triques de performance du mod\u00e8le.\n        params (dict): Dictionnaire des param\u00e8tres du mod\u00e8le.\n        fig (plotly figure): Graphique des r\u00e9sultats.\n        model (object): Le mod\u00e8le entra\u00een\u00e9 (par exemple, un mod\u00e8le scikit-learn).\n    \"\"\"\n    try:\n        logger.info(\"Logging parameters to MLflow\")\n        mlflow.log_params(params) \n\n        logger.info(\"Logging metrics to MLflow\")\n        mlflow.log_metrics(metrics) \n\n        logger.info(\"Logging figure to MLflow\")\n        fig_path = \"results_plot.png\"\n        fig.savefig(fig_path)\n        mlflow.log_artifact(fig_path, \"figures\")  \n\n        logger.info(\"Logging model to MLflow\")\n        mlflow.sklearn.log_model(model, \"model\")  \n\n        logger.info(\"All artifacts logged successfully\")\n    except Exception as e:\n        logger.error(f\"Failed to log artifacts: {e}\")\n</code></pre>"},{"location":"2024/09/01/mlflow--best-practices/#srcplot_utilspy","title":"<code>src/plot_utils.py</code>","text":"<p>Voici une fonction pour cr\u00e9er un graphique comparant les valeurs r\u00e9elles et pr\u00e9dites :</p> <pre><code>import plotly.graph_objects as go\n\ndef create_plot(X_test, y_test, y_pred):\n    \"\"\"Cr\u00e9e un graphique comparant les valeurs r\u00e9elles aux valeurs pr\u00e9dites avec des points pour les vraies valeurs et une ligne pour les pr\u00e9dites.\"\"\"\n    fig = go.Figure()\n\n    fig.add_trace(go.Scatter(\n        x=X_test.squeeze(), \n        y=y_test, \n        mode='markers', \n        name='Actual', \n        marker=dict(size=8, color='rgba(152, 0, 0, .8)', line=dict(width=1, color='DarkSlateGrey'))\n    ))\n\n    fig.add_trace(go.Scatter(\n        x=X_test.squeeze(), \n        y=y_pred, \n        mode='lines', \n        name='Predicted',\n        line=dict(color='rgba(0, 152, 0, .8)', width=2)\n    ))\n\n    fig.update_layout(\n\n\n        title='Actual vs Predicted Values',\n        xaxis_title='Feature Value',\n        yaxis_title='Target Value',\n        template='plotly_white'\n    )\n\n    return fig\n</code></pre>"},{"location":"2024/09/01/mlflow--best-practices/#srcmodelingpy","title":"<code>src/modeling.py</code>","text":"<p>Voici un exemple de module modeling: <pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.metrics import mean_squared_error, r2_score\n\ndef simulate_data(seed=42, size=100):\n    \"\"\"Simulates data for training.\"\"\"\n    np.random.seed(seed)\n    X = np.random.rand(size, 1) * 10\n    y = 3 * X.squeeze() + 4 + np.random.randn(size) * 2\n    return pd.DataFrame(data={'X': X.squeeze(), 'y': y})\n\ndef train_model(X_train, y_train, alpha, l1_ratio):\n    \"\"\"Trains an ElasticNet model.\"\"\"\n    model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n    model.fit(X_train, y_train)\n    return model\n\ndef evaluate_model(model, X_test, y_test):\n    \"\"\"Evaluates the model on the test set.\"\"\"\n    y_pred = model.predict(X_test)\n    mse = round(mean_squared_error(y_test, y_pred), 2)\n    r2 = round(r2_score(y_test, y_pred), 2)\n    return y_pred, {\"mse\": mse, \"r2\": r2}\n</code></pre></p>"},{"location":"2024/09/01/mlflow--best-practices/#script-principal-mainpy","title":"Script Principal <code>main.py</code> :","text":"<p>Le script principal utilise log_artifacts pour int\u00e9grer MLflow et suivre l'entra\u00eenement et l'\u00e9valuation du mod\u00e8le.</p> <pre><code>from loguru import logger \nimport argparse\nimport mlflow\nfrom sklearn.model_selection import train_test_split\nfrom modeling import simulate_data, train_model, evaluate_model\nfrom mlflow_utils import log_artifacts\nfrom plot_utils import create_plot\n\n# Constants\nEXPERIMENT_NAME = 'blog-backbone'\nRUN_NAME = \"simple linear regression\"\nREMOTE_SERVER_URL = \"http://mlflow_server:5000\" \n\ndef main(alpha, l1_ratio):\n    \"\"\"Fonction principale pour ex\u00e9cuter l'entra\u00eenement et l'\u00e9valuation du mod\u00e8le.\"\"\"\n    logger.info(\"Starting script execution\")\n\n    logger.info(\"Simulating data\")\n    data = simulate_data()\n    X_train, X_test, y_train, y_test = train_test_split(data[['X']], data['y'], test_size=0.2, random_state=42)\n\n    logger.info(\"Training model with alpha: {} and l1_ratio: {}\".format(alpha, l1_ratio))\n    model = train_model(X_train, y_train, alpha, l1_ratio)\n\n    logger.info(\"Model evaluation\")\n    y_pred, metrics = evaluate_model(model, X_test, y_test)\n\n    logger.info(\"Preparing results\")\n    params = {\n        \"alpha\": alpha,\n        \"l1_ratio\": l1_ratio,\n        \"test_size\": 0.2\n    }\n    fig = create_plot(X_test, y_test, y_pred)\n\n    logger.info(\"Main execution completed\")\n    return metrics, params, fig, model\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"ElasticNet model training script\")\n    parser.add_argument(\"--alpha\", type=float, required=True, help=\"Alpha parameter for ElasticNet\")\n    parser.add_argument(\"--l1_ratio\", type=float, required=True, help=\"L1 ratio parameter for ElasticNet\")\n    args = parser.parse_args()\n    mlflow.set_tracking_uri(REMOTE_SERVER_URL)\n    mlflow.set_experiment(EXPERIMENT_NAME)    \n    with mlflow.start_run(run_name=RUN_NAME):\n        metrics, params, fig, model = main(args.alpha, args.l1_ratio)\n        log_artifacts(metrics, params, fig, model)\n</code></pre>"},{"location":"2024/09/01/mlflow--best-practices/#etape-dexecution","title":"\u00c9tape d'Ex\u00e9cution","text":"<p>Pour ex\u00e9cuter le projet et observer les r\u00e9sultats de l'enregistrement des exp\u00e9rimentations avec MLflow, on pourrait utiliser la proc\u00e9dure ci-apr\u00e8s:</p> <ol> <li> <p>Configuration du PYTHONPATH :    Avant d'ex\u00e9cuter le script principal, configurez le <code>PYTHONPATH</code> pour que Python reconnaisse correctement les modules de votre projet. Sans cette configuration, vous pourriez \u00eatre amen\u00e9 \u00e0 utiliser des chemins relatifs dans le code, ce qui peut compliquer les choses. \u2019utilisation de PYTHONPATH permet de sp\u00e9cifier le r\u00e9pertoire racine de vos modules Python, \u00e9vitant ainsi les chemins relatifs compliqu\u00e9s dans votre code. Cela simplifie l'importation des modules et rend le code plus propre et plus maintenable. Configurez <code>PYTHONPATH</code> avec la commande suivante dans un terminal :</p> <pre><code>export PYTHONPATH=\"src/\"\n</code></pre> </li> <li> <p>Ex\u00e9cution du Script Principal :    Utilisez <code>argparse</code> pour rendre votre script param\u00e9trable depuis la ligne de commande. Voici comment lancer le script avec diff\u00e9rents param\u00e8tres :</p> <pre><code>python -m main --alpha 0.1 --l1_ratio 0  #L2  ridge regression\npython -m main --alpha 0.1 --l1_ratio 1  #L1  lasso regression\n</code></pre> <ul> <li><code>argparse</code> : Rend le script interactif et param\u00e9trable depuis la ligne de commande, facilitant les tests avec diff\u00e9rents param\u00e8tres sans modifier le code source. </li> </ul> </li> </ol> <p>Bien que vous puissiez utiliser des notebooks, il est souvent recommand\u00e9 de travailler avec des fichiers <code>.py</code> pour une meilleure organisation et gestion du projet, notamment pour des projets de prototypage ou d'industrialisation.</p>"},{"location":"2024/09/01/mlflow--best-practices/#interface-utilisateur-mlflow","title":"Interface Utilisateur MLflow","text":"<p>Apr\u00e8s avoir ex\u00e9cut\u00e9 les commandes ci-dessus, vous pouvez v\u00e9rifier que les exp\u00e9rimentations sont correctement enregistr\u00e9es en acc\u00e9dant \u00e0 l'interface utilisateur de MLflow. Vous y trouverez les param\u00e8tres, les m\u00e9triques, les graphiques, et les mod\u00e8les associ\u00e9s \u00e0 chaque exp\u00e9rimentation. Pour acc\u00e9der \u00e0 cette interface, vous devez utiliser l'URL de votre serveur MLflow. Personnellement, j'utilise un reverse proxy pour acc\u00e9der \u00e0 mon service. En mode local, vous pouvez d\u00e9marrer l'interface utilisateur avec la commande <code>mlflow ui</code>.</p> <p></p> <p>Comme le montre l'image ci-dessus, les deux exp\u00e9rimentations que j'ai lanc\u00e9es sont enregistr\u00e9es. Chacune affiche les param\u00e8tres, les m\u00e9triques et les temps d'ex\u00e9cution. Vous pouvez obtenir plus de d\u00e9tails en cliquant sur une exp\u00e9rimentation sp\u00e9cifique.</p> <p></p> <p>Dans la section des artefacts de la premi\u00e8re exp\u00e9rimentation, vous pouvez r\u00e9cup\u00e9rer votre graphique (n'est-ce pas g\u00e9nial ?), ainsi que les m\u00e9tadonn\u00e9es o\u00f9 sont sauvegard\u00e9s les coefficients. Vous trouverez \u00e9galement le mod\u00e8le enregistr\u00e9, que vous pouvez utiliser directement pour faire des pr\u00e9dictions.</p>"},{"location":"2024/09/01/mlflow--best-practices/#conclusion","title":"Conclusion","text":"<p>Alors, la prochaine fois que quelqu\u2019un vous demande de retrouver des resulats datant de trois mois, vous pourrez vous permettre de sourire et dire : \"Pas de probl\u00e8me, je vais chercher \u00e7a dans MLflow.\" Parce qu'avec MLflow, fini le stress des donn\u00e9es \u00e9gar\u00e9es et des param\u00e8tres oubli\u00e9s ; tout est \u00e0 port\u00e9e de clic, pr\u00eat \u00e0 \u00eatre revisit\u00e9, analys\u00e9 et, bien s\u00fbr, utilis\u00e9 \u00e0 bon escient. Quand on utilise du Mlflow c'est que aimerait faire du MLOps et c'est d\u00e9j\u00e0 un bon debut. Faire du code propre vous permettra d'integrer mlfow dans vos projets</p>"},{"location":"2024/09/01/mlflow--best-practices/#references","title":"R\u00e9f\u00e9rences:","text":"<ul> <li>https://mlflow.org/docs/latest/index.html</li> </ul>"},{"location":"archive/2024/","title":"2024","text":""},{"location":"category/mlflow/","title":"Mlflow","text":""},{"location":"category/mlops/","title":"MlOps","text":""},{"location":"category/cicd/","title":"CI/CD","text":""},{"location":"category/documentation/","title":"documentation","text":""},{"location":"category/aws/","title":"AWS","text":""},{"location":"category/cloud/","title":"Cloud","text":""},{"location":"category/eks/","title":"EKS","text":""},{"location":"category/iam/","title":"IAM","text":""},{"location":"category/ec2/","title":"EC2","text":""},{"location":"category/emr/","title":"EMR","text":""},{"location":"category/spark/","title":"Spark","text":""}]}