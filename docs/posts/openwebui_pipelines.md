---
date : 2024-09-14  
auteurs : 
  - ssawadogo  
cat√©gories : 
  - IAGen
  - MlOps
---

# Faire tourner son chatbot avec une interface √©quivalente √† ChatGPT gr√¢ce √† OpenWebUI

L'√®re des assistants conversationnels est en pleine expansion, et gr√¢ce √† des outils comme **OpenWebUI**, il est d√©sormais possible de cr√©er son propre UI conversationnel avec des fonctionnalit√©s similaires √† celles de ChatGPT sans beaucoup d'effort. On suppose que vous avez d√©j√† un chatbot qui fonctionne correctement et que vous souhaitez une meilleure interface utilisateur. Cet article vous guidera dans la mise en place de cette UI.
<!-- more -->



### 1. Qu‚Äôest-ce qu‚ÄôOpenWebUI ?
**OpenWebUI** est con√ßu pour √™tre une solution flexible d‚Äôinterface utilisateur (UI) open-source qui facilite l‚Äôinteraction avec les LLM comme GPT-3.5 ou GPT-4. Il repose sur deux composants principaux :
- **Le composant OpenWebUI proprement dit** : C‚Äôest l‚Äôinterface utilisateur qui permet de g√©rer les interactions entre l'utilisateur et le mod√®le.
- **Le composant Pipelines** : Cette composante s‚Äôoccupe de la logique LLM. Elle permet d'intercepter, traiter et modifier les prompts utilisateurs avant de les envoyer au mod√®le final.

Bon, comme vous le savez , une image vaut mieux milles vaux. Voici la magie que nous propose openwebui
![image](openwebui_pipelines/presentation.png)

### 2. Concepts de Pipelines

Pour comprendre l'outil pipelines, il faut s‚Äôint√©resser aux valves, filtres (filters) et pipes.

#### 2.1. Concepts de Valves
Les valves jouent un r√¥le de r√©gulation dans le pipeline, autorisant ou bloquant le passage de certaines donn√©es.  
Pour qu'un pipeline soit fonctionnel, on doit avoir une classe valves. Le plus souvent, c‚Äôest l'endroit o√π sont pass√©s les credentials cl√©s et param√®tres des mod√®les.


#### 2.2. Concept de Filter 
Un **Filter Pipeline** est principalement utilis√© pour intercepter le message avant qu'il ne soit envoy√© au LLM, ou apr√®s avoir re√ßu la r√©ponse du LLM mais avant de l'envoyer √† l'utilisateur. L'id√©e derri√®re le **Filter Pipeline** est d‚Äôajouter des √©tapes **avant** ou **apr√®s** l'appel au mod√®le. Il sert donc principalement √† :
- **R√©cup√©rer des informations externes (RAG)** pour enrichir le contexte du message avant l'envoi au LLM.
- **Ex√©cuter des outils** qui ajoutent des donn√©es suppl√©mentaires n√©cessaires au LLM.
- **Appliquer des filtres de s√©curit√©** ou d'autres types de transformation avant que la r√©ponse ne soit affich√©e √† l'utilisateur.

```mermaid
graph LR;
    A[Chat Request] --> B[Inlet];
    B --> C[LLM Model];
    C --> D[Outlet];
    D --> E[Chat Response];

    subgraph Filter Pipeline
        B-->C;
        C-->D;
    end
```

*Exemple* :

Si l'utilisateur demande *"Quelle est la m√©t√©o √† Paris ?"*, le Filter Pipeline peut intercepter la requ√™te avant de l‚Äôenvoyer au LLM, appeler une API m√©t√©o pour obtenir la temp√©rature, et ensuite ajouter cette information dans le message contextuel envoy√© au mod√®le.

Voici un diagramme pour illustrer le flux d'un **Filter Pipeline** :

### 2.3 Concept de Pipe 
Un **Pipe Pipeline** prend **enti√®rement en charge** le traitement des messages. Il remplace ou enrichit la mani√®re dont le message est g√©r√© par le LLM. Au lieu de simplement ajouter des informations autour du message, comme dans un **Filter Pipeline**, le **Pipe Pipeline** contr√¥le **tout** le processus. Cela inclut :
- **Appeler diff√©rents mod√®les LLM** (comme GPT-4, GPT-3.5, Mistral, etc.) pour r√©pondre directement au message.
- **Construire des workflows** complexes qui peuvent int√©grer de nouvelles fonctionnalit√©s, comme ex√©cuter du code, consulter des bases de donn√©es, ou r√©cup√©rer des informations.
- **RAG (Retrieve and Generate)** : Cr√©er un syst√®me complet o√π les informations sont non seulement r√©cup√©r√©es mais aussi g√©n√©r√©es par un mod√®le choisi.

*Exemple* :

Dans un **Pipe Pipeline**, si l'utilisateur demande *"Raconte-moi une histoire"*, ce pipeline pourrait d√©cider quel mod√®le LLM utiliser (GPT-4, Claude, etc.) et cr√©er une r√©ponse en fonction du workflow configur√©. 

Voici un diagramme pour illustrer le flux d'un **Pipe Pipeline** :

```mermaid
graph LR;
    A[Chat Request] --> B[Pipe];
    B --> C[Chat Response];

    subgraph Pipe Pipeline
        B;
    end
```

On parle de **pipelines manifold** lorsque l‚Äôon a un pipe qui sait g√©rer plusieurs mod√®les. En gros, c‚Äôest la m√™me logique d'impl√©mentation, mais le LLM utilis√© pour le chat va diff√©rer. Un peu plus bas, j'ai impl√©ment√© un pipe qui sert de ChatGPT, o√π je peux choisir quel mod√®le utiliser entre GPT-3.5, GPT-4, ou GPT-mini.

#### 2.4. Diff√©rences
La diff√©rence principale entre un **Filter Pipeline** et un **Pipe (ou Manifold) Pipeline** repose sur le **moment** et la **mani√®re** dont les donn√©es sont trait√©es avant ou apr√®s l'appel √† un mod√®le de langage (LLM).

### 3. Impl√©mentation de pipelines

#### 3.1. Pipeline simple :
Voici un exemple d'impl√©mentation d'un pipeline basique, qui utilise l'API OpenAI pour r√©pondre aux messages utilisateur.

```python
from typing import List, Union, Generator, Iterator
from pydantic import BaseModel
import os
import requests

class Pipeline:
    class Valves(BaseModel):
        OPENAI_API_KEY: str = os.getenv("OPENAI_API_KEY", "my-keys")

    def __init__(self):
        self.name = "OpenAI Pipeline GPT3.5"
        self.valves = self.Valves()

    def pipe(self, user_message: str, model_id: str, messages: List[dict], body: dict) -> Union[str, Generator, Iterator]:
        headers = {"Authorization": f"Bearer {self.valves.OPENAI_API_KEY}", "Content-Type": "application/json"}
        payload = {**body, "model": model_id}
        self._clean_payload(payload)

        try:
            response = requests.post(url="https://api.openai.com/v1/chat/completions", json=payload, headers=headers)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            return f"Error: {e}"

    @staticmethod
    def _clean_payload(payload: dict):
        keys_to_remove = {"user", "chat_id", "title"}
        for key in keys_to_remove:
            payload.pop(key, None)
```

#### 3.2 Pipeline manifold (multi-mod√®le) :
Un pipeline manifold permet de g√©rer plusieurs mod√®les d'IA en parall√®le. Voici un exemple qui inclut plusieurs mod√®les d'OpenAI.

```python
from typing import List, Union, Generator, Iterator
from pydantic import BaseModel
import os
import requests

class Pipeline:
    class Valves(BaseModel):
        OPENAI_API_BASE_URL: str = "https://api.openai.com/v1"
        OPENAI_API_KEY: str = os.getenv("OPENAI_API_KEY", "your-openai-api-key")

    def __init__(self, name: str = "manifold: "):
        self.type = "manifold"
        self.name = name
        self.valves = self.Valves()
        self.pipelines = self.get_openai_models()

    def get_openai_models(self):
        predefined_model_ids = ['gpt-4', 'gpt-3.5-turbo', 'gpt-4o-2024-08-06', 'gpt-4o-mini']
        return [{'id': model_id, 'name': model_id} for model_id in predefined_model_ids]

    def pipe(self, user_message: str, model_id: str, messages: List[dict], body: dict) -> Union[str, Generator, Iterator]:
        headers = {"Authorization": f"Bearer {self.valves.OPENAI_API_KEY}", "Content-Type": "application/json"}
        payload = {**body, "model": model_id}
        self._clean_payload(payload)

        try:
            response = requests.post(url=f"{self.valves.OPENAI_API_BASE_URL}/chat/completions", json=payload, headers=headers)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            return f"Error: {e}"

    @staticmethod
    def _clean_payload(payload: dict):
        keys_to_remove = {"user", "chat_id", "title"}
        for key in keys_to_remove:
            payload.pop(key, None)
```

#### 3.3 Pour aller plus loin :
Il n'y a rien de mieux que la documentation officielle. Vous y trouverez une pl√©thore d'exemples d'impl√©mentation de pipelines que vous pourriez personnaliser. Vous trouverez plus d'une cinquantaine d'exemples ici :  
[exemples de pipelines](https://github.com/open-webui/pipelines/tree/main/examples/)

### 4. Construire votre stack avec Docker Compose


#### 4.1. Motivation

L'une des mani√®res les plus efficaces de mettre en place cette architecture est d‚Äôutiliser **Docker Compose**. Voici un exemple de configuration pour orchestrer les services n√©cessaires au fonctionnement de votre chatbot.

Selon la documentation officielle pour installer **open-webui/open-webui**, une commande telle que :

```bash
docker run -d -p 3000:8080 -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama
```

est recommand√©e. D'autres m√©thodes d'installation sont aussi d√©taill√©es sur le site officiel [ici](https://github.com/open-webui/open-webui).

En utilisant cette commande, l'interface est d√©j√† pr√™te √† l'emploi. Cependant, pour la communication avec l'UI, [https://github.com/open-webui/pipelines](https://github.com/open-webui/pipelines) propose une configuration simplifi√©e via Docker :

Ex√©cutez le conteneur **Pipelines** avec la commande suivante :

```bash
docker run -d -p 9099:9099 --add-host=host.docker.internal:host-gateway -v pipelines:/app/pipelines --name pipelines --restart always ghcr.io/open-webui/pipelines:main
```

Ensuite, connectez **Open WebUI** :

- Allez dans **Settings > Connections > OpenAI API** dans Open WebUI.
- Configurez l'URL de l'API √† `http://localhost:9099` et la cl√© API √† `0p3n-w3bu!`. Vos pipelines devraient maintenant √™tre actifs.

Cependant, la connexion n‚Äôest pas toujours aussi simple üòÖ. La documentation n‚Äôest pas encore optimale. Je vous conseille de bien explorer **Pipelines** sur le site et de comprendre comment l‚Äôinteraction entre les deux services devrait se faire pour am√©liorer la communication.

Parfois, des pipelines sont d√©j√† disponibles et vous pouvez vous inspirer des exemples ici : [Pipelines Exemples](https://github.com/open-webui/pipelines/tree/main/examples). Cependant, l‚Äôint√©gration est une autre affaire. Apr√®s plusieurs essais, j‚Äôai r√©ussi √† connecter les deux services en ajustant des variables cl√©s comme **REQUIREMENTS_PATH**, **PYTHONPATH**, et d'autres, gr√¢ce √† des volumes de copie pour les pipelines.

#### 4.2. Exemple de `docker-compose.yml` :
```yaml
ervices:
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    volumes:
      - open-webui:/app/backend/data
    ports:
      - ${OPEN_WEBUI_PORT-3000}:8080
    environment:
      - WEBUI_SECRET_KEY=
      - OPENAI_API_BASE_URL=http://pipelines:9099
      - OPENAI_API_KEY=0p3n-w3bu!
      - ENABLE_OLLAMA_API=false
    extra_hosts:
      - host.docker.internal:host-gateway
    restart: unless-stopped

  pipelines:
    image: ghcr.io/open-webui/pipelines:main
    container_name: pipelines
    volumes:
      - ./chat_pipelines/pipelines:/app/pipelines
      - ./chat_pipelines/openwebui_utils:/app/openwebui_utils
      - ./src/onepiece_bot:/app/onepiece_bot
      - ./requirements.txt:/app/requirements_custom.txt
    extra_hosts:
      - host.docker.internal:host-gateway
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - PIPELINES_DIR=${PIPELINES_DIR}
      - RESET_PIPELINES_DIR=${RESET_PIPELINES_DIR}
      - PIPELINES_REQUIREMENTS_PATH=${PIPELINES_REQUIREMENTS_PATH}
      - PYTHONPATH=/app
    restart: unless-stopped
    ports:
      - 9099:9099

volumes:
  open-webui:
```


Comme vous travaillez avec Docker, vous pouvez facilement inspecter ce qui se passe et v√©rifier si tout fonctionne correctement ou non. Le service UI (**OpenWebUI**) fonctionne g√©n√©ralement tr√®s bien ; cependant, il faut porter une attention particuli√®re √† **Pipelines**. 

Pour d√©boguer, vous pouvez ex√©cuter une commande comme `docker ps` ou `docker logs pipelines`. Si vous utilisez Docker Desktop, vous devriez voir quelque chose de similaire √† ceci, montrant que vos deux conteneurs sont en cours d'ex√©cution :

![image](openwebui_pipelines/docker_view.PNG)

Les logs sont disponibles en cliquant sur les noms de chaque service :

![image](openwebui_pipelines/docker_pipelines_log_if_setup_ok.PNG)

Ici, on voit que tout fonctionne bien ! üòä

Pour un exemple complet d'impl√©mentation de bout en bout et comment l'ex√©cuter, je vous invite √† consulter mon d√©p√¥t GitHub o√π vous trouverez des exemples et des pipelines pr√™ts √† l'emploi : [Mon d√©p√¥t GitHub](https://github.com/sawadogosalif/openwebui/tree/main). Vous y trouverez √©galement des informations sur les variables d'environnement et d'autres configurations utiles.


### Conclusion
En combinant OpenWebUI et une architecture pipeline, il est possible de cr√©er un UI de conversion flexible commme celui de chatgpt, tout en ayant un contr√¥le total sur l'interface utilisateur et la logique de traitement des requ√™tes. Que vous soyez un passionn√© de LLM ou simplement curieux de tester, OpenWebUI avec Pipelines offre une base solide pour innover. Comme le projet est actuellment nouveau, n'hesitez pas √† suivre de pr√®s leurs evolutions.
