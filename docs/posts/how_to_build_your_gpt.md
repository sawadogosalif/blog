# Comment est construit les assistants conversationnels? GPT3.5, Claude3, Mistral

**Salut √† tous mes passionn√©s de l'IA et du machine learning !**  
Aujourd'hui, on plonge dans les coulisses des grands mod√®les de langage (LLM) comme GPT-3.5. Vous √™tes-vous d√©j√† demand√© **comment ces mod√®les arrivent √† r√©pondre avec autant de fluidit√© ?** La r√©ponse tient en trois √©tapes-cl√©s :  

1. Le **pr√©-entra√Ænement** sur des montagnes de donn√©es,  
2. Le **fine-tuning** pour sp√©cialiser le mod√®le, et  
3. L‚Äô**apprentissage par renforcement avec feedback humain** (RLHF).  
Quand on parle de chatgpt, techniquement c'est le produit du dernier mod√®le openai ap√®rs avoir fait une serie d'entrainement.

Dans cet article, je retrace les principales √©tapes avec vous . Un petit billet de blog pour clore l'ann√©e 2024. On ne fera pas de MLOPS aujourdh'ui , promis :)

---

### 1. Pr√©-entra√Ænement : construire son mod√®le de fondation

Imaginez qu'on veuille apprendre √† un enfant √† parler. Quelle est la premi√®re √©tape ? Exactement, on lui expose **√©norm√©ment de mots et phrases**‚ÄØ: conversations, livres, histoires. C‚Äôest exactement ce qu‚Äôon fait avec un LLM, mais √† une √©chelle gigantesque.

**Comment √ßa marche ?**  
On nourrit le mod√®le avec un √©norme corpus de donn√©es‚ÄØ: sites web, livres, forums, articles scientifiques‚Ä¶ Le mod√®le doit pr√©dire **le mot suivant** dans une phrase. Par exemple‚ÄØ: si je dis _"Les oiseaux volent dans le ciel‚Ä¶"_, √† votre avis, quel sera le mot suivant‚ÄØ? Voil√†, c‚Äôest ce que le mod√®le apprend √† faire.

> **Exemple concret : Le dataset utilis√© pour LLaMA**
>
> Voici un aper√ßu des donn√©es qui ont servi pour LLaMA (un mod√®le open-source) :

| **Source**        | **Proportion utilis√©e** | **√âpoques (Passages)** | **Taille** |
|-------------------|-------------------------|------------------------|------------|
| CommonCrawl       | 67 %                   | 1.10                   | 3,3 To     |
| C4                | 15 %                   | 1.06                   | 783 Go     |
| GitHub            | 4,5 %                  | 0.64                   | 328 Go     |
| Wikipedia         | 4,5 %                  | 2.45                   | 83 Go      |
| Livres            | 4,5 %                  | 2.23                   | 85 Go      |
| ArXiv             | 2,5 %                  | 1.06                   | 92 Go      |
| StackExchange     | 2 %                    | 1.03                   | 78 Go      |

Ces donn√©es brutes (non √©tiquet√©es) permettent au mod√®le d'apprendre **la grammaire, les relations entre les mots et le contexte**. Mais attention, cette √©tape n‚Äôest qu‚Äôune fondation. Construire les mod√®les de fondation n'est pas du ressort de p√©tites entrepises ou startup.  N√©anmoins √† partir de mod√®les de fondations OpenSource on peut cr√©eer d'autre d'autre mod√®les de fondations comme ce qu'on actuellement autour LLama( le mod√®le open source par excelleence)
Les LLMS comme GPT-3 n√©cessitent d‚Äô√©normes ressources de calcul. Par exemple, l‚Äôentra√Ænement de GPT-3 a √©t√© estim√© par des chercheurs comme **Tim Dettmers** et d‚Äôautres experts en IA :

- **Nombre de param√®tres** : 175 milliards (*Source : OpenAI, *Language Models are Few-Shot Learners*).  
- **Corpus de donn√©es** : Environ 570 Go de texte filtr√© (*Source : OpenAI, m√™me article*).  
- **Infrastructure** : Utilisation de clusters de GPU, notamment des **NVIDIA V100** (*Source : blog de Tim Dettmers*).  
- **Dur√©e estim√©e** : Entre **10 000 et 50 000 heures GPU**, d‚Äôapr√®s des calculs ind√©pendants de la communaut√©, bien que les donn√©es exactes ne soient pas publi√©es.

---

Cela permet de garantir une transparence totale, en citant explicitement les sources ou les auteurs d‚Äôestimations lorsque les donn√©es officielles manquent. üòä


Il faut gardant en tete que les mod√®les de fondations n sont pas des assistants, ils savent juste completer des phrases. D'autres couches sont ajout√©es pour arriver aux agents conversationnelles. J'ai trou√© un arbre sur github qui illustre bien cela 
Il faut gardant en tete que les mod√®les de fondations n sont pas des assistants, ils savent juste completer des phrases. D'autres couches sont ajout√©es pour arriver aux agents conversationnelles. J'ai trou√© un arbre sur github qui illustre bien cela 

![alt text](https://raw.githubusercontent.com/Mooler0410/LLMsPracticalGuide/main/imgs/tree.jpg)

source:  https://github.com/Mooler0410/LLMsPracticalGuide/


Vous verrez que la plus part des asistants conversiationnellem comme Bard, Chattgpt ouu Claude sont aux extremeits de l'arbres car ce sont des LLM mais pas de mod√®les de fondations.

La preuve que cene sont pas  des assistants est bien dans les examples apres

Le mod√®le de fondations  ne r√©pond pas aux questions
- Il veut seulement compl√©ter les documents Internet
- R√©pond souvent aux questions par d'autres questions

![image](./how_to_build_your_gpt/fondation_output.PNG)

---

### 2. Fine-Tuning : Sp√©cialiser le Mod√®le

Ici, on fait un peu comme avec un apprenti : apr√®s lui avoir montr√© plein de concepts g√©n√©raux, on l‚Äôentra√Æne pour des t√¢ches sp√©cifiques. Pour un mod√®le conversationnel, on lui montre des dialogues bien construits, o√π la question est claire et la r√©ponse pertinente.

**Pourquoi c‚Äôest important ?**  
Un mod√®le brut sait parler, mais pas toujours de mani√®re coh√©rente. Le fine-tuning lui apprend √† r√©pondre de fa√ßon pr√©cise dans un contexte sp√©cifique. **Vous imaginez un mod√®le qui parle moor√© sans confondre les tons ni les contextes ?** C‚Äôest ici que la magie op√®re.

> **Note pratique** : Vous pouvez utiliser des mod√®les open-source d√©j√† pr√©-entra√Æn√©s, comme LLaMA ou GPT-J, pour gagner du temps. Ajoutez vos propres donn√©es annot√©es pour un fine-tuning personnalis√©.

---

### 3. RLHF : Le Dernier Coup de Pinceau

Voici l‚Äô√©tape la plus fascinante, mais aussi la plus complexe‚ÄØ: le **Reinforcement Learning from Human Feedback*** (RLHF). Pourquoi cette √©tape‚ÄØ? Pour que le mod√®le ne soit pas seulement performant, mais qu‚Äôil soit aussi **align√© sur vos attentes**.  

En gros, √ßa marche comment‚ÄØ?

1. **On forme un mod√®le de r√©compense :** Les annotateurs humains √©valuent les r√©ponses du mod√®le. Par exemple, si le mod√®le r√©pond √† c√¥t√©, √ßa vaut 0. Si c‚Äôest parfait, √ßa vaut 1.
2. **Le mod√®le s‚Äôam√©liore :** Il apprend √† maximiser son score en g√©n√©rant des r√©ponses qui plaisent.
3. **R√©sultat‚ÄØ:** Vous obtenez un agent qui sait √™tre poli, pr√©cis et surtout **utile**.

> **Question pour vous :** Si vous formiez un mod√®le pour une communaut√© locale, comment d√©finiriez-vous les "bonnes r√©ponses" ? Politesse‚ÄØ? Contexte culturel‚ÄØ?

---

### Petit R√©cap avec un Visuel

Voici un sch√©ma tir√© de la pr√©sentation d‚ÄôAndrej Karpathy. Il r√©sume bien les √©tapes‚ÄØ:

![Pipeline GPT](./how_to_build_your_gpt/gpt_training_pipeline.PNG)

---

### Alors, pr√™ts √† passer √† l'action ? 

Cr√©er un assistant comme chatgopt conversationnel peut sembler complexe, mais en suivant ces trois √©tapes, on peut transformer un mod√®le de fondation en un assistant puissant. Et vous, qu'en pensez-vous des mod√®les de fondations open source et comment faites vous du RHLF,

Pour aller plus loin :  
- **Conf√©rence d‚ÄôAndrej Karpathy sur GPT** : [Regarder ici](https://www.youtube.com/watch?v=bZQun8Y4L2A).  
- **Blog de Hugging Face sur RLHF** : [Lire ici](https://huggingface.co/blog/rlhf).  
- **Ressources sur les LLM** : [Explorer ici](https://dsp-routine.ppd-datascience.analytics.safran/concepts/modeling/llms_101/#a-brief-history).

