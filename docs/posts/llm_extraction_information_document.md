---
date: 2024-11-24  
authors:  
- ssawadogo  
categories:  
- MLOps 
- IAGen 
---

# Adieu OCR, Place aux LLM Multimodaux pour l'Extraction des Informations dans les Documents  


Les formats comme **PDF**, **Word** ou **PowerPoint** sont omnipr√©sents pour le partage de documents, mais d√®s qu‚Äôil s‚Äôagit d‚Äôextraire des donn√©es structur√©es ou de g√©rer des contenus complexes, c‚Äôest une toute autre histoire.  

Ceux qui se sont d√©j√† aventur√©s dans ces t√¢ches connaissent la frustration des outils traditionnels comme `PDFium`, `pypdf` ou `textract`. Bien que pratiques pour des besoins basiques, ces solutions montrent vite leurs limites face aux documents denses, aux tableaux imbriqu√©s, ou au texte qui se chevauche.  

Mais les temps changent. Avec l‚Äôarriv√©e des **LLM multimodaux**, les choses s'am√©liorent drastiquement. Ces mod√®les permettent non seulement de traiter des fichiers comme des **PDF**, **PPTX** ou **Word**, qu‚Äôils soient purement textuels ou visuels, mais offrent √©galement la possibilit√© de personnaliser l‚Äôextraction. Par exemple, vous pouvez demander un formatage pr√©cis des donn√©es extraites.  

Dans cet article, je vous propose de d√©couvrir comment les LLM multimodaux r√©volutionnent cette t√¢che autrefois laborieuse, en rendant les processus non seulement plus simples mais aussi plus abordables. 
**Allons-y !**

<!-- more -->

## I. OCR et D√©fis 

Avant d‚Äôaller loin, voici un aper√ßu d‚Äôune page d‚Äôun document que je traite (plus de **320 pages** au total). Mon objectif est simple mais ambitieux : extraire pour chaque mot en **moor√©** son √©quivalent ou explication associ√©e en **fran√ßais**.  

![alt text](./llm_extraction_information_document/introduction.jpg)  

Quand il s‚Äôagit de documents simples, l‚Äôextraction de texte est faisable sans trop de souci. Mais d√®s qu‚Äôon passe √† des documents complexes, les vrais d√©fis apparaissent :  
1. **D√©tection des structures hasardeuse** : les tableaux, colonnes et sections imbriqu√©es deviennent illisibles pour certains outils.  
2. **Multi-langues** : le contenu en plusieurs langues pose probl√®me √† de nombreux OCR, surtout lorsqu'il m√©lange des langues locales et internationales.  
3. **Fiabilit√© limit√©e** : beaucoup d'OCR ajoutent du bruit au lieu d‚Äôapporter des r√©sultats propres et exploitables.  

Au vu de ces limitations, j‚Äôai vite abandonn√© la solution **OCR classique** pour mon projet.


## II. Une solution gagnante : les LLM multimodaux  

### Processus simplifi√©

```mermaid  
graph LR;  
    A["document"] --> B[Images]  
    B --> C[Images B64]  
    C -->D[LLM multimodal]  
    D --> E[JSON structur√©]  
    E --> F[(Stocker)]  
```

Ce diagramme illustre parfaitement pourquoi cette approche est moins fastidieuse. Gr√¢ce aux **LLM multimodaux**, m√™me les t√¢ches complexes, comme celle-ci, deviennent g√©rables en si peu de temps. Par exemple, **GPT-4o** ou **GPT-4o-mini** excelle dans ce genre de traitement.

üëâ *Pourquoi c‚Äôest gagnant ?*  
- **Meilleure compr√©hension des structures** : les tableaux et sections imbriqu√©es sont correctement analys√©s.  
- **Prise en charge multilingue** : les LLM reconnaissent les langues locales et internationales sans trop de pertes.  
- **R√©sultats propres** : moins de bruit et plus d'informations directement exploitables.  

Avec cette strat√©gie, chaque page est trait√©e avec pr√©cision, et les donn√©es sont pr√™tes √† √™tre utilis√©es pour alimenter ma base de traduction moor√©-fran√ßais.

### Estimation du co√ªt par page  

D'apr√®s la documentation d'OpenAI(*gpt vision*), chaque image est redimensionn√©e si n√©cessaire pour s'adapter √† un carr√© de 1024x1024 pixels, g√©n√©rant ainsi 85 tokens de base. Mais pour des images complexes, le mod√®le proc√®de diff√©remment : il les divise en tuiles de 512x512 pixels pour une reconnaissance compl√®te. Tu peux trouver plus de d√©tails [ici](https://community.openai.com/t/how-do-i-calculate-image-tokens-in-gpt4-vision/492318/3).

**Comment les tuiles affectent le co√ªt ?**  

Chaque tuile g√©n√®re 170 tokens. Donc, la formule de calcul est simple :  
**Total tokens = 85 + 170 * n**, o√π **n** repr√©sente le nombre de tuiles n√©cessaires pour couvrir l'image.

Voici une fonction Python qui illustre cela :  

```python  
from math import ceil  

def resize(width, height):  
    if width > 1024 or height > 1024:  
        if width > height:  
            height = int(height * 1024 / width)  
            width = 1024  
        else:  
            width = int(width * 1024 / height)  
            height = 1024  
    return width, height  

def count_image_tokens(width: int, height: int):  
    width, height = resize(width, height)  
    h = ceil(height / 512)  
    w = ceil(width / 512)  
    total = 85 + 170 * h * w  
    return total  
```

**Exemples pour clarifier :**  
- 500x500 ‚Üí 1 tuile suffit : total tokens = 85 + 170 = 255 
- 513x500 ‚Üí 2 tuiles : total tokens = 85 + 170 * 2 = 425

Dans mon cas, mes images mesurent 2480 x 3509, ce qui n√©cessite environ 4 tuiles, soit 780 tokens.

Cependant, le texte contenu dans une page d√©passe g√©n√©ralement 780  tokens. Regarde ce qui suit :  

```markdown  
1 paragraphe ‚âà 100 tokens  
1 500 mots ‚âà 2048 tokens  
```  
Source : [OpenAI Help](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them)

Avec mes pages, qui contiennent plusieurs paragraphes, on d√©passe largement les **780 tokens** d'une image seule . Convaincu maintenant‚ÄØ?
 J'ai inclus ces calculs pour bien montrer comment le comptage des tokens fonctionne.

**Co√ªt estim√© avec GPT-4o :**  
- **Entr√©e** : Environ 1 300 tokens (image + instruction).  
- **Sortie** : Environ 900 tokens.  
- **Co√ªt** : **0,002 $ par image**.  

Tout ceci pour montrer qu'il est crucial de bien √©valuer le co√ªt avant de traiter un volume important de documents ! üòâ


## III. Conversion PDF en images  

Bon, Arr√™tons les calculs maintenant hahaah. Pour convertir des PDF en images, nous avons besoin d‚Äôun outil nomm√© **Poppler**.

**Poppler** est une biblioth√®que gratuite et open-source pour le rendu des documents PDF, soutenue par freedesktop.org.  

### Pr√©-requis  
1. Installer **Poppler** :  
   T√©l√©chargez [Poppler pour Windows](https://github.com/oschwartz10612/poppler-windows/releases/) et ajoutez son *chemin* dans vos variables d‚Äôenvironnement.  

2. Installer **pdf2image** avec pip :  
   ```bash  
   pip install pdf2image  
   ```  

### Code Python  
Voici comment convertir un PDF en images JPEG :  
```python  
from pdf2image import convert_from_path  

pdf_path = "./Dictionnaire.pdf"  
output_folder = "./images"  

print("D√©but de la conversion...")  
images = convert_from_path(pdf_path, dpi=300, output_folder=output_folder, fmt='jpeg')  

for i, image in enumerate(images):  
    image.save(f"{output_folder}/page_{i + 1}.jpg", "JPEG")  
    print(f"Page {i + 1} sauvegard√©e.")  
```  
Ce bout de code transforme toutes les pages en images.  
Quand tu ouvres ton dossier de sortie, tu devrais voir quelque chose comme ceci :  
![alt text](llm_extraction_information_document/image_generation.PNG)


Voici la version corrig√©e avec les erreurs grammaticales et d'orthographe corrig√©es :


## IV. Extraction d‚Äôinformations avec un LLM  

Maintenant que nous avons les documents, comment r√©cup√©rer les informations qui nous int√©ressent ? Toutes les √©tapes sont dans l'illustration ci-dessous :

```mermaid
flowchart LR
    A[Image] --> B[Encoder en Base64]
    B --> C[Cr√©er un prompt]
    C --> D[Appeler l'API OpenAI]
    D --> E[R√©ponse structur√©e LLM]
    E --> F[Parser la r√©ponse]
    F --> G[Structure Python]
    G --> H[(Persister)]
```

### √âtape 1 : Encoder en base64  

Le mod√®le doit √™tre capable de traiter les images, mais l‚ÄôAPI ne permet pas d'envoyer des fichiers d‚Äôimages directement. C‚Äôest pourquoi on utilise le format **base64** pour convertir l‚Äôimage en une cha√Æne de texte que le mod√®le pourra comprendre.  

```python  
import base64  

def convert_image_to_base64(image_path: str) -> str:  
    """Encode une image en une cha√Æne base64."""  
    with open(image_path, "rb") as f:  
        return base64.b64encode(f.read()).decode("utf-8")  
```  

> **R√©sum√©** : Cette fonction ouvre l‚Äôimage, la lit en mode binaire, puis la transforme en une cha√Æne base64. Cette cha√Æne est ensuite utilis√©e pour communiquer avec le mod√®le.

### √âtape 2 : Prompt engineering  

Pour que le mod√®le comprenne que nous lui envoyons une image, il faut l‚Äôindiquer clairement dans le prompt. 

```python  
def create_prompt_for_dictionary(base64_image: str) -> List[dict]:  
    """Cr√©e un prompt adapt√© pour extraire des donn√©es structur√©es."""  
    instruction = """  
    Tu es un syst√®me con√ßu pour extraire les connaissances de documents. Le document contient un dictionnaire Moore-Fran√ßais.  
    Retourne les entr√©es au format XML-like tag <output>...</output>. Pour chaque mot, inclut les explications d√©taill√©es.  
    Ignore les parties anglaises et conserve les symboles sp√©ciaux.  
    """  
    return [  
        {  
            "role": "user",  
            "content": [  
                {"type": "text", "text": instruction},  
                {"type": "image_url", "image_url": {"url": f"data:image/jpg;base64,{base64_image}"}}  
            ],  
        }  
    ]  
```  

**√Ä retenir** : 

1. Le prompt indique clairement que le document contient des mots en Moor√© accompagn√©s de leur traduction en fran√ßais √† extraire. J‚Äôajoute aussi un exemple de r√©ponse attendue pour guider le LLM efficacement.  
2. La r√©ponse est demand√©e dans un format structur√© avec des balises `<output>...</output>`.  
3. L‚Äôimage encod√©e est encapsul√©e dans une balise `"image_url"`, ce qui permet au mod√®le de comprendre qu‚Äôil s‚Äôagit bien d‚Äôune image.  
4. Ne t'attarde pas trop sur la finesse du prompt üòÖ, il n‚Äôest pas super √©l√©gant, je l‚Äôavoue‚ÄØ!  
5. Par choix personnel, je n‚Äôai pas utilis√© de **syst√®me prompt**, uniquement un **query prompt**. Pour les images, il est n√©cessaire de sp√©cifier le type avec `{"type": "image_url", ...}`, une exigence sp√©cifique qui ne s‚Äôapplique pas aux cas standards.  


### √âtape 3 : Appel √† l‚ÄôAPI OpenAI  
On utilise l‚ÄôAPI OpenAI pour interroger le mod√®le. Le mod√®le est configur√© pour r√©pondre en respectant les instructions et en g√©n√©rant une r√©ponse structur√©e.  

```python  
import openai  

openai.api_key = "sk-xxxxxxxxxxxxxxxx"  

def get_llm_response(base64_image: str, model_name: str = "gpt-4o") -> str:  
    """Appelle l'API OpenAI avec une image encod√©e."""  
    messages = create_prompt_for_dictionary(base64_image)  
    response = openai.ChatCompletion.create(  
        model=model_name,  
        messages=messages,  
        max_tokens=8000,  
    )  
    return response["choices"][0]["message"]["content"]  
```  

> **R√©sum√©** :  
- L‚Äôimage encod√©e, accompagn√©e des instructions, est transmise au mod√®le via l‚ÄôAPI.  
- En retour, le mod√®le fournit une r√©ponse qui peut contenir des mots en Moor√© et leurs traductions.  

Un point int√©ressant √† noter est la possibilit√© de r√©aliser des appels asynchrones avec `openai.ChatCompletion.acreate`. Cette m√©thode permet de traiter plusieurs requ√™tes en parall√®le, ce qui est id√©al pour un grand volume d‚Äôimages. Cependant, j‚Äôai pr√©f√©r√© une approche plus simple dans mon cas. Pourquoi‚ÄØ? Parce que j‚Äôavais le temps üòÑ et, honn√™tement, jongler avec des `await` dans mes fonctions, ce n‚Äôest pas trop mon truc‚ÄØ!


### √âtape 4 : Parsing et sauvegarde  

Avec les LLM, le formatage des r√©ponses n‚Äôest pas toujours parfait. Pourtant, une strat√©gie simple et efficace consiste √† utiliser des balises XML pour structurer les r√©ponses. En pratique, une expression r√©guli√®re permet d‚Äôextraire uniquement ce qui se trouve entre des balises sp√©cifiques comme `<output>...</output>`. Cela r√©sout les probl√®mes o√π le mod√®le ajoute du texte ou des explications inutiles avant le contenu pertinent.  

Voici une impl√©mentation typique :  

```python  
import re  
import ast  
from typing import Optional, List  

def extract_output(text: str, tag: str) -> Optional[str]:  
    """Extrait le contenu entre balises XML."""  
    pattern = fr"<{tag}>(.*?)</{tag}>"  
    matches = re.findall(pattern, text, re.DOTALL)  
    return matches[0] if matches else None  

def parse_page_with_gpt(image_path: str, model_name: str = "gpt-4o") -> Optional[List[dict]]:  
    """Processus complet : encoder une image, appeler le LLM et parser le r√©sultat."""  
    image_base64 = convert_image_to_base64(image_path)  
    llm_output = get_llm_response(image_base64, model_name)  
    clean_output = extract_output(llm_output, "output")  
    return ast.literal_eval(clean_output) if clean_output else None  
```  

1. **Extraction XML** :  
   - L‚Äôutilisation de balises comme `<output>` garantit que seules les donn√©es importantes sont extraites.  
   - Le module `re` aide √† identifier ces sections en utilisant des motifs pr√©cis (ici, le contenu entre `<output>` et `</output>`).  

2. **Parsing Python** :  
   - Une fois le texte brut extrait, il est souvent encore au format cha√Æne de caract√®res.  
   - `ast.literal_eval` le transforme en un objet Python (liste ou dictionnaire), ce qui facilite son utilisation ult√©rieure.  

3. **Encapsulation compl√®te** :  
   - La fonction `parse_page_with_gpt` combine toutes les √©tapes, de l‚Äôencodage de l‚Äôimage √† la r√©cup√©ration des donn√©es, dans un processus fluide et r√©utilisable.  

> **Astuce** : Pour des projets plus avanc√©s(fine-tuning), explorez **[Outlines](https://github.com/OpenLLM/Outlines)**. Cet outil est parfait pour g√©n√©rer des parseurs robustes et bien formatter les resultats selon un schema demand√©.  

Avec cette approche, vous vous assurez d‚Äôune extraction propre et fiable, m√™me dans des sc√©narios o√π les LLM g√©n√®rent des r√©ponses un peu "bavardes".

Voici √† quoi ressemble ma sortie : 
![alt text](./llm_extraction_information_document/output.PNG)  

## Conclusion  

Avec leur capacit√© √† comprendre et interpr√©ter des formats vari√©s, les **LLM multimodaux** offrent une solution √©l√©gante pour l‚Äôextraction de donn√©es complexes, tout en r√©duisant consid√©rablement les co√ªts. L‚Äôapproche repose sur un pipeline efficace :  
1. Conversion des documents en images.  
2. Encodage en **Base64**.  
3. Utilisation d‚Äôun **prompt structur√©**.  
4. Parsing pr√©cis pour produire des donn√©es structur√©es.  

En quelques lignes de code, vous pouvez transformer des centaines de pages en donn√©es directement exploitables, √©liminant ainsi les frustrations des solutions classiques.  

**Et toi, quelles sont tes astuces pour extraire des donn√©es structur√©es √† partir de documents complexes ?** üòä